{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "task_name = \"stsb\"\n",
    "base_model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'stsb', split='train')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_dataset = load_dataset('glue', 'stsb', split='validation')#,cache_dir=\"/work/mhessent/cache\")\n",
    "test_dataset = load_dataset('glue', 'stsb', split='test')#,cache_dir=\"/work/mhessent/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2a1905efa4704bcb.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e194e0b596fcf478.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-c97e180e5b68e6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [],
   "source": [
    "#model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8ce536a089ec4a7b.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5d5d583045ec07fc.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4b0cd40886585b17.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(3.8000),\n",
       " 'input_ids': tensor([   0,  250,  313,   16,  816,   10,  739, 2342, 4467,    4,    2,    2,\n",
       "          250,  313,   16,  816,   10, 2342, 4467,    4,    2,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(4.7500),\n",
       " 'input_ids': tensor([   0,  250,  664,  920,   16, 5793,   10, 5253,    4,    2,    2,  250,\n",
       "          920,   16, 5793,   10, 5253,    4,    2,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    metric = load_metric(\"glue\",\"stsb\")\n",
    "    return metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=3e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=500,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/stsb_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer, RobertaConfig, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilroberta_config = RobertaConfig.from_pretrained(\"distilroberta-base\", output_hidden_states=True)\n",
    "distilroberta_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "50265\n",
      "50265\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "#config = RobertaConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "config.output_hidden_states = True\n",
    "#config.vocab_size = len(tokenizer)\n",
    "config.num_labels = 1\n",
    "\n",
    "teacher_model = RobertaForSequenceClassification(config)\n",
    "#teacher_model.load_state_dict(torch.load('/work/mhessent/master_thesis/eval_out/roberta-base/stsb/lr3e-05_bs16_epochs10/torch_state_dict.pt'))\n",
    "teacher_model.load_state_dict(torch.load('outputs/stsb_teacher_model.pt'))\n",
    "\"\"\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"/work/mhessent/master_thesis/eval_out/bert-base-uncased/mnli/lr3e-05_bs32_epochs3/checkpoint-36816\")\n",
    "torch.save(model.state_dict(), 'outputs/hub_mnli_teacher_model.pt')\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "student_config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "student_config.output_hidden_states = True\n",
    "student_config.num_labels = 1\n",
    "student_config.num_hidden_layers = 3\n",
    "student_config.hidden_dropout_prob = 0.3\n",
    "student_config.attention_probs_dropout_prob = 0.3\n",
    "\n",
    "#student_config.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "student_model = RobertaForSequenceClassification(student_config)\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\n",
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/03/29 18:48:48 - INFO - Distillation -  Training steps per epoch: 180\n",
      "2022/03/29 18:48:48 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/03/29 18:48:48 - INFO - Distillation -  Epoch 1\n",
      "2022/03/29 18:48:48 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:48:49 - INFO - Distillation -  Global step: 9, epoch step:9\n",
      "2022/03/29 18:48:52 - INFO - Distillation -  Global step: 18, epoch step:18\n",
      "2022/03/29 18:48:54 - INFO - Distillation -  Global step: 27, epoch step:27\n",
      "2022/03/29 18:48:56 - INFO - Distillation -  Global step: 36, epoch step:36\n",
      "2022/03/29 18:48:58 - INFO - Distillation -  Global step: 45, epoch step:45\n",
      "2022/03/29 18:49:00 - INFO - Distillation -  Global step: 54, epoch step:54\n",
      "2022/03/29 18:49:02 - INFO - Distillation -  Global step: 63, epoch step:63\n",
      "2022/03/29 18:49:04 - INFO - Distillation -  Global step: 72, epoch step:72\n",
      "2022/03/29 18:49:06 - INFO - Distillation -  Global step: 81, epoch step:81\n",
      "2022/03/29 18:49:08 - INFO - Distillation -  Global step: 90, epoch step:90\n",
      "2022/03/29 18:49:10 - INFO - Distillation -  Global step: 99, epoch step:99\n",
      "2022/03/29 18:49:12 - INFO - Distillation -  Global step: 108, epoch step:108\n",
      "2022/03/29 18:49:14 - INFO - Distillation -  Global step: 117, epoch step:117\n",
      "2022/03/29 18:49:16 - INFO - Distillation -  Global step: 126, epoch step:126\n",
      "2022/03/29 18:49:18 - INFO - Distillation -  Global step: 135, epoch step:135\n",
      "2022/03/29 18:49:20 - INFO - Distillation -  Global step: 144, epoch step:144\n",
      "2022/03/29 18:49:22 - INFO - Distillation -  Global step: 153, epoch step:153\n",
      "2022/03/29 18:49:24 - INFO - Distillation -  Global step: 162, epoch step:162\n",
      "2022/03/29 18:49:26 - INFO - Distillation -  Global step: 171, epoch step:171\n",
      "2022/03/29 18:49:28 - INFO - Distillation -  Global step: 180, epoch step:180\n",
      "2022/03/29 18:49:28 - INFO - Distillation -  Saving at global step 180, epoch step 180 epoch 1\n",
      "2022/03/29 18:49:29 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:49:43 - INFO - Distillation -  {'stsb': {'pearson': -0.02580611495272786, 'spearmanr': -0.01853209753939286}, 'train': {'pearson': 0.09929003399318972, 'spearmanr': 0.09315101740889888}}\n",
      "2022/03/29 18:49:43 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/03/29 18:49:43 - INFO - Distillation -  Epoch 2\n",
      "2022/03/29 18:49:43 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:49:45 - INFO - Distillation -  Global step: 189, epoch step:9\n",
      "2022/03/29 18:49:47 - INFO - Distillation -  Global step: 198, epoch step:18\n",
      "2022/03/29 18:49:49 - INFO - Distillation -  Global step: 207, epoch step:27\n",
      "2022/03/29 18:49:51 - INFO - Distillation -  Global step: 216, epoch step:36\n",
      "2022/03/29 18:49:53 - INFO - Distillation -  Global step: 225, epoch step:45\n",
      "2022/03/29 18:49:55 - INFO - Distillation -  Global step: 234, epoch step:54\n",
      "2022/03/29 18:49:57 - INFO - Distillation -  Global step: 243, epoch step:63\n",
      "2022/03/29 18:49:59 - INFO - Distillation -  Global step: 252, epoch step:72\n",
      "2022/03/29 18:50:01 - INFO - Distillation -  Global step: 261, epoch step:81\n",
      "2022/03/29 18:50:03 - INFO - Distillation -  Global step: 270, epoch step:90\n",
      "2022/03/29 18:50:05 - INFO - Distillation -  Global step: 279, epoch step:99\n",
      "2022/03/29 18:50:07 - INFO - Distillation -  Global step: 288, epoch step:108\n",
      "2022/03/29 18:50:09 - INFO - Distillation -  Global step: 297, epoch step:117\n",
      "2022/03/29 18:50:11 - INFO - Distillation -  Global step: 306, epoch step:126\n",
      "2022/03/29 18:50:13 - INFO - Distillation -  Global step: 315, epoch step:135\n",
      "2022/03/29 18:50:15 - INFO - Distillation -  Global step: 324, epoch step:144\n",
      "2022/03/29 18:50:17 - INFO - Distillation -  Global step: 333, epoch step:153\n",
      "2022/03/29 18:50:19 - INFO - Distillation -  Global step: 342, epoch step:162\n",
      "2022/03/29 18:50:21 - INFO - Distillation -  Global step: 351, epoch step:171\n",
      "2022/03/29 18:50:23 - INFO - Distillation -  Global step: 360, epoch step:180\n",
      "2022/03/29 18:50:23 - INFO - Distillation -  Saving at global step 360, epoch step 180 epoch 2\n",
      "2022/03/29 18:50:24 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:50:38 - INFO - Distillation -  {'stsb': {'pearson': 0.048751494906887405, 'spearmanr': 0.04487404888131848}, 'train': {'pearson': 0.27831550242648956, 'spearmanr': 0.2623480376646477}}\n",
      "2022/03/29 18:50:38 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/03/29 18:50:38 - INFO - Distillation -  Epoch 3\n",
      "2022/03/29 18:50:38 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:50:40 - INFO - Distillation -  Global step: 369, epoch step:9\n",
      "2022/03/29 18:50:42 - INFO - Distillation -  Global step: 378, epoch step:18\n",
      "2022/03/29 18:50:44 - INFO - Distillation -  Global step: 387, epoch step:27\n",
      "2022/03/29 18:50:46 - INFO - Distillation -  Global step: 396, epoch step:36\n",
      "2022/03/29 18:50:48 - INFO - Distillation -  Global step: 405, epoch step:45\n",
      "2022/03/29 18:50:50 - INFO - Distillation -  Global step: 414, epoch step:54\n",
      "2022/03/29 18:50:52 - INFO - Distillation -  Global step: 423, epoch step:63\n",
      "2022/03/29 18:50:54 - INFO - Distillation -  Global step: 432, epoch step:72\n",
      "2022/03/29 18:50:56 - INFO - Distillation -  Global step: 441, epoch step:81\n",
      "2022/03/29 18:50:58 - INFO - Distillation -  Global step: 450, epoch step:90\n",
      "2022/03/29 18:51:00 - INFO - Distillation -  Global step: 459, epoch step:99\n",
      "2022/03/29 18:51:02 - INFO - Distillation -  Global step: 468, epoch step:108\n",
      "2022/03/29 18:51:04 - INFO - Distillation -  Global step: 477, epoch step:117\n",
      "2022/03/29 18:51:06 - INFO - Distillation -  Global step: 486, epoch step:126\n",
      "2022/03/29 18:51:08 - INFO - Distillation -  Global step: 495, epoch step:135\n",
      "2022/03/29 18:51:10 - INFO - Distillation -  Global step: 504, epoch step:144\n",
      "2022/03/29 18:51:12 - INFO - Distillation -  Global step: 513, epoch step:153\n",
      "2022/03/29 18:51:14 - INFO - Distillation -  Global step: 522, epoch step:162\n",
      "2022/03/29 18:51:16 - INFO - Distillation -  Global step: 531, epoch step:171\n",
      "2022/03/29 18:51:18 - INFO - Distillation -  Global step: 540, epoch step:180\n",
      "2022/03/29 18:51:18 - INFO - Distillation -  Saving at global step 540, epoch step 180 epoch 3\n",
      "2022/03/29 18:51:19 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:51:33 - INFO - Distillation -  {'stsb': {'pearson': 0.07614275418836873, 'spearmanr': 0.06956944569744973}, 'train': {'pearson': 0.3097168830281247, 'spearmanr': 0.2928936674989105}}\n",
      "2022/03/29 18:51:33 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/03/29 18:51:33 - INFO - Distillation -  Epoch 4\n",
      "2022/03/29 18:51:33 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:51:35 - INFO - Distillation -  Global step: 549, epoch step:9\n",
      "2022/03/29 18:51:37 - INFO - Distillation -  Global step: 558, epoch step:18\n",
      "2022/03/29 18:51:39 - INFO - Distillation -  Global step: 567, epoch step:27\n",
      "2022/03/29 18:51:41 - INFO - Distillation -  Global step: 576, epoch step:36\n",
      "2022/03/29 18:51:43 - INFO - Distillation -  Global step: 585, epoch step:45\n",
      "2022/03/29 18:51:45 - INFO - Distillation -  Global step: 594, epoch step:54\n",
      "2022/03/29 18:51:47 - INFO - Distillation -  Global step: 603, epoch step:63\n",
      "2022/03/29 18:51:49 - INFO - Distillation -  Global step: 612, epoch step:72\n",
      "2022/03/29 18:51:51 - INFO - Distillation -  Global step: 621, epoch step:81\n",
      "2022/03/29 18:51:53 - INFO - Distillation -  Global step: 630, epoch step:90\n",
      "2022/03/29 18:51:55 - INFO - Distillation -  Global step: 639, epoch step:99\n",
      "2022/03/29 18:51:57 - INFO - Distillation -  Global step: 648, epoch step:108\n",
      "2022/03/29 18:51:59 - INFO - Distillation -  Global step: 657, epoch step:117\n",
      "2022/03/29 18:52:01 - INFO - Distillation -  Global step: 666, epoch step:126\n",
      "2022/03/29 18:52:03 - INFO - Distillation -  Global step: 675, epoch step:135\n",
      "2022/03/29 18:52:06 - INFO - Distillation -  Global step: 684, epoch step:144\n",
      "2022/03/29 18:52:07 - INFO - Distillation -  Global step: 693, epoch step:153\n",
      "2022/03/29 18:52:10 - INFO - Distillation -  Global step: 702, epoch step:162\n",
      "2022/03/29 18:52:12 - INFO - Distillation -  Global step: 711, epoch step:171\n",
      "2022/03/29 18:52:14 - INFO - Distillation -  Global step: 720, epoch step:180\n",
      "2022/03/29 18:52:14 - INFO - Distillation -  Saving at global step 720, epoch step 180 epoch 4\n",
      "2022/03/29 18:52:15 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:52:29 - INFO - Distillation -  {'stsb': {'pearson': 0.10672537030606977, 'spearmanr': 0.10168244073082101}, 'train': {'pearson': 0.31495212193864586, 'spearmanr': 0.30802328916177396}}\n",
      "2022/03/29 18:52:29 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/03/29 18:52:29 - INFO - Distillation -  Epoch 5\n",
      "2022/03/29 18:52:29 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:52:31 - INFO - Distillation -  Global step: 729, epoch step:9\n",
      "2022/03/29 18:52:33 - INFO - Distillation -  Global step: 738, epoch step:18\n",
      "2022/03/29 18:52:35 - INFO - Distillation -  Global step: 747, epoch step:27\n",
      "2022/03/29 18:52:37 - INFO - Distillation -  Global step: 756, epoch step:36\n",
      "2022/03/29 18:52:39 - INFO - Distillation -  Global step: 765, epoch step:45\n",
      "2022/03/29 18:52:41 - INFO - Distillation -  Global step: 774, epoch step:54\n",
      "2022/03/29 18:52:43 - INFO - Distillation -  Global step: 783, epoch step:63\n",
      "2022/03/29 18:52:45 - INFO - Distillation -  Global step: 792, epoch step:72\n",
      "2022/03/29 18:52:47 - INFO - Distillation -  Global step: 801, epoch step:81\n",
      "2022/03/29 18:52:49 - INFO - Distillation -  Global step: 810, epoch step:90\n",
      "2022/03/29 18:52:51 - INFO - Distillation -  Global step: 819, epoch step:99\n",
      "2022/03/29 18:52:53 - INFO - Distillation -  Global step: 828, epoch step:108\n",
      "2022/03/29 18:52:55 - INFO - Distillation -  Global step: 837, epoch step:117\n",
      "2022/03/29 18:52:58 - INFO - Distillation -  Global step: 846, epoch step:126\n",
      "2022/03/29 18:53:00 - INFO - Distillation -  Global step: 855, epoch step:135\n",
      "2022/03/29 18:53:02 - INFO - Distillation -  Global step: 864, epoch step:144\n",
      "2022/03/29 18:53:04 - INFO - Distillation -  Global step: 873, epoch step:153\n",
      "2022/03/29 18:53:06 - INFO - Distillation -  Global step: 882, epoch step:162\n",
      "2022/03/29 18:53:08 - INFO - Distillation -  Global step: 891, epoch step:171\n",
      "2022/03/29 18:53:10 - INFO - Distillation -  Global step: 900, epoch step:180\n",
      "2022/03/29 18:53:10 - INFO - Distillation -  Saving at global step 900, epoch step 180 epoch 5\n",
      "2022/03/29 18:53:15 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:53:29 - INFO - Distillation -  {'stsb': {'pearson': 0.13276645107233537, 'spearmanr': 0.15095303162310292}, 'train': {'pearson': 0.28560625606721685, 'spearmanr': 0.30616964055506957}}\n",
      "2022/03/29 18:53:29 - INFO - Distillation -  Epoch 5 finished\n",
      "2022/03/29 18:53:29 - INFO - Distillation -  Epoch 6\n",
      "2022/03/29 18:53:29 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:53:31 - INFO - Distillation -  Global step: 909, epoch step:9\n",
      "2022/03/29 18:53:33 - INFO - Distillation -  Global step: 918, epoch step:18\n",
      "2022/03/29 18:53:35 - INFO - Distillation -  Global step: 927, epoch step:27\n",
      "2022/03/29 18:53:37 - INFO - Distillation -  Global step: 936, epoch step:36\n",
      "2022/03/29 18:53:39 - INFO - Distillation -  Global step: 945, epoch step:45\n",
      "2022/03/29 18:53:41 - INFO - Distillation -  Global step: 954, epoch step:54\n",
      "2022/03/29 18:53:43 - INFO - Distillation -  Global step: 963, epoch step:63\n",
      "2022/03/29 18:53:45 - INFO - Distillation -  Global step: 972, epoch step:72\n",
      "2022/03/29 18:53:47 - INFO - Distillation -  Global step: 981, epoch step:81\n",
      "2022/03/29 18:53:49 - INFO - Distillation -  Global step: 990, epoch step:90\n",
      "2022/03/29 18:53:52 - INFO - Distillation -  Global step: 999, epoch step:99\n",
      "2022/03/29 18:53:54 - INFO - Distillation -  Global step: 1008, epoch step:108\n",
      "2022/03/29 18:53:56 - INFO - Distillation -  Global step: 1017, epoch step:117\n",
      "2022/03/29 18:53:58 - INFO - Distillation -  Global step: 1026, epoch step:126\n",
      "2022/03/29 18:54:00 - INFO - Distillation -  Global step: 1035, epoch step:135\n",
      "2022/03/29 18:54:02 - INFO - Distillation -  Global step: 1044, epoch step:144\n",
      "2022/03/29 18:54:04 - INFO - Distillation -  Global step: 1053, epoch step:153\n",
      "2022/03/29 18:54:06 - INFO - Distillation -  Global step: 1062, epoch step:162\n",
      "2022/03/29 18:54:08 - INFO - Distillation -  Global step: 1071, epoch step:171\n",
      "2022/03/29 18:54:10 - INFO - Distillation -  Global step: 1080, epoch step:180\n",
      "2022/03/29 18:54:10 - INFO - Distillation -  Saving at global step 1080, epoch step 180 epoch 6\n",
      "2022/03/29 18:54:12 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:54:26 - INFO - Distillation -  {'stsb': {'pearson': 0.15555980334458763, 'spearmanr': 0.16646976627608712}, 'train': {'pearson': 0.28006936707338403, 'spearmanr': 0.2875381437090814}}\n",
      "2022/03/29 18:54:26 - INFO - Distillation -  Epoch 6 finished\n",
      "2022/03/29 18:54:26 - INFO - Distillation -  Epoch 7\n",
      "2022/03/29 18:54:26 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:54:28 - INFO - Distillation -  Global step: 1089, epoch step:9\n",
      "2022/03/29 18:54:30 - INFO - Distillation -  Global step: 1098, epoch step:18\n",
      "2022/03/29 18:54:32 - INFO - Distillation -  Global step: 1107, epoch step:27\n",
      "2022/03/29 18:54:34 - INFO - Distillation -  Global step: 1116, epoch step:36\n",
      "2022/03/29 18:54:36 - INFO - Distillation -  Global step: 1125, epoch step:45\n",
      "2022/03/29 18:54:38 - INFO - Distillation -  Global step: 1134, epoch step:54\n",
      "2022/03/29 18:54:40 - INFO - Distillation -  Global step: 1143, epoch step:63\n",
      "2022/03/29 18:54:42 - INFO - Distillation -  Global step: 1152, epoch step:72\n",
      "2022/03/29 18:54:44 - INFO - Distillation -  Global step: 1161, epoch step:81\n",
      "2022/03/29 18:54:46 - INFO - Distillation -  Global step: 1170, epoch step:90\n",
      "2022/03/29 18:54:48 - INFO - Distillation -  Global step: 1179, epoch step:99\n",
      "2022/03/29 18:54:51 - INFO - Distillation -  Global step: 1188, epoch step:108\n",
      "2022/03/29 18:54:53 - INFO - Distillation -  Global step: 1197, epoch step:117\n",
      "2022/03/29 18:54:55 - INFO - Distillation -  Global step: 1206, epoch step:126\n",
      "2022/03/29 18:54:57 - INFO - Distillation -  Global step: 1215, epoch step:135\n",
      "2022/03/29 18:54:59 - INFO - Distillation -  Global step: 1224, epoch step:144\n",
      "2022/03/29 18:55:01 - INFO - Distillation -  Global step: 1233, epoch step:153\n",
      "2022/03/29 18:55:03 - INFO - Distillation -  Global step: 1242, epoch step:162\n",
      "2022/03/29 18:55:05 - INFO - Distillation -  Global step: 1251, epoch step:171\n",
      "2022/03/29 18:55:07 - INFO - Distillation -  Global step: 1260, epoch step:180\n",
      "2022/03/29 18:55:07 - INFO - Distillation -  Saving at global step 1260, epoch step 180 epoch 7\n",
      "2022/03/29 18:55:09 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:55:23 - INFO - Distillation -  {'stsb': {'pearson': 0.08995399220273662, 'spearmanr': 0.005909875959336249}, 'train': {'pearson': 0.22329687198678513, 'spearmanr': 0.09601561906304486}}\n",
      "2022/03/29 18:55:23 - INFO - Distillation -  Epoch 7 finished\n",
      "2022/03/29 18:55:23 - INFO - Distillation -  Epoch 8\n",
      "2022/03/29 18:55:23 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:55:24 - INFO - Distillation -  Global step: 1269, epoch step:9\n",
      "2022/03/29 18:55:27 - INFO - Distillation -  Global step: 1278, epoch step:18\n",
      "2022/03/29 18:55:29 - INFO - Distillation -  Global step: 1287, epoch step:27\n",
      "2022/03/29 18:55:31 - INFO - Distillation -  Global step: 1296, epoch step:36\n",
      "2022/03/29 18:55:33 - INFO - Distillation -  Global step: 1305, epoch step:45\n",
      "2022/03/29 18:55:35 - INFO - Distillation -  Global step: 1314, epoch step:54\n",
      "2022/03/29 18:55:37 - INFO - Distillation -  Global step: 1323, epoch step:63\n",
      "2022/03/29 18:55:39 - INFO - Distillation -  Global step: 1332, epoch step:72\n",
      "2022/03/29 18:55:41 - INFO - Distillation -  Global step: 1341, epoch step:81\n",
      "2022/03/29 18:55:44 - INFO - Distillation -  Global step: 1350, epoch step:90\n",
      "2022/03/29 18:55:45 - INFO - Distillation -  Global step: 1359, epoch step:99\n",
      "2022/03/29 18:55:48 - INFO - Distillation -  Global step: 1368, epoch step:108\n",
      "2022/03/29 18:55:50 - INFO - Distillation -  Global step: 1377, epoch step:117\n",
      "2022/03/29 18:55:52 - INFO - Distillation -  Global step: 1386, epoch step:126\n",
      "2022/03/29 18:55:54 - INFO - Distillation -  Global step: 1395, epoch step:135\n",
      "2022/03/29 18:55:56 - INFO - Distillation -  Global step: 1404, epoch step:144\n",
      "2022/03/29 18:55:58 - INFO - Distillation -  Global step: 1413, epoch step:153\n",
      "2022/03/29 18:56:00 - INFO - Distillation -  Global step: 1422, epoch step:162\n",
      "2022/03/29 18:56:02 - INFO - Distillation -  Global step: 1431, epoch step:171\n",
      "2022/03/29 18:56:04 - INFO - Distillation -  Global step: 1440, epoch step:180\n",
      "2022/03/29 18:56:04 - INFO - Distillation -  Saving at global step 1440, epoch step 180 epoch 8\n",
      "2022/03/29 18:56:06 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:56:19 - INFO - Distillation -  {'stsb': {'pearson': 0.10161338731500895, 'spearmanr': 0.11979908989855632}, 'train': {'pearson': 0.3553206800259837, 'spearmanr': 0.45009993610938664}}\n",
      "2022/03/29 18:56:19 - INFO - Distillation -  Epoch 8 finished\n",
      "2022/03/29 18:56:19 - INFO - Distillation -  Epoch 9\n",
      "2022/03/29 18:56:19 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:56:21 - INFO - Distillation -  Global step: 1449, epoch step:9\n",
      "2022/03/29 18:56:23 - INFO - Distillation -  Global step: 1458, epoch step:18\n",
      "2022/03/29 18:56:26 - INFO - Distillation -  Global step: 1467, epoch step:27\n",
      "2022/03/29 18:56:27 - INFO - Distillation -  Global step: 1476, epoch step:36\n",
      "2022/03/29 18:56:30 - INFO - Distillation -  Global step: 1485, epoch step:45\n",
      "2022/03/29 18:56:32 - INFO - Distillation -  Global step: 1494, epoch step:54\n",
      "2022/03/29 18:56:34 - INFO - Distillation -  Global step: 1503, epoch step:63\n",
      "2022/03/29 18:56:36 - INFO - Distillation -  Global step: 1512, epoch step:72\n",
      "2022/03/29 18:56:38 - INFO - Distillation -  Global step: 1521, epoch step:81\n",
      "2022/03/29 18:56:40 - INFO - Distillation -  Global step: 1530, epoch step:90\n",
      "2022/03/29 18:56:42 - INFO - Distillation -  Global step: 1539, epoch step:99\n",
      "2022/03/29 18:56:44 - INFO - Distillation -  Global step: 1548, epoch step:108\n",
      "2022/03/29 18:56:46 - INFO - Distillation -  Global step: 1557, epoch step:117\n",
      "2022/03/29 18:56:48 - INFO - Distillation -  Global step: 1566, epoch step:126\n",
      "2022/03/29 18:56:51 - INFO - Distillation -  Global step: 1575, epoch step:135\n",
      "2022/03/29 18:56:52 - INFO - Distillation -  Global step: 1584, epoch step:144\n",
      "2022/03/29 18:56:55 - INFO - Distillation -  Global step: 1593, epoch step:153\n",
      "2022/03/29 18:56:57 - INFO - Distillation -  Global step: 1602, epoch step:162\n",
      "2022/03/29 18:56:59 - INFO - Distillation -  Global step: 1611, epoch step:171\n",
      "2022/03/29 18:57:01 - INFO - Distillation -  Global step: 1620, epoch step:180\n",
      "2022/03/29 18:57:01 - INFO - Distillation -  Saving at global step 1620, epoch step 180 epoch 9\n",
      "2022/03/29 18:57:02 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:57:16 - INFO - Distillation -  {'stsb': {'pearson': 0.21176193383594566, 'spearmanr': 0.18130588118772095}, 'train': {'pearson': 0.49324727942866753, 'spearmanr': 0.5646540083270567}}\n",
      "2022/03/29 18:57:16 - INFO - Distillation -  Epoch 9 finished\n",
      "2022/03/29 18:57:16 - INFO - Distillation -  Epoch 10\n",
      "2022/03/29 18:57:16 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:57:18 - INFO - Distillation -  Global step: 1629, epoch step:9\n",
      "2022/03/29 18:57:21 - INFO - Distillation -  Global step: 1638, epoch step:18\n",
      "2022/03/29 18:57:23 - INFO - Distillation -  Global step: 1647, epoch step:27\n",
      "2022/03/29 18:57:25 - INFO - Distillation -  Global step: 1656, epoch step:36\n",
      "2022/03/29 18:57:27 - INFO - Distillation -  Global step: 1665, epoch step:45\n",
      "2022/03/29 18:57:29 - INFO - Distillation -  Global step: 1674, epoch step:54\n",
      "2022/03/29 18:57:31 - INFO - Distillation -  Global step: 1683, epoch step:63\n",
      "2022/03/29 18:57:33 - INFO - Distillation -  Global step: 1692, epoch step:72\n",
      "2022/03/29 18:57:35 - INFO - Distillation -  Global step: 1701, epoch step:81\n",
      "2022/03/29 18:57:37 - INFO - Distillation -  Global step: 1710, epoch step:90\n",
      "2022/03/29 18:57:39 - INFO - Distillation -  Global step: 1719, epoch step:99\n",
      "2022/03/29 18:57:41 - INFO - Distillation -  Global step: 1728, epoch step:108\n",
      "2022/03/29 18:57:43 - INFO - Distillation -  Global step: 1737, epoch step:117\n",
      "2022/03/29 18:57:45 - INFO - Distillation -  Global step: 1746, epoch step:126\n",
      "2022/03/29 18:57:47 - INFO - Distillation -  Global step: 1755, epoch step:135\n",
      "2022/03/29 18:57:49 - INFO - Distillation -  Global step: 1764, epoch step:144\n",
      "2022/03/29 18:57:51 - INFO - Distillation -  Global step: 1773, epoch step:153\n",
      "2022/03/29 18:57:53 - INFO - Distillation -  Global step: 1782, epoch step:162\n",
      "2022/03/29 18:57:55 - INFO - Distillation -  Global step: 1791, epoch step:171\n",
      "2022/03/29 18:57:57 - INFO - Distillation -  Global step: 1800, epoch step:180\n",
      "2022/03/29 18:57:57 - INFO - Distillation -  Saving at global step 1800, epoch step 180 epoch 10\n",
      "2022/03/29 18:57:59 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:58:13 - INFO - Distillation -  {'stsb': {'pearson': 0.2033288839351179, 'spearmanr': 0.19681377958656066}, 'train': {'pearson': 0.5709345789893647, 'spearmanr': 0.5778042359322153}}\n",
      "2022/03/29 18:58:13 - INFO - Distillation -  Epoch 10 finished\n",
      "2022/03/29 18:58:13 - INFO - Distillation -  Epoch 11\n",
      "2022/03/29 18:58:13 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:58:15 - INFO - Distillation -  Global step: 1809, epoch step:9\n",
      "2022/03/29 18:58:17 - INFO - Distillation -  Global step: 1818, epoch step:18\n",
      "2022/03/29 18:58:19 - INFO - Distillation -  Global step: 1827, epoch step:27\n",
      "2022/03/29 18:58:21 - INFO - Distillation -  Global step: 1836, epoch step:36\n",
      "2022/03/29 18:58:23 - INFO - Distillation -  Global step: 1845, epoch step:45\n",
      "2022/03/29 18:58:25 - INFO - Distillation -  Global step: 1854, epoch step:54\n",
      "2022/03/29 18:58:27 - INFO - Distillation -  Global step: 1863, epoch step:63\n",
      "2022/03/29 18:58:29 - INFO - Distillation -  Global step: 1872, epoch step:72\n",
      "2022/03/29 18:58:31 - INFO - Distillation -  Global step: 1881, epoch step:81\n",
      "2022/03/29 18:58:34 - INFO - Distillation -  Global step: 1890, epoch step:90\n",
      "2022/03/29 18:58:36 - INFO - Distillation -  Global step: 1899, epoch step:99\n",
      "2022/03/29 18:58:38 - INFO - Distillation -  Global step: 1908, epoch step:108\n",
      "2022/03/29 18:58:40 - INFO - Distillation -  Global step: 1917, epoch step:117\n",
      "2022/03/29 18:58:42 - INFO - Distillation -  Global step: 1926, epoch step:126\n",
      "2022/03/29 18:58:44 - INFO - Distillation -  Global step: 1935, epoch step:135\n",
      "2022/03/29 18:58:46 - INFO - Distillation -  Global step: 1944, epoch step:144\n",
      "2022/03/29 18:58:48 - INFO - Distillation -  Global step: 1953, epoch step:153\n",
      "2022/03/29 18:58:50 - INFO - Distillation -  Global step: 1962, epoch step:162\n",
      "2022/03/29 18:58:52 - INFO - Distillation -  Global step: 1971, epoch step:171\n",
      "2022/03/29 18:58:54 - INFO - Distillation -  Global step: 1980, epoch step:180\n",
      "2022/03/29 18:58:54 - INFO - Distillation -  Saving at global step 1980, epoch step 180 epoch 11\n",
      "2022/03/29 18:58:55 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 18:59:09 - INFO - Distillation -  {'stsb': {'pearson': 0.1571672295359926, 'spearmanr': 0.1853614462604848}, 'train': {'pearson': 0.5978876719989239, 'spearmanr': 0.6123350979693905}}\n",
      "2022/03/29 18:59:09 - INFO - Distillation -  Epoch 11 finished\n",
      "2022/03/29 18:59:09 - INFO - Distillation -  Epoch 12\n",
      "2022/03/29 18:59:09 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 18:59:11 - INFO - Distillation -  Global step: 1989, epoch step:9\n",
      "2022/03/29 18:59:13 - INFO - Distillation -  Global step: 1998, epoch step:18\n",
      "2022/03/29 18:59:15 - INFO - Distillation -  Global step: 2007, epoch step:27\n",
      "2022/03/29 18:59:17 - INFO - Distillation -  Global step: 2016, epoch step:36\n",
      "2022/03/29 18:59:19 - INFO - Distillation -  Global step: 2025, epoch step:45\n",
      "2022/03/29 18:59:21 - INFO - Distillation -  Global step: 2034, epoch step:54\n",
      "2022/03/29 18:59:24 - INFO - Distillation -  Global step: 2043, epoch step:63\n",
      "2022/03/29 18:59:26 - INFO - Distillation -  Global step: 2052, epoch step:72\n",
      "2022/03/29 18:59:28 - INFO - Distillation -  Global step: 2061, epoch step:81\n",
      "2022/03/29 18:59:30 - INFO - Distillation -  Global step: 2070, epoch step:90\n",
      "2022/03/29 18:59:32 - INFO - Distillation -  Global step: 2079, epoch step:99\n",
      "2022/03/29 18:59:34 - INFO - Distillation -  Global step: 2088, epoch step:108\n",
      "2022/03/29 18:59:36 - INFO - Distillation -  Global step: 2097, epoch step:117\n",
      "2022/03/29 18:59:38 - INFO - Distillation -  Global step: 2106, epoch step:126\n",
      "2022/03/29 18:59:40 - INFO - Distillation -  Global step: 2115, epoch step:135\n",
      "2022/03/29 18:59:42 - INFO - Distillation -  Global step: 2124, epoch step:144\n",
      "2022/03/29 18:59:44 - INFO - Distillation -  Global step: 2133, epoch step:153\n",
      "2022/03/29 18:59:46 - INFO - Distillation -  Global step: 2142, epoch step:162\n",
      "2022/03/29 18:59:48 - INFO - Distillation -  Global step: 2151, epoch step:171\n",
      "2022/03/29 18:59:50 - INFO - Distillation -  Global step: 2160, epoch step:180\n",
      "2022/03/29 18:59:50 - INFO - Distillation -  Saving at global step 2160, epoch step 180 epoch 12\n",
      "2022/03/29 18:59:51 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:00:05 - INFO - Distillation -  {'stsb': {'pearson': 0.16336960006972578, 'spearmanr': 0.18385043276354535}, 'train': {'pearson': 0.6865123892292626, 'spearmanr': 0.6900738060539938}}\n",
      "2022/03/29 19:00:05 - INFO - Distillation -  Epoch 12 finished\n",
      "2022/03/29 19:00:05 - INFO - Distillation -  Epoch 13\n",
      "2022/03/29 19:00:05 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:00:07 - INFO - Distillation -  Global step: 2169, epoch step:9\n",
      "2022/03/29 19:00:09 - INFO - Distillation -  Global step: 2178, epoch step:18\n",
      "2022/03/29 19:00:11 - INFO - Distillation -  Global step: 2187, epoch step:27\n",
      "2022/03/29 19:00:14 - INFO - Distillation -  Global step: 2196, epoch step:36\n",
      "2022/03/29 19:00:15 - INFO - Distillation -  Global step: 2205, epoch step:45\n",
      "2022/03/29 19:00:18 - INFO - Distillation -  Global step: 2214, epoch step:54\n",
      "2022/03/29 19:00:19 - INFO - Distillation -  Global step: 2223, epoch step:63\n",
      "2022/03/29 19:00:22 - INFO - Distillation -  Global step: 2232, epoch step:72\n",
      "2022/03/29 19:00:23 - INFO - Distillation -  Global step: 2241, epoch step:81\n",
      "2022/03/29 19:00:26 - INFO - Distillation -  Global step: 2250, epoch step:90\n",
      "2022/03/29 19:00:28 - INFO - Distillation -  Global step: 2259, epoch step:99\n",
      "2022/03/29 19:00:30 - INFO - Distillation -  Global step: 2268, epoch step:108\n",
      "2022/03/29 19:00:32 - INFO - Distillation -  Global step: 2277, epoch step:117\n",
      "2022/03/29 19:00:34 - INFO - Distillation -  Global step: 2286, epoch step:126\n",
      "2022/03/29 19:00:36 - INFO - Distillation -  Global step: 2295, epoch step:135\n",
      "2022/03/29 19:00:38 - INFO - Distillation -  Global step: 2304, epoch step:144\n",
      "2022/03/29 19:00:40 - INFO - Distillation -  Global step: 2313, epoch step:153\n",
      "2022/03/29 19:00:42 - INFO - Distillation -  Global step: 2322, epoch step:162\n",
      "2022/03/29 19:00:44 - INFO - Distillation -  Global step: 2331, epoch step:171\n",
      "2022/03/29 19:00:46 - INFO - Distillation -  Global step: 2340, epoch step:180\n",
      "2022/03/29 19:00:46 - INFO - Distillation -  Saving at global step 2340, epoch step 180 epoch 13\n",
      "2022/03/29 19:00:49 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:01:03 - INFO - Distillation -  {'stsb': {'pearson': 0.14219528041498924, 'spearmanr': 0.16729013843168028}, 'train': {'pearson': 0.7231382300457263, 'spearmanr': 0.7122359869248843}}\n",
      "2022/03/29 19:01:03 - INFO - Distillation -  Epoch 13 finished\n",
      "2022/03/29 19:01:03 - INFO - Distillation -  Epoch 14\n",
      "2022/03/29 19:01:03 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:01:05 - INFO - Distillation -  Global step: 2349, epoch step:9\n",
      "2022/03/29 19:01:07 - INFO - Distillation -  Global step: 2358, epoch step:18\n",
      "2022/03/29 19:01:09 - INFO - Distillation -  Global step: 2367, epoch step:27\n",
      "2022/03/29 19:01:11 - INFO - Distillation -  Global step: 2376, epoch step:36\n",
      "2022/03/29 19:01:13 - INFO - Distillation -  Global step: 2385, epoch step:45\n",
      "2022/03/29 19:01:15 - INFO - Distillation -  Global step: 2394, epoch step:54\n",
      "2022/03/29 19:01:17 - INFO - Distillation -  Global step: 2403, epoch step:63\n",
      "2022/03/29 19:01:19 - INFO - Distillation -  Global step: 2412, epoch step:72\n",
      "2022/03/29 19:01:21 - INFO - Distillation -  Global step: 2421, epoch step:81\n",
      "2022/03/29 19:01:23 - INFO - Distillation -  Global step: 2430, epoch step:90\n",
      "2022/03/29 19:01:25 - INFO - Distillation -  Global step: 2439, epoch step:99\n",
      "2022/03/29 19:01:27 - INFO - Distillation -  Global step: 2448, epoch step:108\n",
      "2022/03/29 19:01:29 - INFO - Distillation -  Global step: 2457, epoch step:117\n",
      "2022/03/29 19:01:31 - INFO - Distillation -  Global step: 2466, epoch step:126\n",
      "2022/03/29 19:01:33 - INFO - Distillation -  Global step: 2475, epoch step:135\n",
      "2022/03/29 19:01:35 - INFO - Distillation -  Global step: 2484, epoch step:144\n",
      "2022/03/29 19:01:37 - INFO - Distillation -  Global step: 2493, epoch step:153\n",
      "2022/03/29 19:01:39 - INFO - Distillation -  Global step: 2502, epoch step:162\n",
      "2022/03/29 19:01:41 - INFO - Distillation -  Global step: 2511, epoch step:171\n",
      "2022/03/29 19:01:43 - INFO - Distillation -  Global step: 2520, epoch step:180\n",
      "2022/03/29 19:01:43 - INFO - Distillation -  Saving at global step 2520, epoch step 180 epoch 14\n",
      "2022/03/29 19:01:45 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:01:57 - INFO - Distillation -  {'stsb': {'pearson': 0.15090500460484632, 'spearmanr': 0.16039145321454412}, 'train': {'pearson': 0.7591768863717709, 'spearmanr': 0.76214291266971}}\n",
      "2022/03/29 19:01:57 - INFO - Distillation -  Epoch 14 finished\n",
      "2022/03/29 19:01:57 - INFO - Distillation -  Epoch 15\n",
      "2022/03/29 19:01:57 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:01:58 - INFO - Distillation -  Global step: 2529, epoch step:9\n",
      "2022/03/29 19:02:01 - INFO - Distillation -  Global step: 2538, epoch step:18\n",
      "2022/03/29 19:02:02 - INFO - Distillation -  Global step: 2547, epoch step:27\n",
      "2022/03/29 19:02:05 - INFO - Distillation -  Global step: 2556, epoch step:36\n",
      "2022/03/29 19:02:07 - INFO - Distillation -  Global step: 2565, epoch step:45\n",
      "2022/03/29 19:02:09 - INFO - Distillation -  Global step: 2574, epoch step:54\n",
      "2022/03/29 19:02:11 - INFO - Distillation -  Global step: 2583, epoch step:63\n",
      "2022/03/29 19:02:13 - INFO - Distillation -  Global step: 2592, epoch step:72\n",
      "2022/03/29 19:02:15 - INFO - Distillation -  Global step: 2601, epoch step:81\n",
      "2022/03/29 19:02:17 - INFO - Distillation -  Global step: 2610, epoch step:90\n",
      "2022/03/29 19:02:19 - INFO - Distillation -  Global step: 2619, epoch step:99\n",
      "2022/03/29 19:02:21 - INFO - Distillation -  Global step: 2628, epoch step:108\n",
      "2022/03/29 19:02:23 - INFO - Distillation -  Global step: 2637, epoch step:117\n",
      "2022/03/29 19:02:25 - INFO - Distillation -  Global step: 2646, epoch step:126\n",
      "2022/03/29 19:02:27 - INFO - Distillation -  Global step: 2655, epoch step:135\n",
      "2022/03/29 19:02:29 - INFO - Distillation -  Global step: 2664, epoch step:144\n",
      "2022/03/29 19:02:31 - INFO - Distillation -  Global step: 2673, epoch step:153\n",
      "2022/03/29 19:02:33 - INFO - Distillation -  Global step: 2682, epoch step:162\n",
      "2022/03/29 19:02:35 - INFO - Distillation -  Global step: 2691, epoch step:171\n",
      "2022/03/29 19:02:37 - INFO - Distillation -  Global step: 2700, epoch step:180\n",
      "2022/03/29 19:02:37 - INFO - Distillation -  Saving at global step 2700, epoch step 180 epoch 15\n",
      "2022/03/29 19:02:38 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:02:50 - INFO - Distillation -  {'stsb': {'pearson': 0.15731671084718687, 'spearmanr': 0.15791426542158166}, 'train': {'pearson': 0.7854547561743703, 'spearmanr': 0.7847137265827361}}\n",
      "2022/03/29 19:02:50 - INFO - Distillation -  Epoch 15 finished\n",
      "2022/03/29 19:02:50 - INFO - Distillation -  Epoch 16\n",
      "2022/03/29 19:02:50 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:02:52 - INFO - Distillation -  Global step: 2709, epoch step:9\n",
      "2022/03/29 19:02:54 - INFO - Distillation -  Global step: 2718, epoch step:18\n",
      "2022/03/29 19:02:56 - INFO - Distillation -  Global step: 2727, epoch step:27\n",
      "2022/03/29 19:02:58 - INFO - Distillation -  Global step: 2736, epoch step:36\n",
      "2022/03/29 19:03:00 - INFO - Distillation -  Global step: 2745, epoch step:45\n",
      "2022/03/29 19:03:02 - INFO - Distillation -  Global step: 2754, epoch step:54\n",
      "2022/03/29 19:03:04 - INFO - Distillation -  Global step: 2763, epoch step:63\n",
      "2022/03/29 19:03:06 - INFO - Distillation -  Global step: 2772, epoch step:72\n",
      "2022/03/29 19:03:08 - INFO - Distillation -  Global step: 2781, epoch step:81\n",
      "2022/03/29 19:03:10 - INFO - Distillation -  Global step: 2790, epoch step:90\n",
      "2022/03/29 19:03:12 - INFO - Distillation -  Global step: 2799, epoch step:99\n",
      "2022/03/29 19:03:14 - INFO - Distillation -  Global step: 2808, epoch step:108\n",
      "2022/03/29 19:03:16 - INFO - Distillation -  Global step: 2817, epoch step:117\n",
      "2022/03/29 19:03:18 - INFO - Distillation -  Global step: 2826, epoch step:126\n",
      "2022/03/29 19:03:20 - INFO - Distillation -  Global step: 2835, epoch step:135\n",
      "2022/03/29 19:03:22 - INFO - Distillation -  Global step: 2844, epoch step:144\n",
      "2022/03/29 19:03:24 - INFO - Distillation -  Global step: 2853, epoch step:153\n",
      "2022/03/29 19:03:26 - INFO - Distillation -  Global step: 2862, epoch step:162\n",
      "2022/03/29 19:03:28 - INFO - Distillation -  Global step: 2871, epoch step:171\n",
      "2022/03/29 19:03:30 - INFO - Distillation -  Global step: 2880, epoch step:180\n",
      "2022/03/29 19:03:30 - INFO - Distillation -  Saving at global step 2880, epoch step 180 epoch 16\n",
      "2022/03/29 19:03:32 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:03:44 - INFO - Distillation -  {'stsb': {'pearson': 0.16424561388708067, 'spearmanr': 0.16633854880785245}, 'train': {'pearson': 0.8113381777163952, 'spearmanr': 0.8038311101140743}}\n",
      "2022/03/29 19:03:44 - INFO - Distillation -  Epoch 16 finished\n",
      "2022/03/29 19:03:44 - INFO - Distillation -  Epoch 17\n",
      "2022/03/29 19:03:44 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:03:46 - INFO - Distillation -  Global step: 2889, epoch step:9\n",
      "2022/03/29 19:03:48 - INFO - Distillation -  Global step: 2898, epoch step:18\n",
      "2022/03/29 19:03:50 - INFO - Distillation -  Global step: 2907, epoch step:27\n",
      "2022/03/29 19:03:52 - INFO - Distillation -  Global step: 2916, epoch step:36\n",
      "2022/03/29 19:03:54 - INFO - Distillation -  Global step: 2925, epoch step:45\n",
      "2022/03/29 19:03:56 - INFO - Distillation -  Global step: 2934, epoch step:54\n",
      "2022/03/29 19:03:58 - INFO - Distillation -  Global step: 2943, epoch step:63\n",
      "2022/03/29 19:04:00 - INFO - Distillation -  Global step: 2952, epoch step:72\n",
      "2022/03/29 19:04:02 - INFO - Distillation -  Global step: 2961, epoch step:81\n",
      "2022/03/29 19:04:04 - INFO - Distillation -  Global step: 2970, epoch step:90\n",
      "2022/03/29 19:04:06 - INFO - Distillation -  Global step: 2979, epoch step:99\n",
      "2022/03/29 19:04:08 - INFO - Distillation -  Global step: 2988, epoch step:108\n",
      "2022/03/29 19:04:11 - INFO - Distillation -  Global step: 2997, epoch step:117\n",
      "2022/03/29 19:04:13 - INFO - Distillation -  Global step: 3006, epoch step:126\n",
      "2022/03/29 19:04:15 - INFO - Distillation -  Global step: 3015, epoch step:135\n",
      "2022/03/29 19:04:17 - INFO - Distillation -  Global step: 3024, epoch step:144\n",
      "2022/03/29 19:04:19 - INFO - Distillation -  Global step: 3033, epoch step:153\n",
      "2022/03/29 19:04:21 - INFO - Distillation -  Global step: 3042, epoch step:162\n",
      "2022/03/29 19:04:23 - INFO - Distillation -  Global step: 3051, epoch step:171\n",
      "2022/03/29 19:04:25 - INFO - Distillation -  Global step: 3060, epoch step:180\n",
      "2022/03/29 19:04:25 - INFO - Distillation -  Saving at global step 3060, epoch step 180 epoch 17\n",
      "2022/03/29 19:04:26 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:04:38 - INFO - Distillation -  {'stsb': {'pearson': 0.16064636378911681, 'spearmanr': 0.16687272711196283}, 'train': {'pearson': 0.8174649260202388, 'spearmanr': 0.8126282503997261}}\n",
      "2022/03/29 19:04:38 - INFO - Distillation -  Epoch 17 finished\n",
      "2022/03/29 19:04:38 - INFO - Distillation -  Epoch 18\n",
      "2022/03/29 19:04:38 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:04:40 - INFO - Distillation -  Global step: 3069, epoch step:9\n",
      "2022/03/29 19:04:42 - INFO - Distillation -  Global step: 3078, epoch step:18\n",
      "2022/03/29 19:04:44 - INFO - Distillation -  Global step: 3087, epoch step:27\n",
      "2022/03/29 19:04:46 - INFO - Distillation -  Global step: 3096, epoch step:36\n",
      "2022/03/29 19:04:48 - INFO - Distillation -  Global step: 3105, epoch step:45\n",
      "2022/03/29 19:04:50 - INFO - Distillation -  Global step: 3114, epoch step:54\n",
      "2022/03/29 19:04:52 - INFO - Distillation -  Global step: 3123, epoch step:63\n",
      "2022/03/29 19:04:54 - INFO - Distillation -  Global step: 3132, epoch step:72\n",
      "2022/03/29 19:04:56 - INFO - Distillation -  Global step: 3141, epoch step:81\n",
      "2022/03/29 19:04:58 - INFO - Distillation -  Global step: 3150, epoch step:90\n",
      "2022/03/29 19:05:00 - INFO - Distillation -  Global step: 3159, epoch step:99\n",
      "2022/03/29 19:05:02 - INFO - Distillation -  Global step: 3168, epoch step:108\n",
      "2022/03/29 19:05:04 - INFO - Distillation -  Global step: 3177, epoch step:117\n",
      "2022/03/29 19:05:06 - INFO - Distillation -  Global step: 3186, epoch step:126\n",
      "2022/03/29 19:05:08 - INFO - Distillation -  Global step: 3195, epoch step:135\n",
      "2022/03/29 19:05:10 - INFO - Distillation -  Global step: 3204, epoch step:144\n",
      "2022/03/29 19:05:12 - INFO - Distillation -  Global step: 3213, epoch step:153\n",
      "2022/03/29 19:05:14 - INFO - Distillation -  Global step: 3222, epoch step:162\n",
      "2022/03/29 19:05:16 - INFO - Distillation -  Global step: 3231, epoch step:171\n",
      "2022/03/29 19:05:18 - INFO - Distillation -  Global step: 3240, epoch step:180\n",
      "2022/03/29 19:05:18 - INFO - Distillation -  Saving at global step 3240, epoch step 180 epoch 18\n",
      "2022/03/29 19:05:19 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:05:31 - INFO - Distillation -  {'stsb': {'pearson': 0.14176605313656937, 'spearmanr': 0.1548085408611198}, 'train': {'pearson': 0.824894094349898, 'spearmanr': 0.82133587013753}}\n",
      "2022/03/29 19:05:31 - INFO - Distillation -  Epoch 18 finished\n",
      "2022/03/29 19:05:31 - INFO - Distillation -  Epoch 19\n",
      "2022/03/29 19:05:31 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:05:33 - INFO - Distillation -  Global step: 3249, epoch step:9\n",
      "2022/03/29 19:05:35 - INFO - Distillation -  Global step: 3258, epoch step:18\n",
      "2022/03/29 19:05:37 - INFO - Distillation -  Global step: 3267, epoch step:27\n",
      "2022/03/29 19:05:39 - INFO - Distillation -  Global step: 3276, epoch step:36\n",
      "2022/03/29 19:05:41 - INFO - Distillation -  Global step: 3285, epoch step:45\n",
      "2022/03/29 19:05:43 - INFO - Distillation -  Global step: 3294, epoch step:54\n",
      "2022/03/29 19:05:45 - INFO - Distillation -  Global step: 3303, epoch step:63\n",
      "2022/03/29 19:05:47 - INFO - Distillation -  Global step: 3312, epoch step:72\n",
      "2022/03/29 19:05:49 - INFO - Distillation -  Global step: 3321, epoch step:81\n",
      "2022/03/29 19:05:51 - INFO - Distillation -  Global step: 3330, epoch step:90\n",
      "2022/03/29 19:05:53 - INFO - Distillation -  Global step: 3339, epoch step:99\n",
      "2022/03/29 19:05:55 - INFO - Distillation -  Global step: 3348, epoch step:108\n",
      "2022/03/29 19:05:57 - INFO - Distillation -  Global step: 3357, epoch step:117\n",
      "2022/03/29 19:05:59 - INFO - Distillation -  Global step: 3366, epoch step:126\n",
      "2022/03/29 19:06:01 - INFO - Distillation -  Global step: 3375, epoch step:135\n",
      "2022/03/29 19:06:03 - INFO - Distillation -  Global step: 3384, epoch step:144\n",
      "2022/03/29 19:06:05 - INFO - Distillation -  Global step: 3393, epoch step:153\n",
      "2022/03/29 19:06:07 - INFO - Distillation -  Global step: 3402, epoch step:162\n",
      "2022/03/29 19:06:09 - INFO - Distillation -  Global step: 3411, epoch step:171\n",
      "2022/03/29 19:06:11 - INFO - Distillation -  Global step: 3420, epoch step:180\n",
      "2022/03/29 19:06:11 - INFO - Distillation -  Saving at global step 3420, epoch step 180 epoch 19\n",
      "2022/03/29 19:06:12 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:06:24 - INFO - Distillation -  {'stsb': {'pearson': 0.1510767224881293, 'spearmanr': 0.15521226711802444}, 'train': {'pearson': 0.8201723097264702, 'spearmanr': 0.8247351993616581}}\n",
      "2022/03/29 19:06:24 - INFO - Distillation -  Epoch 19 finished\n",
      "2022/03/29 19:06:24 - INFO - Distillation -  Epoch 20\n",
      "2022/03/29 19:06:24 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:06:26 - INFO - Distillation -  Global step: 3429, epoch step:9\n",
      "2022/03/29 19:06:28 - INFO - Distillation -  Global step: 3438, epoch step:18\n",
      "2022/03/29 19:06:30 - INFO - Distillation -  Global step: 3447, epoch step:27\n",
      "2022/03/29 19:06:32 - INFO - Distillation -  Global step: 3456, epoch step:36\n",
      "2022/03/29 19:06:34 - INFO - Distillation -  Global step: 3465, epoch step:45\n",
      "2022/03/29 19:06:36 - INFO - Distillation -  Global step: 3474, epoch step:54\n",
      "2022/03/29 19:06:38 - INFO - Distillation -  Global step: 3483, epoch step:63\n",
      "2022/03/29 19:06:41 - INFO - Distillation -  Global step: 3492, epoch step:72\n",
      "2022/03/29 19:06:42 - INFO - Distillation -  Global step: 3501, epoch step:81\n",
      "2022/03/29 19:06:45 - INFO - Distillation -  Global step: 3510, epoch step:90\n",
      "2022/03/29 19:06:46 - INFO - Distillation -  Global step: 3519, epoch step:99\n",
      "2022/03/29 19:06:49 - INFO - Distillation -  Global step: 3528, epoch step:108\n",
      "2022/03/29 19:06:50 - INFO - Distillation -  Global step: 3537, epoch step:117\n",
      "2022/03/29 19:06:53 - INFO - Distillation -  Global step: 3546, epoch step:126\n",
      "2022/03/29 19:06:55 - INFO - Distillation -  Global step: 3555, epoch step:135\n",
      "2022/03/29 19:06:57 - INFO - Distillation -  Global step: 3564, epoch step:144\n",
      "2022/03/29 19:06:59 - INFO - Distillation -  Global step: 3573, epoch step:153\n",
      "2022/03/29 19:07:01 - INFO - Distillation -  Global step: 3582, epoch step:162\n",
      "2022/03/29 19:07:03 - INFO - Distillation -  Global step: 3591, epoch step:171\n",
      "2022/03/29 19:07:05 - INFO - Distillation -  Global step: 3600, epoch step:180\n",
      "2022/03/29 19:07:05 - INFO - Distillation -  Saving at global step 3600, epoch step 180 epoch 20\n",
      "2022/03/29 19:07:06 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:07:18 - INFO - Distillation -  {'stsb': {'pearson': 0.1481698193749838, 'spearmanr': 0.15274391310361382}, 'train': {'pearson': 0.8231840858255508, 'spearmanr': 0.8259812011363835}}\n",
      "2022/03/29 19:07:18 - INFO - Distillation -  Epoch 20 finished\n",
      "2022/03/29 19:07:18 - INFO - Distillation -  Epoch 21\n",
      "2022/03/29 19:07:18 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:07:20 - INFO - Distillation -  Global step: 3609, epoch step:9\n",
      "2022/03/29 19:07:22 - INFO - Distillation -  Global step: 3618, epoch step:18\n",
      "2022/03/29 19:07:24 - INFO - Distillation -  Global step: 3627, epoch step:27\n",
      "2022/03/29 19:07:26 - INFO - Distillation -  Global step: 3636, epoch step:36\n",
      "2022/03/29 19:07:28 - INFO - Distillation -  Global step: 3645, epoch step:45\n",
      "2022/03/29 19:07:30 - INFO - Distillation -  Global step: 3654, epoch step:54\n",
      "2022/03/29 19:07:32 - INFO - Distillation -  Global step: 3663, epoch step:63\n",
      "2022/03/29 19:07:34 - INFO - Distillation -  Global step: 3672, epoch step:72\n",
      "2022/03/29 19:07:36 - INFO - Distillation -  Global step: 3681, epoch step:81\n",
      "2022/03/29 19:07:38 - INFO - Distillation -  Global step: 3690, epoch step:90\n",
      "2022/03/29 19:07:41 - INFO - Distillation -  Global step: 3699, epoch step:99\n",
      "2022/03/29 19:07:43 - INFO - Distillation -  Global step: 3708, epoch step:108\n",
      "2022/03/29 19:07:45 - INFO - Distillation -  Global step: 3717, epoch step:117\n",
      "2022/03/29 19:07:47 - INFO - Distillation -  Global step: 3726, epoch step:126\n",
      "2022/03/29 19:07:49 - INFO - Distillation -  Global step: 3735, epoch step:135\n",
      "2022/03/29 19:07:51 - INFO - Distillation -  Global step: 3744, epoch step:144\n",
      "2022/03/29 19:07:53 - INFO - Distillation -  Global step: 3753, epoch step:153\n",
      "2022/03/29 19:07:56 - INFO - Distillation -  Global step: 3762, epoch step:162\n",
      "2022/03/29 19:07:58 - INFO - Distillation -  Global step: 3771, epoch step:171\n",
      "2022/03/29 19:08:00 - INFO - Distillation -  Global step: 3780, epoch step:180\n",
      "2022/03/29 19:08:00 - INFO - Distillation -  Saving at global step 3780, epoch step 180 epoch 21\n",
      "2022/03/29 19:08:01 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:08:12 - INFO - Distillation -  {'stsb': {'pearson': 0.15285054702834344, 'spearmanr': 0.15699544628297343}, 'train': {'pearson': 0.8210471855377934, 'spearmanr': 0.8250624203137371}}\n",
      "2022/03/29 19:08:12 - INFO - Distillation -  Epoch 21 finished\n",
      "2022/03/29 19:08:12 - INFO - Distillation -  Epoch 22\n",
      "2022/03/29 19:08:12 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:08:15 - INFO - Distillation -  Global step: 3789, epoch step:9\n",
      "2022/03/29 19:08:17 - INFO - Distillation -  Global step: 3798, epoch step:18\n",
      "2022/03/29 19:08:19 - INFO - Distillation -  Global step: 3807, epoch step:27\n",
      "2022/03/29 19:08:21 - INFO - Distillation -  Global step: 3816, epoch step:36\n",
      "2022/03/29 19:08:23 - INFO - Distillation -  Global step: 3825, epoch step:45\n",
      "2022/03/29 19:08:25 - INFO - Distillation -  Global step: 3834, epoch step:54\n",
      "2022/03/29 19:08:27 - INFO - Distillation -  Global step: 3843, epoch step:63\n",
      "2022/03/29 19:08:29 - INFO - Distillation -  Global step: 3852, epoch step:72\n",
      "2022/03/29 19:08:31 - INFO - Distillation -  Global step: 3861, epoch step:81\n",
      "2022/03/29 19:08:33 - INFO - Distillation -  Global step: 3870, epoch step:90\n",
      "2022/03/29 19:08:35 - INFO - Distillation -  Global step: 3879, epoch step:99\n",
      "2022/03/29 19:08:37 - INFO - Distillation -  Global step: 3888, epoch step:108\n",
      "2022/03/29 19:08:39 - INFO - Distillation -  Global step: 3897, epoch step:117\n",
      "2022/03/29 19:08:41 - INFO - Distillation -  Global step: 3906, epoch step:126\n",
      "2022/03/29 19:08:43 - INFO - Distillation -  Global step: 3915, epoch step:135\n",
      "2022/03/29 19:08:45 - INFO - Distillation -  Global step: 3924, epoch step:144\n",
      "2022/03/29 19:08:47 - INFO - Distillation -  Global step: 3933, epoch step:153\n",
      "2022/03/29 19:08:49 - INFO - Distillation -  Global step: 3942, epoch step:162\n",
      "2022/03/29 19:08:51 - INFO - Distillation -  Global step: 3951, epoch step:171\n",
      "2022/03/29 19:08:53 - INFO - Distillation -  Global step: 3960, epoch step:180\n",
      "2022/03/29 19:08:53 - INFO - Distillation -  Saving at global step 3960, epoch step 180 epoch 22\n",
      "2022/03/29 19:08:55 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:09:07 - INFO - Distillation -  {'stsb': {'pearson': 0.1477112407849439, 'spearmanr': 0.15558872289350018}, 'train': {'pearson': 0.827041464612334, 'spearmanr': 0.8267075885307043}}\n",
      "2022/03/29 19:09:07 - INFO - Distillation -  Epoch 22 finished\n",
      "2022/03/29 19:09:07 - INFO - Distillation -  Epoch 23\n",
      "2022/03/29 19:09:07 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:09:09 - INFO - Distillation -  Global step: 3969, epoch step:9\n",
      "2022/03/29 19:09:11 - INFO - Distillation -  Global step: 3978, epoch step:18\n",
      "2022/03/29 19:09:14 - INFO - Distillation -  Global step: 3987, epoch step:27\n",
      "2022/03/29 19:09:16 - INFO - Distillation -  Global step: 3996, epoch step:36\n",
      "2022/03/29 19:09:18 - INFO - Distillation -  Global step: 4005, epoch step:45\n",
      "2022/03/29 19:09:20 - INFO - Distillation -  Global step: 4014, epoch step:54\n",
      "2022/03/29 19:09:22 - INFO - Distillation -  Global step: 4023, epoch step:63\n",
      "2022/03/29 19:09:24 - INFO - Distillation -  Global step: 4032, epoch step:72\n",
      "2022/03/29 19:09:27 - INFO - Distillation -  Global step: 4041, epoch step:81\n",
      "2022/03/29 19:09:28 - INFO - Distillation -  Global step: 4050, epoch step:90\n",
      "2022/03/29 19:09:31 - INFO - Distillation -  Global step: 4059, epoch step:99\n",
      "2022/03/29 19:09:32 - INFO - Distillation -  Global step: 4068, epoch step:108\n",
      "2022/03/29 19:09:35 - INFO - Distillation -  Global step: 4077, epoch step:117\n",
      "2022/03/29 19:09:36 - INFO - Distillation -  Global step: 4086, epoch step:126\n",
      "2022/03/29 19:09:39 - INFO - Distillation -  Global step: 4095, epoch step:135\n",
      "2022/03/29 19:09:41 - INFO - Distillation -  Global step: 4104, epoch step:144\n",
      "2022/03/29 19:09:43 - INFO - Distillation -  Global step: 4113, epoch step:153\n",
      "2022/03/29 19:09:45 - INFO - Distillation -  Global step: 4122, epoch step:162\n",
      "2022/03/29 19:09:47 - INFO - Distillation -  Global step: 4131, epoch step:171\n",
      "2022/03/29 19:09:49 - INFO - Distillation -  Global step: 4140, epoch step:180\n",
      "2022/03/29 19:09:49 - INFO - Distillation -  Saving at global step 4140, epoch step 180 epoch 23\n",
      "2022/03/29 19:09:50 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:10:02 - INFO - Distillation -  {'stsb': {'pearson': 0.14911532495131322, 'spearmanr': 0.1562008140626977}, 'train': {'pearson': 0.8365633884138387, 'spearmanr': 0.8303816238188007}}\n",
      "2022/03/29 19:10:02 - INFO - Distillation -  Epoch 23 finished\n",
      "2022/03/29 19:10:02 - INFO - Distillation -  Epoch 24\n",
      "2022/03/29 19:10:02 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:10:04 - INFO - Distillation -  Global step: 4149, epoch step:9\n",
      "2022/03/29 19:10:06 - INFO - Distillation -  Global step: 4158, epoch step:18\n",
      "2022/03/29 19:10:08 - INFO - Distillation -  Global step: 4167, epoch step:27\n",
      "2022/03/29 19:10:10 - INFO - Distillation -  Global step: 4176, epoch step:36\n",
      "2022/03/29 19:10:12 - INFO - Distillation -  Global step: 4185, epoch step:45\n",
      "2022/03/29 19:10:15 - INFO - Distillation -  Global step: 4194, epoch step:54\n",
      "2022/03/29 19:10:17 - INFO - Distillation -  Global step: 4203, epoch step:63\n",
      "2022/03/29 19:10:19 - INFO - Distillation -  Global step: 4212, epoch step:72\n",
      "2022/03/29 19:10:21 - INFO - Distillation -  Global step: 4221, epoch step:81\n",
      "2022/03/29 19:10:23 - INFO - Distillation -  Global step: 4230, epoch step:90\n",
      "2022/03/29 19:10:25 - INFO - Distillation -  Global step: 4239, epoch step:99\n",
      "2022/03/29 19:10:27 - INFO - Distillation -  Global step: 4248, epoch step:108\n",
      "2022/03/29 19:10:29 - INFO - Distillation -  Global step: 4257, epoch step:117\n",
      "2022/03/29 19:10:32 - INFO - Distillation -  Global step: 4266, epoch step:126\n",
      "2022/03/29 19:10:34 - INFO - Distillation -  Global step: 4275, epoch step:135\n",
      "2022/03/29 19:10:36 - INFO - Distillation -  Global step: 4284, epoch step:144\n",
      "2022/03/29 19:10:38 - INFO - Distillation -  Global step: 4293, epoch step:153\n",
      "2022/03/29 19:10:40 - INFO - Distillation -  Global step: 4302, epoch step:162\n",
      "2022/03/29 19:10:42 - INFO - Distillation -  Global step: 4311, epoch step:171\n",
      "2022/03/29 19:10:44 - INFO - Distillation -  Global step: 4320, epoch step:180\n",
      "2022/03/29 19:10:44 - INFO - Distillation -  Saving at global step 4320, epoch step 180 epoch 24\n",
      "2022/03/29 19:10:45 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:10:57 - INFO - Distillation -  {'stsb': {'pearson': 0.13493738379911657, 'spearmanr': 0.14292625865023975}, 'train': {'pearson': 0.8532465019203226, 'spearmanr': 0.8380223777565354}}\n",
      "2022/03/29 19:10:57 - INFO - Distillation -  Epoch 24 finished\n",
      "2022/03/29 19:10:57 - INFO - Distillation -  Epoch 25\n",
      "2022/03/29 19:10:57 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:10:59 - INFO - Distillation -  Global step: 4329, epoch step:9\n",
      "2022/03/29 19:11:01 - INFO - Distillation -  Global step: 4338, epoch step:18\n",
      "2022/03/29 19:11:03 - INFO - Distillation -  Global step: 4347, epoch step:27\n",
      "2022/03/29 19:11:05 - INFO - Distillation -  Global step: 4356, epoch step:36\n",
      "2022/03/29 19:11:07 - INFO - Distillation -  Global step: 4365, epoch step:45\n",
      "2022/03/29 19:11:10 - INFO - Distillation -  Global step: 4374, epoch step:54\n",
      "2022/03/29 19:11:11 - INFO - Distillation -  Global step: 4383, epoch step:63\n",
      "2022/03/29 19:11:14 - INFO - Distillation -  Global step: 4392, epoch step:72\n",
      "2022/03/29 19:11:16 - INFO - Distillation -  Global step: 4401, epoch step:81\n",
      "2022/03/29 19:11:18 - INFO - Distillation -  Global step: 4410, epoch step:90\n",
      "2022/03/29 19:11:20 - INFO - Distillation -  Global step: 4419, epoch step:99\n",
      "2022/03/29 19:11:22 - INFO - Distillation -  Global step: 4428, epoch step:108\n",
      "2022/03/29 19:11:24 - INFO - Distillation -  Global step: 4437, epoch step:117\n",
      "2022/03/29 19:11:26 - INFO - Distillation -  Global step: 4446, epoch step:126\n",
      "2022/03/29 19:11:28 - INFO - Distillation -  Global step: 4455, epoch step:135\n",
      "2022/03/29 19:11:30 - INFO - Distillation -  Global step: 4464, epoch step:144\n",
      "2022/03/29 19:11:32 - INFO - Distillation -  Global step: 4473, epoch step:153\n",
      "2022/03/29 19:11:34 - INFO - Distillation -  Global step: 4482, epoch step:162\n",
      "2022/03/29 19:11:36 - INFO - Distillation -  Global step: 4491, epoch step:171\n",
      "2022/03/29 19:11:38 - INFO - Distillation -  Global step: 4500, epoch step:180\n",
      "2022/03/29 19:11:38 - INFO - Distillation -  Saving at global step 4500, epoch step 180 epoch 25\n",
      "2022/03/29 19:11:39 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:11:51 - INFO - Distillation -  {'stsb': {'pearson': 0.12938475271224395, 'spearmanr': 0.1398582423008206}, 'train': {'pearson': 0.8688758444563921, 'spearmanr': 0.853294012337027}}\n",
      "2022/03/29 19:11:51 - INFO - Distillation -  Epoch 25 finished\n",
      "2022/03/29 19:11:51 - INFO - Distillation -  Epoch 26\n",
      "2022/03/29 19:11:51 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:11:53 - INFO - Distillation -  Global step: 4509, epoch step:9\n",
      "2022/03/29 19:11:55 - INFO - Distillation -  Global step: 4518, epoch step:18\n",
      "2022/03/29 19:11:57 - INFO - Distillation -  Global step: 4527, epoch step:27\n",
      "2022/03/29 19:11:59 - INFO - Distillation -  Global step: 4536, epoch step:36\n",
      "2022/03/29 19:12:01 - INFO - Distillation -  Global step: 4545, epoch step:45\n",
      "2022/03/29 19:12:03 - INFO - Distillation -  Global step: 4554, epoch step:54\n",
      "2022/03/29 19:12:05 - INFO - Distillation -  Global step: 4563, epoch step:63\n",
      "2022/03/29 19:12:07 - INFO - Distillation -  Global step: 4572, epoch step:72\n",
      "2022/03/29 19:12:09 - INFO - Distillation -  Global step: 4581, epoch step:81\n",
      "2022/03/29 19:12:11 - INFO - Distillation -  Global step: 4590, epoch step:90\n",
      "2022/03/29 19:12:13 - INFO - Distillation -  Global step: 4599, epoch step:99\n",
      "2022/03/29 19:12:16 - INFO - Distillation -  Global step: 4608, epoch step:108\n",
      "2022/03/29 19:12:17 - INFO - Distillation -  Global step: 4617, epoch step:117\n",
      "2022/03/29 19:12:20 - INFO - Distillation -  Global step: 4626, epoch step:126\n",
      "2022/03/29 19:12:22 - INFO - Distillation -  Global step: 4635, epoch step:135\n",
      "2022/03/29 19:12:24 - INFO - Distillation -  Global step: 4644, epoch step:144\n",
      "2022/03/29 19:12:26 - INFO - Distillation -  Global step: 4653, epoch step:153\n",
      "2022/03/29 19:12:28 - INFO - Distillation -  Global step: 4662, epoch step:162\n",
      "2022/03/29 19:12:30 - INFO - Distillation -  Global step: 4671, epoch step:171\n",
      "2022/03/29 19:12:32 - INFO - Distillation -  Global step: 4680, epoch step:180\n",
      "2022/03/29 19:12:32 - INFO - Distillation -  Saving at global step 4680, epoch step 180 epoch 26\n",
      "2022/03/29 19:12:33 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:12:44 - INFO - Distillation -  {'stsb': {'pearson': 0.12216983837248555, 'spearmanr': 0.1315764902685324}, 'train': {'pearson': 0.8794752684420621, 'spearmanr': 0.8642360924907615}}\n",
      "2022/03/29 19:12:44 - INFO - Distillation -  Epoch 26 finished\n",
      "2022/03/29 19:12:44 - INFO - Distillation -  Epoch 27\n",
      "2022/03/29 19:12:44 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:12:47 - INFO - Distillation -  Global step: 4689, epoch step:9\n",
      "2022/03/29 19:12:48 - INFO - Distillation -  Global step: 4698, epoch step:18\n",
      "2022/03/29 19:12:51 - INFO - Distillation -  Global step: 4707, epoch step:27\n",
      "2022/03/29 19:12:53 - INFO - Distillation -  Global step: 4716, epoch step:36\n",
      "2022/03/29 19:12:55 - INFO - Distillation -  Global step: 4725, epoch step:45\n",
      "2022/03/29 19:12:57 - INFO - Distillation -  Global step: 4734, epoch step:54\n",
      "2022/03/29 19:12:59 - INFO - Distillation -  Global step: 4743, epoch step:63\n",
      "2022/03/29 19:13:01 - INFO - Distillation -  Global step: 4752, epoch step:72\n",
      "2022/03/29 19:13:03 - INFO - Distillation -  Global step: 4761, epoch step:81\n",
      "2022/03/29 19:13:05 - INFO - Distillation -  Global step: 4770, epoch step:90\n",
      "2022/03/29 19:13:08 - INFO - Distillation -  Global step: 4779, epoch step:99\n",
      "2022/03/29 19:13:10 - INFO - Distillation -  Global step: 4788, epoch step:108\n",
      "2022/03/29 19:13:12 - INFO - Distillation -  Global step: 4797, epoch step:117\n",
      "2022/03/29 19:13:14 - INFO - Distillation -  Global step: 4806, epoch step:126\n",
      "2022/03/29 19:13:16 - INFO - Distillation -  Global step: 4815, epoch step:135\n",
      "2022/03/29 19:13:18 - INFO - Distillation -  Global step: 4824, epoch step:144\n",
      "2022/03/29 19:13:20 - INFO - Distillation -  Global step: 4833, epoch step:153\n",
      "2022/03/29 19:13:22 - INFO - Distillation -  Global step: 4842, epoch step:162\n",
      "2022/03/29 19:13:24 - INFO - Distillation -  Global step: 4851, epoch step:171\n",
      "2022/03/29 19:13:26 - INFO - Distillation -  Global step: 4860, epoch step:180\n",
      "2022/03/29 19:13:26 - INFO - Distillation -  Saving at global step 4860, epoch step 180 epoch 27\n",
      "2022/03/29 19:13:27 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:13:39 - INFO - Distillation -  {'stsb': {'pearson': 0.1153766010044458, 'spearmanr': 0.12489295796506833}, 'train': {'pearson': 0.8884833678877628, 'spearmanr': 0.8770733071451324}}\n",
      "2022/03/29 19:13:39 - INFO - Distillation -  Epoch 27 finished\n",
      "2022/03/29 19:13:39 - INFO - Distillation -  Epoch 28\n",
      "2022/03/29 19:13:39 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:13:41 - INFO - Distillation -  Global step: 4869, epoch step:9\n",
      "2022/03/29 19:13:43 - INFO - Distillation -  Global step: 4878, epoch step:18\n",
      "2022/03/29 19:13:45 - INFO - Distillation -  Global step: 4887, epoch step:27\n",
      "2022/03/29 19:13:48 - INFO - Distillation -  Global step: 4896, epoch step:36\n",
      "2022/03/29 19:13:49 - INFO - Distillation -  Global step: 4905, epoch step:45\n",
      "2022/03/29 19:13:52 - INFO - Distillation -  Global step: 4914, epoch step:54\n",
      "2022/03/29 19:13:54 - INFO - Distillation -  Global step: 4923, epoch step:63\n",
      "2022/03/29 19:13:56 - INFO - Distillation -  Global step: 4932, epoch step:72\n",
      "2022/03/29 19:13:58 - INFO - Distillation -  Global step: 4941, epoch step:81\n",
      "2022/03/29 19:14:00 - INFO - Distillation -  Global step: 4950, epoch step:90\n",
      "2022/03/29 19:14:02 - INFO - Distillation -  Global step: 4959, epoch step:99\n",
      "2022/03/29 19:14:04 - INFO - Distillation -  Global step: 4968, epoch step:108\n",
      "2022/03/29 19:14:06 - INFO - Distillation -  Global step: 4977, epoch step:117\n",
      "2022/03/29 19:14:08 - INFO - Distillation -  Global step: 4986, epoch step:126\n",
      "2022/03/29 19:14:10 - INFO - Distillation -  Global step: 4995, epoch step:135\n",
      "2022/03/29 19:14:12 - INFO - Distillation -  Global step: 5004, epoch step:144\n",
      "2022/03/29 19:14:14 - INFO - Distillation -  Global step: 5013, epoch step:153\n",
      "2022/03/29 19:14:16 - INFO - Distillation -  Global step: 5022, epoch step:162\n",
      "2022/03/29 19:14:18 - INFO - Distillation -  Global step: 5031, epoch step:171\n",
      "2022/03/29 19:14:20 - INFO - Distillation -  Global step: 5040, epoch step:180\n",
      "2022/03/29 19:14:20 - INFO - Distillation -  Saving at global step 5040, epoch step 180 epoch 28\n",
      "2022/03/29 19:14:23 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:14:35 - INFO - Distillation -  {'stsb': {'pearson': 0.12546685545261257, 'spearmanr': 0.13386446592778647}, 'train': {'pearson': 0.8914575417638946, 'spearmanr': 0.8873695968798401}}\n",
      "2022/03/29 19:14:35 - INFO - Distillation -  Epoch 28 finished\n",
      "2022/03/29 19:14:35 - INFO - Distillation -  Epoch 29\n",
      "2022/03/29 19:14:35 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:14:37 - INFO - Distillation -  Global step: 5049, epoch step:9\n",
      "2022/03/29 19:14:39 - INFO - Distillation -  Global step: 5058, epoch step:18\n",
      "2022/03/29 19:14:41 - INFO - Distillation -  Global step: 5067, epoch step:27\n",
      "2022/03/29 19:14:43 - INFO - Distillation -  Global step: 5076, epoch step:36\n",
      "2022/03/29 19:14:45 - INFO - Distillation -  Global step: 5085, epoch step:45\n",
      "2022/03/29 19:14:47 - INFO - Distillation -  Global step: 5094, epoch step:54\n",
      "2022/03/29 19:14:49 - INFO - Distillation -  Global step: 5103, epoch step:63\n",
      "2022/03/29 19:14:51 - INFO - Distillation -  Global step: 5112, epoch step:72\n",
      "2022/03/29 19:14:53 - INFO - Distillation -  Global step: 5121, epoch step:81\n",
      "2022/03/29 19:14:55 - INFO - Distillation -  Global step: 5130, epoch step:90\n",
      "2022/03/29 19:14:57 - INFO - Distillation -  Global step: 5139, epoch step:99\n",
      "2022/03/29 19:14:59 - INFO - Distillation -  Global step: 5148, epoch step:108\n",
      "2022/03/29 19:15:01 - INFO - Distillation -  Global step: 5157, epoch step:117\n",
      "2022/03/29 19:15:03 - INFO - Distillation -  Global step: 5166, epoch step:126\n",
      "2022/03/29 19:15:05 - INFO - Distillation -  Global step: 5175, epoch step:135\n",
      "2022/03/29 19:15:07 - INFO - Distillation -  Global step: 5184, epoch step:144\n",
      "2022/03/29 19:15:10 - INFO - Distillation -  Global step: 5193, epoch step:153\n",
      "2022/03/29 19:15:12 - INFO - Distillation -  Global step: 5202, epoch step:162\n",
      "2022/03/29 19:15:14 - INFO - Distillation -  Global step: 5211, epoch step:171\n",
      "2022/03/29 19:15:16 - INFO - Distillation -  Global step: 5220, epoch step:180\n",
      "2022/03/29 19:15:16 - INFO - Distillation -  Saving at global step 5220, epoch step 180 epoch 29\n",
      "2022/03/29 19:15:18 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:15:30 - INFO - Distillation -  {'stsb': {'pearson': 0.1274304517424769, 'spearmanr': 0.13573968608143902}, 'train': {'pearson': 0.8993368907793745, 'spearmanr': 0.8957295343501926}}\n",
      "2022/03/29 19:15:30 - INFO - Distillation -  Epoch 29 finished\n",
      "2022/03/29 19:15:30 - INFO - Distillation -  Epoch 30\n",
      "2022/03/29 19:15:30 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:15:32 - INFO - Distillation -  Global step: 5229, epoch step:9\n",
      "2022/03/29 19:15:34 - INFO - Distillation -  Global step: 5238, epoch step:18\n",
      "2022/03/29 19:15:36 - INFO - Distillation -  Global step: 5247, epoch step:27\n",
      "2022/03/29 19:15:38 - INFO - Distillation -  Global step: 5256, epoch step:36\n",
      "2022/03/29 19:15:40 - INFO - Distillation -  Global step: 5265, epoch step:45\n",
      "2022/03/29 19:15:42 - INFO - Distillation -  Global step: 5274, epoch step:54\n",
      "2022/03/29 19:15:45 - INFO - Distillation -  Global step: 5283, epoch step:63\n",
      "2022/03/29 19:15:47 - INFO - Distillation -  Global step: 5292, epoch step:72\n",
      "2022/03/29 19:15:49 - INFO - Distillation -  Global step: 5301, epoch step:81\n",
      "2022/03/29 19:15:51 - INFO - Distillation -  Global step: 5310, epoch step:90\n",
      "2022/03/29 19:15:53 - INFO - Distillation -  Global step: 5319, epoch step:99\n",
      "2022/03/29 19:15:55 - INFO - Distillation -  Global step: 5328, epoch step:108\n",
      "2022/03/29 19:15:58 - INFO - Distillation -  Global step: 5337, epoch step:117\n",
      "2022/03/29 19:15:59 - INFO - Distillation -  Global step: 5346, epoch step:126\n",
      "2022/03/29 19:16:02 - INFO - Distillation -  Global step: 5355, epoch step:135\n",
      "2022/03/29 19:16:04 - INFO - Distillation -  Global step: 5364, epoch step:144\n",
      "2022/03/29 19:16:06 - INFO - Distillation -  Global step: 5373, epoch step:153\n",
      "2022/03/29 19:16:08 - INFO - Distillation -  Global step: 5382, epoch step:162\n",
      "2022/03/29 19:16:10 - INFO - Distillation -  Global step: 5391, epoch step:171\n",
      "2022/03/29 19:16:12 - INFO - Distillation -  Global step: 5400, epoch step:180\n",
      "2022/03/29 19:16:12 - INFO - Distillation -  Saving at global step 5400, epoch step 180 epoch 30\n",
      "2022/03/29 19:16:16 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:16:28 - INFO - Distillation -  {'stsb': {'pearson': 0.1323975232732893, 'spearmanr': 0.13619584487988587}, 'train': {'pearson': 0.9040416354837295, 'spearmanr': 0.9045015008789777}}\n",
      "2022/03/29 19:16:28 - INFO - Distillation -  Epoch 30 finished\n",
      "2022/03/29 19:16:28 - INFO - Distillation -  Epoch 31\n",
      "2022/03/29 19:16:28 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:16:30 - INFO - Distillation -  Global step: 5409, epoch step:9\n",
      "2022/03/29 19:16:32 - INFO - Distillation -  Global step: 5418, epoch step:18\n",
      "2022/03/29 19:16:34 - INFO - Distillation -  Global step: 5427, epoch step:27\n",
      "2022/03/29 19:16:36 - INFO - Distillation -  Global step: 5436, epoch step:36\n",
      "2022/03/29 19:16:38 - INFO - Distillation -  Global step: 5445, epoch step:45\n",
      "2022/03/29 19:16:40 - INFO - Distillation -  Global step: 5454, epoch step:54\n",
      "2022/03/29 19:16:42 - INFO - Distillation -  Global step: 5463, epoch step:63\n",
      "2022/03/29 19:16:45 - INFO - Distillation -  Global step: 5472, epoch step:72\n",
      "2022/03/29 19:16:47 - INFO - Distillation -  Global step: 5481, epoch step:81\n",
      "2022/03/29 19:16:49 - INFO - Distillation -  Global step: 5490, epoch step:90\n",
      "2022/03/29 19:16:51 - INFO - Distillation -  Global step: 5499, epoch step:99\n",
      "2022/03/29 19:16:53 - INFO - Distillation -  Global step: 5508, epoch step:108\n",
      "2022/03/29 19:16:55 - INFO - Distillation -  Global step: 5517, epoch step:117\n",
      "2022/03/29 19:16:57 - INFO - Distillation -  Global step: 5526, epoch step:126\n",
      "2022/03/29 19:16:59 - INFO - Distillation -  Global step: 5535, epoch step:135\n",
      "2022/03/29 19:17:02 - INFO - Distillation -  Global step: 5544, epoch step:144\n",
      "2022/03/29 19:17:04 - INFO - Distillation -  Global step: 5553, epoch step:153\n",
      "2022/03/29 19:17:06 - INFO - Distillation -  Global step: 5562, epoch step:162\n",
      "2022/03/29 19:17:08 - INFO - Distillation -  Global step: 5571, epoch step:171\n",
      "2022/03/29 19:17:10 - INFO - Distillation -  Global step: 5580, epoch step:180\n",
      "2022/03/29 19:17:10 - INFO - Distillation -  Saving at global step 5580, epoch step 180 epoch 31\n",
      "2022/03/29 19:17:11 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:17:19 - INFO - Distillation -  {'stsb': {'pearson': 0.13361873404458124, 'spearmanr': 0.13813309443744304}, 'train': {'pearson': 0.9103745380723404, 'spearmanr': 0.9109842610123937}}\n",
      "2022/03/29 19:17:19 - INFO - Distillation -  Epoch 31 finished\n",
      "2022/03/29 19:17:19 - INFO - Distillation -  Epoch 32\n",
      "2022/03/29 19:17:19 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:17:20 - INFO - Distillation -  Global step: 5589, epoch step:9\n",
      "2022/03/29 19:17:22 - INFO - Distillation -  Global step: 5598, epoch step:18\n",
      "2022/03/29 19:17:23 - INFO - Distillation -  Global step: 5607, epoch step:27\n",
      "2022/03/29 19:17:24 - INFO - Distillation -  Global step: 5616, epoch step:36\n",
      "2022/03/29 19:17:25 - INFO - Distillation -  Global step: 5625, epoch step:45\n",
      "2022/03/29 19:17:27 - INFO - Distillation -  Global step: 5634, epoch step:54\n",
      "2022/03/29 19:17:28 - INFO - Distillation -  Global step: 5643, epoch step:63\n",
      "2022/03/29 19:17:29 - INFO - Distillation -  Global step: 5652, epoch step:72\n",
      "2022/03/29 19:17:31 - INFO - Distillation -  Global step: 5661, epoch step:81\n",
      "2022/03/29 19:17:32 - INFO - Distillation -  Global step: 5670, epoch step:90\n",
      "2022/03/29 19:17:33 - INFO - Distillation -  Global step: 5679, epoch step:99\n",
      "2022/03/29 19:17:34 - INFO - Distillation -  Global step: 5688, epoch step:108\n",
      "2022/03/29 19:17:36 - INFO - Distillation -  Global step: 5697, epoch step:117\n",
      "2022/03/29 19:17:37 - INFO - Distillation -  Global step: 5706, epoch step:126\n",
      "2022/03/29 19:17:38 - INFO - Distillation -  Global step: 5715, epoch step:135\n",
      "2022/03/29 19:17:39 - INFO - Distillation -  Global step: 5724, epoch step:144\n",
      "2022/03/29 19:17:40 - INFO - Distillation -  Global step: 5733, epoch step:153\n",
      "2022/03/29 19:17:42 - INFO - Distillation -  Global step: 5742, epoch step:162\n",
      "2022/03/29 19:17:43 - INFO - Distillation -  Global step: 5751, epoch step:171\n",
      "2022/03/29 19:17:44 - INFO - Distillation -  Global step: 5760, epoch step:180\n",
      "2022/03/29 19:17:44 - INFO - Distillation -  Saving at global step 5760, epoch step 180 epoch 32\n",
      "2022/03/29 19:17:46 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:17:52 - INFO - Distillation -  {'stsb': {'pearson': 0.14226146155346867, 'spearmanr': 0.14458683630720157}, 'train': {'pearson': 0.9117369420568839, 'spearmanr': 0.9163708185798228}}\n",
      "2022/03/29 19:17:52 - INFO - Distillation -  Epoch 32 finished\n",
      "2022/03/29 19:17:52 - INFO - Distillation -  Epoch 33\n",
      "2022/03/29 19:17:52 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:17:53 - INFO - Distillation -  Global step: 5769, epoch step:9\n",
      "2022/03/29 19:17:54 - INFO - Distillation -  Global step: 5778, epoch step:18\n",
      "2022/03/29 19:17:56 - INFO - Distillation -  Global step: 5787, epoch step:27\n",
      "2022/03/29 19:17:56 - INFO - Distillation -  Global step: 5796, epoch step:36\n",
      "2022/03/29 19:17:58 - INFO - Distillation -  Global step: 5805, epoch step:45\n",
      "2022/03/29 19:17:59 - INFO - Distillation -  Global step: 5814, epoch step:54\n",
      "2022/03/29 19:18:00 - INFO - Distillation -  Global step: 5823, epoch step:63\n",
      "2022/03/29 19:18:02 - INFO - Distillation -  Global step: 5832, epoch step:72\n",
      "2022/03/29 19:18:03 - INFO - Distillation -  Global step: 5841, epoch step:81\n",
      "2022/03/29 19:18:04 - INFO - Distillation -  Global step: 5850, epoch step:90\n",
      "2022/03/29 19:18:05 - INFO - Distillation -  Global step: 5859, epoch step:99\n",
      "2022/03/29 19:18:07 - INFO - Distillation -  Global step: 5868, epoch step:108\n",
      "2022/03/29 19:18:08 - INFO - Distillation -  Global step: 5877, epoch step:117\n",
      "2022/03/29 19:18:09 - INFO - Distillation -  Global step: 5886, epoch step:126\n",
      "2022/03/29 19:18:10 - INFO - Distillation -  Global step: 5895, epoch step:135\n",
      "2022/03/29 19:18:12 - INFO - Distillation -  Global step: 5904, epoch step:144\n",
      "2022/03/29 19:18:13 - INFO - Distillation -  Global step: 5913, epoch step:153\n",
      "2022/03/29 19:18:14 - INFO - Distillation -  Global step: 5922, epoch step:162\n",
      "2022/03/29 19:18:15 - INFO - Distillation -  Global step: 5931, epoch step:171\n",
      "2022/03/29 19:18:16 - INFO - Distillation -  Global step: 5940, epoch step:180\n",
      "2022/03/29 19:18:16 - INFO - Distillation -  Saving at global step 5940, epoch step 180 epoch 33\n",
      "2022/03/29 19:18:17 - INFO - Distillation -  Running callback function...\n",
      "2022/03/29 19:18:24 - INFO - Distillation -  {'stsb': {'pearson': 0.1374864313174382, 'spearmanr': 0.14004837232510078}, 'train': {'pearson': 0.9157695188012862, 'spearmanr': 0.9158940491103562}}\n",
      "2022/03/29 19:18:24 - INFO - Distillation -  Epoch 33 finished\n",
      "2022/03/29 19:18:24 - INFO - Distillation -  Epoch 34\n",
      "2022/03/29 19:18:24 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/03/29 19:18:25 - INFO - Distillation -  Global step: 5949, epoch step:9\n",
      "2022/03/29 19:18:26 - INFO - Distillation -  Global step: 5958, epoch step:18\n",
      "2022/03/29 19:18:27 - INFO - Distillation -  Global step: 5967, epoch step:27\n",
      "2022/03/29 19:18:29 - INFO - Distillation -  Global step: 5976, epoch step:36\n",
      "2022/03/29 19:18:30 - INFO - Distillation -  Global step: 5985, epoch step:45\n",
      "2022/03/29 19:18:31 - INFO - Distillation -  Global step: 5994, epoch step:54\n",
      "2022/03/29 19:18:32 - INFO - Distillation -  Global step: 6003, epoch step:63\n",
      "2022/03/29 19:18:33 - INFO - Distillation -  Global step: 6012, epoch step:72\n",
      "2022/03/29 19:18:35 - INFO - Distillation -  Global step: 6021, epoch step:81\n",
      "2022/03/29 19:18:35 - INFO - Distillation -  Global step: 6030, epoch step:90\n",
      "2022/03/29 19:18:37 - INFO - Distillation -  Global step: 6039, epoch step:99\n",
      "2022/03/29 19:18:38 - INFO - Distillation -  Global step: 6048, epoch step:108\n",
      "2022/03/29 19:18:40 - INFO - Distillation -  Global step: 6057, epoch step:117\n",
      "2022/03/29 19:18:41 - INFO - Distillation -  Global step: 6066, epoch step:126\n",
      "2022/03/29 19:18:42 - INFO - Distillation -  Global step: 6075, epoch step:135\n",
      "2022/03/29 19:18:43 - INFO - Distillation -  Global step: 6084, epoch step:144\n",
      "2022/03/29 19:18:44 - INFO - Distillation -  Global step: 6093, epoch step:153\n",
      "2022/03/29 19:18:46 - INFO - Distillation -  Global step: 6102, epoch step:162\n",
      "2022/03/29 19:18:47 - INFO - Distillation -  Global step: 6111, epoch step:171\n",
      "2022/03/29 19:18:48 - INFO - Distillation -  Global step: 6120, epoch step:180\n",
      "2022/03/29 19:18:48 - INFO - Distillation -  Saving at global step 6120, epoch step 180 epoch 34\n",
      "2022/03/29 19:18:50 - INFO - Distillation -  Running callback function...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2127944/1976111126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, dataloader, num_epochs, scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain_with_num_epochs\u001b[0;34m(self, optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_epochs, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_epoch_frequency\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_and_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {current_epoch+1} finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_general.py\u001b[0m in \u001b[0;36msave_and_callback\u001b[0;34m(self, global_step, step, epoch, callback)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneralDistiller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_and_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_custom_matches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36msave_and_callback\u001b[0;34m(self, global_step, step, epoch, callback)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running callback function...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_S\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/TextBrewer/examples/notebook_examples/predict_function.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, eval_datasets, step, output_dir, task_name, local_rank, predict_batch_size, device, do_train_eval, train_dataset)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[1;32m     61\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glue\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m          \u001b[0;31m#       print(idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m          \u001b[0;31m#       print(self.dataset[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m          \u001b[0;31m#       print(idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m          \u001b[0;31m#       print(self.dataset[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m         return self._getitem(\n\u001b[0m\u001b[1;32m   1766\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         )\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         )\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mformatted_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"row\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py\u001b[0m in \u001b[0;36mformat_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"torch.Tensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py\u001b[0m in \u001b[0;36mrecursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecursive_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive_tensorize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mnum_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         mapped = [\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         mapped = [\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         ]\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Singleton first to spare some computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py\u001b[0m in \u001b[0;36m_recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mdata_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytorch tensors cannot be instantied from an array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstruct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubstruct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "\n",
    "from matches import matches\n",
    "intermediate_matches = None\n",
    "match_list_L4t = [\"L4t_hidden_mse\", \"L4_hidden_smmd\"]\n",
    "match_list_L3 = [\"L3_hidden_mse\", \"L3_hidden_smmd\"]\n",
    "intermediate_matches = []\n",
    "for match in match_list_L3:\n",
    "        intermediate_matches += matches[match]\n",
    "\n",
    "distill_config = DistillationConfig(kd_loss_type='mse',temperature=4)#,intermediate_matches=intermediate_matches)\n",
    "train_config = TrainingConfig(device=device)\n",
    "\n",
    "\n",
    "\n",
    "task_name = \"stsb\"\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "output_dir = \"outputs/\" + task_name + \"/\" \n",
    "eval_datasets = [val_dataset]\n",
    "do_train_eval = True\n",
    "\n",
    "callback_func = partial(predict, eval_datasets=eval_datasets, output_dir=output_dir,task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device, do_train_eval=do_train_eval, train_dataset=train_dataset)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbrewer.distiller_utils import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [],
   "source": [
    "test_model = RobertaForSequenceClassification(student_config)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs9900.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [],
   "source": [
    "metric= load_metric(\"glue\",\"stsb\")\n",
    "test_model.to(device)\n",
    "test_model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    metric.add_batch(predictions=logits, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher_model = RobertaForSequenceClassification.from_pretrained('/work/mhessent/master_thesis/eval_out/roberta-base/stsb/lr3e-05_bs32_epochs10/checkpoint-1620')\n",
    "metric= load_metric(\"glue\",\"stsb\")\n",
    "#teacher_model.cpu()\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    metric.add_batch(predictions=logits, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
