{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "task_name = \"mnli\"\n",
    "base_model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'mnli', split='train')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_dataset = load_dataset('glue', 'mnli', split='validation_matched')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_mm_dataset = load_dataset('glue', 'mnli', split='validation_mismatched')\n",
    "test_dataset = load_dataset('glue', 'mnli', split='test_matched')#,cache_dir=\"/work/mhessent/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b84661f5fe95e1e2.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43d59c952978c460.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cce2791de0a64e6f.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d933186d36b8e3ab.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "val_mm_dataset = val_mm_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [],
   "source": [
    "#model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "#tokenizer = RobertaTokenizer.from_pretrained(\"/work/mhessent/master_thesis/eval_out/roberta-base/mnli/lr3e-05_bs32_epochs10/checkpoint-49088\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-63d98290670a9a33.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3b3d6e5eb30d55e3.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-53c6e076a5990ca7.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-25162f32847fd610.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_mm_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:][\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    metric = load_metric(\"glue\",\"mnli\")\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=2e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=100,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/mnli_roberta_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer, RobertaConfig, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer, RobertaConfig, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmupInitialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hub_model = RobertaForSequenceClassification.from_pretrained(\"/work/mhessent/master_thesis/eval_out/roberta-base/mnli/lr2e-05_bs32_epochs10/checkpoint-49088\")\n",
    "#torch.save(hub_model.state_dict(), 'outputs/hub_roberta_mnli_teacher_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "50265\n",
      "50265\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "#bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "config.output_hidden_states = True\n",
    "config.num_labels = 3\n",
    "teacher_model = RobertaForSequenceClassification(config) #, num_labels = 2\n",
    "#teacher_model.load_state_dict(torch.load('outputs/mnli_teacher_model.pt'))\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_roberta_mnli_teacher_model.pt'))\n",
    "                             \n",
    "\"\"\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"/work/mhessent/master_thesis/eval_out/bert-base-uncased/mnli/lr3e-05_bs32_epochs3/checkpoint-36816\")\n",
    "torch.save(model.state_dict(), 'outputs/hub_mnli_teacher_model.pt')\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "student_config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "student_config.output_hidden_states = True\n",
    "student_config.num_labels = 3\n",
    "student_config.num_hidden_layers = 3\n",
    "#student_config.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "continue_training = False\n",
    "student_model = RobertaForSequenceClassification(student_config)\n",
    "if continue_training:\n",
    "    student_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs490880.pkl'))\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\n",
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8712175241976566}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "from textbrewer.distiller_utils import move_to_device\n",
    "\n",
    "metric= load_metric(\"glue\",\"mnli\")\n",
    "#teacher_model.cpu()\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2022/04/05 21:51:48 - INFO - Distillation -  Training steps per epoch: 3068\n",
      "2022/04/05 21:51:48 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/04/05 21:51:48 - INFO - Distillation -  Epoch 1\n",
      "2022/04/05 21:51:48 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 21:52:25 - INFO - Distillation -  Global step: 153, epoch step:153\n",
      "2022/04/05 21:53:03 - INFO - Distillation -  Global step: 306, epoch step:306\n",
      "2022/04/05 21:53:42 - INFO - Distillation -  Global step: 459, epoch step:459\n",
      "2022/04/05 21:54:20 - INFO - Distillation -  Global step: 612, epoch step:612\n",
      "2022/04/05 21:54:58 - INFO - Distillation -  Global step: 765, epoch step:765\n",
      "2022/04/05 21:55:36 - INFO - Distillation -  Global step: 918, epoch step:918\n",
      "2022/04/05 21:56:15 - INFO - Distillation -  Global step: 1071, epoch step:1071\n",
      "2022/04/05 21:56:53 - INFO - Distillation -  Global step: 1224, epoch step:1224\n",
      "2022/04/05 21:57:31 - INFO - Distillation -  Global step: 1377, epoch step:1377\n",
      "2022/04/05 21:58:11 - INFO - Distillation -  Global step: 1530, epoch step:1530\n",
      "2022/04/05 21:58:51 - INFO - Distillation -  Global step: 1683, epoch step:1683\n",
      "2022/04/05 21:59:30 - INFO - Distillation -  Global step: 1836, epoch step:1836\n",
      "2022/04/05 22:00:08 - INFO - Distillation -  Global step: 1989, epoch step:1989\n",
      "2022/04/05 22:00:48 - INFO - Distillation -  Global step: 2142, epoch step:2142\n",
      "2022/04/05 22:01:26 - INFO - Distillation -  Global step: 2295, epoch step:2295\n",
      "2022/04/05 22:02:04 - INFO - Distillation -  Global step: 2448, epoch step:2448\n",
      "2022/04/05 22:02:44 - INFO - Distillation -  Global step: 2601, epoch step:2601\n",
      "2022/04/05 22:03:26 - INFO - Distillation -  Global step: 2754, epoch step:2754\n",
      "2022/04/05 22:04:04 - INFO - Distillation -  Global step: 2907, epoch step:2907\n",
      "2022/04/05 22:04:43 - INFO - Distillation -  Global step: 3060, epoch step:3060\n",
      "2022/04/05 22:04:45 - INFO - Distillation -  Saving at global step 3068, epoch step 3068 epoch 1\n",
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/nn/modules/module.py:1385: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "2022/04/05 22:04:46 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 22:08:40 - INFO - Distillation -  {'mnli': {'accuracy': 0.5061640346408558}, 'mnli-mm': {'accuracy': 0.5104759967453214}, 'train': {'accuracy': 0.5086528716431289}}\n",
      "2022/04/05 22:08:40 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/04/05 22:08:40 - INFO - Distillation -  Epoch 2\n",
      "2022/04/05 22:08:40 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 22:09:15 - INFO - Distillation -  Global step: 3213, epoch step:145\n",
      "2022/04/05 22:09:53 - INFO - Distillation -  Global step: 3366, epoch step:298\n",
      "2022/04/05 22:10:31 - INFO - Distillation -  Global step: 3519, epoch step:451\n",
      "2022/04/05 22:11:08 - INFO - Distillation -  Global step: 3672, epoch step:604\n",
      "2022/04/05 22:11:46 - INFO - Distillation -  Global step: 3825, epoch step:757\n",
      "2022/04/05 22:12:23 - INFO - Distillation -  Global step: 3978, epoch step:910\n",
      "2022/04/05 22:13:01 - INFO - Distillation -  Global step: 4131, epoch step:1063\n",
      "2022/04/05 22:13:39 - INFO - Distillation -  Global step: 4284, epoch step:1216\n",
      "2022/04/05 22:14:16 - INFO - Distillation -  Global step: 4437, epoch step:1369\n",
      "2022/04/05 22:14:54 - INFO - Distillation -  Global step: 4590, epoch step:1522\n",
      "2022/04/05 22:15:31 - INFO - Distillation -  Global step: 4743, epoch step:1675\n",
      "2022/04/05 22:16:09 - INFO - Distillation -  Global step: 4896, epoch step:1828\n",
      "2022/04/05 22:16:47 - INFO - Distillation -  Global step: 5049, epoch step:1981\n",
      "2022/04/05 22:17:24 - INFO - Distillation -  Global step: 5202, epoch step:2134\n",
      "2022/04/05 22:18:02 - INFO - Distillation -  Global step: 5355, epoch step:2287\n",
      "2022/04/05 22:18:39 - INFO - Distillation -  Global step: 5508, epoch step:2440\n",
      "2022/04/05 22:19:17 - INFO - Distillation -  Global step: 5661, epoch step:2593\n",
      "2022/04/05 22:19:54 - INFO - Distillation -  Global step: 5814, epoch step:2746\n",
      "2022/04/05 22:20:32 - INFO - Distillation -  Global step: 5967, epoch step:2899\n",
      "2022/04/05 22:21:15 - INFO - Distillation -  Global step: 6120, epoch step:3052\n",
      "2022/04/05 22:21:19 - INFO - Distillation -  Saving at global step 6136, epoch step 3068 epoch 2\n",
      "2022/04/05 22:21:21 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 22:25:08 - INFO - Distillation -  {'mnli': {'accuracy': 0.5380539989811512}, 'mnli-mm': {'accuracy': 0.5426159479251423}, 'train': {'accuracy': 0.558489643546506}}\n",
      "2022/04/05 22:25:08 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/04/05 22:25:08 - INFO - Distillation -  Epoch 3\n",
      "2022/04/05 22:25:08 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 22:25:42 - INFO - Distillation -  Global step: 6273, epoch step:137\n",
      "2022/04/05 22:26:19 - INFO - Distillation -  Global step: 6426, epoch step:290\n",
      "2022/04/05 22:26:56 - INFO - Distillation -  Global step: 6579, epoch step:443\n",
      "2022/04/05 22:27:34 - INFO - Distillation -  Global step: 6732, epoch step:596\n",
      "2022/04/05 22:28:15 - INFO - Distillation -  Global step: 6885, epoch step:749\n",
      "2022/04/05 22:28:52 - INFO - Distillation -  Global step: 7038, epoch step:902\n",
      "2022/04/05 22:29:30 - INFO - Distillation -  Global step: 7191, epoch step:1055\n",
      "2022/04/05 22:30:08 - INFO - Distillation -  Global step: 7344, epoch step:1208\n",
      "2022/04/05 22:30:45 - INFO - Distillation -  Global step: 7497, epoch step:1361\n",
      "2022/04/05 22:31:22 - INFO - Distillation -  Global step: 7650, epoch step:1514\n",
      "2022/04/05 22:32:00 - INFO - Distillation -  Global step: 7803, epoch step:1667\n",
      "2022/04/05 22:32:33 - INFO - Distillation -  Global step: 7956, epoch step:1820\n",
      "2022/04/05 22:33:11 - INFO - Distillation -  Global step: 8109, epoch step:1973\n",
      "2022/04/05 22:33:53 - INFO - Distillation -  Global step: 8262, epoch step:2126\n",
      "2022/04/05 22:34:35 - INFO - Distillation -  Global step: 8415, epoch step:2279\n",
      "2022/04/05 22:35:17 - INFO - Distillation -  Global step: 8568, epoch step:2432\n",
      "2022/04/05 22:35:58 - INFO - Distillation -  Global step: 8721, epoch step:2585\n",
      "2022/04/05 22:36:40 - INFO - Distillation -  Global step: 8874, epoch step:2738\n",
      "2022/04/05 22:37:23 - INFO - Distillation -  Global step: 9027, epoch step:2891\n",
      "2022/04/05 22:38:04 - INFO - Distillation -  Global step: 9180, epoch step:3044\n",
      "2022/04/05 22:38:11 - INFO - Distillation -  Saving at global step 9204, epoch step 3068 epoch 3\n",
      "2022/04/05 22:38:13 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 22:42:40 - INFO - Distillation -  {'mnli': {'accuracy': 0.5511971472236373}, 'mnli-mm': {'accuracy': 0.5487184703010578}, 'train': {'accuracy': 0.5938217783459213}}\n",
      "2022/04/05 22:42:40 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/04/05 22:42:40 - INFO - Distillation -  Epoch 4\n",
      "2022/04/05 22:42:40 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 22:43:18 - INFO - Distillation -  Global step: 9333, epoch step:129\n",
      "2022/04/05 22:44:05 - INFO - Distillation -  Global step: 9486, epoch step:282\n",
      "2022/04/05 22:44:51 - INFO - Distillation -  Global step: 9639, epoch step:435\n",
      "2022/04/05 22:45:37 - INFO - Distillation -  Global step: 9792, epoch step:588\n",
      "2022/04/05 22:46:24 - INFO - Distillation -  Global step: 9945, epoch step:741\n",
      "2022/04/05 22:47:10 - INFO - Distillation -  Global step: 10098, epoch step:894\n",
      "2022/04/05 22:47:57 - INFO - Distillation -  Global step: 10251, epoch step:1047\n",
      "2022/04/05 22:48:44 - INFO - Distillation -  Global step: 10404, epoch step:1200\n",
      "2022/04/05 22:49:30 - INFO - Distillation -  Global step: 10557, epoch step:1353\n",
      "2022/04/05 22:50:16 - INFO - Distillation -  Global step: 10710, epoch step:1506\n",
      "2022/04/05 22:51:03 - INFO - Distillation -  Global step: 10863, epoch step:1659\n",
      "2022/04/05 22:51:49 - INFO - Distillation -  Global step: 11016, epoch step:1812\n",
      "2022/04/05 22:52:35 - INFO - Distillation -  Global step: 11169, epoch step:1965\n",
      "2022/04/05 22:53:22 - INFO - Distillation -  Global step: 11322, epoch step:2118\n",
      "2022/04/05 22:54:08 - INFO - Distillation -  Global step: 11475, epoch step:2271\n",
      "2022/04/05 22:54:55 - INFO - Distillation -  Global step: 11628, epoch step:2424\n",
      "2022/04/05 22:55:41 - INFO - Distillation -  Global step: 11781, epoch step:2577\n",
      "2022/04/05 22:56:27 - INFO - Distillation -  Global step: 11934, epoch step:2730\n",
      "2022/04/05 22:57:14 - INFO - Distillation -  Global step: 12087, epoch step:2883\n",
      "2022/04/05 22:58:00 - INFO - Distillation -  Global step: 12240, epoch step:3036\n",
      "2022/04/05 22:58:10 - INFO - Distillation -  Saving at global step 12272, epoch step 3068 epoch 4\n",
      "2022/04/05 22:58:14 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 23:02:26 - INFO - Distillation -  {'mnli': {'accuracy': 0.5735099337748344}, 'mnli-mm': {'accuracy': 0.5697721724979659}, 'train': {'accuracy': 0.6369155237304622}}\n",
      "2022/04/05 23:02:26 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/04/05 23:02:26 - INFO - Distillation -  Epoch 5\n",
      "2022/04/05 23:02:26 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 23:02:59 - INFO - Distillation -  Global step: 12393, epoch step:121\n",
      "2022/04/05 23:03:41 - INFO - Distillation -  Global step: 12546, epoch step:274\n",
      "2022/04/05 23:04:23 - INFO - Distillation -  Global step: 12699, epoch step:427\n",
      "2022/04/05 23:05:05 - INFO - Distillation -  Global step: 12852, epoch step:580\n",
      "2022/04/05 23:05:47 - INFO - Distillation -  Global step: 13005, epoch step:733\n",
      "2022/04/05 23:06:30 - INFO - Distillation -  Global step: 13158, epoch step:886\n",
      "2022/04/05 23:07:12 - INFO - Distillation -  Global step: 13311, epoch step:1039\n",
      "2022/04/05 23:07:54 - INFO - Distillation -  Global step: 13464, epoch step:1192\n",
      "2022/04/05 23:08:36 - INFO - Distillation -  Global step: 13617, epoch step:1345\n",
      "2022/04/05 23:09:18 - INFO - Distillation -  Global step: 13770, epoch step:1498\n",
      "2022/04/05 23:10:00 - INFO - Distillation -  Global step: 13923, epoch step:1651\n",
      "2022/04/05 23:10:42 - INFO - Distillation -  Global step: 14076, epoch step:1804\n",
      "2022/04/05 23:11:24 - INFO - Distillation -  Global step: 14229, epoch step:1957\n",
      "2022/04/05 23:12:06 - INFO - Distillation -  Global step: 14382, epoch step:2110\n",
      "2022/04/05 23:12:48 - INFO - Distillation -  Global step: 14535, epoch step:2263\n",
      "2022/04/05 23:13:30 - INFO - Distillation -  Global step: 14688, epoch step:2416\n",
      "2022/04/05 23:14:12 - INFO - Distillation -  Global step: 14841, epoch step:2569\n",
      "2022/04/05 23:14:54 - INFO - Distillation -  Global step: 14994, epoch step:2722\n",
      "2022/04/05 23:15:36 - INFO - Distillation -  Global step: 15147, epoch step:2875\n",
      "2022/04/05 23:16:18 - INFO - Distillation -  Global step: 15300, epoch step:3028\n",
      "2022/04/05 23:16:30 - INFO - Distillation -  Saving at global step 15340, epoch step 3068 epoch 5\n",
      "2022/04/05 23:16:33 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 23:21:02 - INFO - Distillation -  {'mnli': {'accuracy': 0.5802343352012226}, 'mnli-mm': {'accuracy': 0.576179820992677}, 'train': {'accuracy': 0.6697062912844854}}\n",
      "2022/04/05 23:21:02 - INFO - Distillation -  Epoch 5 finished\n",
      "2022/04/05 23:21:02 - INFO - Distillation -  Epoch 6\n",
      "2022/04/05 23:21:02 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 23:21:33 - INFO - Distillation -  Global step: 15453, epoch step:113\n",
      "2022/04/05 23:22:14 - INFO - Distillation -  Global step: 15606, epoch step:266\n",
      "2022/04/05 23:22:56 - INFO - Distillation -  Global step: 15759, epoch step:419\n",
      "2022/04/05 23:23:38 - INFO - Distillation -  Global step: 15912, epoch step:572\n",
      "2022/04/05 23:24:20 - INFO - Distillation -  Global step: 16065, epoch step:725\n",
      "2022/04/05 23:25:02 - INFO - Distillation -  Global step: 16218, epoch step:878\n",
      "2022/04/05 23:25:43 - INFO - Distillation -  Global step: 16371, epoch step:1031\n",
      "2022/04/05 23:26:25 - INFO - Distillation -  Global step: 16524, epoch step:1184\n",
      "2022/04/05 23:27:07 - INFO - Distillation -  Global step: 16677, epoch step:1337\n",
      "2022/04/05 23:27:49 - INFO - Distillation -  Global step: 16830, epoch step:1490\n",
      "2022/04/05 23:28:30 - INFO - Distillation -  Global step: 16983, epoch step:1643\n",
      "2022/04/05 23:29:12 - INFO - Distillation -  Global step: 17136, epoch step:1796\n",
      "2022/04/05 23:29:54 - INFO - Distillation -  Global step: 17289, epoch step:1949\n",
      "2022/04/05 23:30:36 - INFO - Distillation -  Global step: 17442, epoch step:2102\n",
      "2022/04/05 23:31:18 - INFO - Distillation -  Global step: 17595, epoch step:2255\n",
      "2022/04/05 23:31:59 - INFO - Distillation -  Global step: 17748, epoch step:2408\n",
      "2022/04/05 23:32:42 - INFO - Distillation -  Global step: 17901, epoch step:2561\n",
      "2022/04/05 23:33:24 - INFO - Distillation -  Global step: 18054, epoch step:2714\n",
      "2022/04/05 23:34:06 - INFO - Distillation -  Global step: 18207, epoch step:2867\n",
      "2022/04/05 23:34:48 - INFO - Distillation -  Global step: 18360, epoch step:3020\n",
      "2022/04/05 23:35:01 - INFO - Distillation -  Saving at global step 18408, epoch step 3068 epoch 6\n",
      "2022/04/05 23:35:03 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 23:39:17 - INFO - Distillation -  {'mnli': {'accuracy': 0.5875700458481915}, 'mnli-mm': {'accuracy': 0.5811635475996745}, 'train': {'accuracy': 0.7071621738621143}}\n",
      "2022/04/05 23:39:17 - INFO - Distillation -  Epoch 6 finished\n",
      "2022/04/05 23:39:17 - INFO - Distillation -  Epoch 7\n",
      "2022/04/05 23:39:17 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 23:39:46 - INFO - Distillation -  Global step: 18513, epoch step:105\n",
      "2022/04/05 23:40:28 - INFO - Distillation -  Global step: 18666, epoch step:258\n",
      "2022/04/05 23:41:10 - INFO - Distillation -  Global step: 18819, epoch step:411\n",
      "2022/04/05 23:41:52 - INFO - Distillation -  Global step: 18972, epoch step:564\n",
      "2022/04/05 23:42:34 - INFO - Distillation -  Global step: 19125, epoch step:717\n",
      "2022/04/05 23:43:16 - INFO - Distillation -  Global step: 19278, epoch step:870\n",
      "2022/04/05 23:43:57 - INFO - Distillation -  Global step: 19431, epoch step:1023\n",
      "2022/04/05 23:44:40 - INFO - Distillation -  Global step: 19584, epoch step:1176\n",
      "2022/04/05 23:45:22 - INFO - Distillation -  Global step: 19737, epoch step:1329\n",
      "2022/04/05 23:46:03 - INFO - Distillation -  Global step: 19890, epoch step:1482\n",
      "2022/04/05 23:46:45 - INFO - Distillation -  Global step: 20043, epoch step:1635\n",
      "2022/04/05 23:47:27 - INFO - Distillation -  Global step: 20196, epoch step:1788\n",
      "2022/04/05 23:48:09 - INFO - Distillation -  Global step: 20349, epoch step:1941\n",
      "2022/04/05 23:48:50 - INFO - Distillation -  Global step: 20502, epoch step:2094\n",
      "2022/04/05 23:49:32 - INFO - Distillation -  Global step: 20655, epoch step:2247\n",
      "2022/04/05 23:50:14 - INFO - Distillation -  Global step: 20808, epoch step:2400\n",
      "2022/04/05 23:50:56 - INFO - Distillation -  Global step: 20961, epoch step:2553\n",
      "2022/04/05 23:51:39 - INFO - Distillation -  Global step: 21114, epoch step:2706\n",
      "2022/04/05 23:52:20 - INFO - Distillation -  Global step: 21267, epoch step:2859\n",
      "2022/04/05 23:53:01 - INFO - Distillation -  Global step: 21420, epoch step:3012\n",
      "2022/04/05 23:53:17 - INFO - Distillation -  Saving at global step 21476, epoch step 3068 epoch 7\n",
      "2022/04/05 23:53:18 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 23:57:23 - INFO - Distillation -  {'mnli': {'accuracy': 0.5871625063678044}, 'mnli-mm': {'accuracy': 0.5838079739625712}, 'train': {'accuracy': 0.7289445941197142}}\n",
      "2022/04/05 23:57:23 - INFO - Distillation -  Epoch 7 finished\n",
      "2022/04/05 23:57:23 - INFO - Distillation -  Epoch 8\n",
      "2022/04/05 23:57:23 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/05 23:57:48 - INFO - Distillation -  Global step: 21573, epoch step:97\n",
      "2022/04/05 23:58:30 - INFO - Distillation -  Global step: 21726, epoch step:250\n",
      "2022/04/05 23:59:12 - INFO - Distillation -  Global step: 21879, epoch step:403\n",
      "2022/04/05 23:59:54 - INFO - Distillation -  Global step: 22032, epoch step:556\n",
      "2022/04/06 00:00:36 - INFO - Distillation -  Global step: 22185, epoch step:709\n",
      "2022/04/06 00:01:18 - INFO - Distillation -  Global step: 22338, epoch step:862\n",
      "2022/04/06 00:01:59 - INFO - Distillation -  Global step: 22491, epoch step:1015\n",
      "2022/04/06 00:02:41 - INFO - Distillation -  Global step: 22644, epoch step:1168\n",
      "2022/04/06 00:03:23 - INFO - Distillation -  Global step: 22797, epoch step:1321\n",
      "2022/04/06 00:04:05 - INFO - Distillation -  Global step: 22950, epoch step:1474\n",
      "2022/04/06 00:04:47 - INFO - Distillation -  Global step: 23103, epoch step:1627\n",
      "2022/04/06 00:05:28 - INFO - Distillation -  Global step: 23256, epoch step:1780\n",
      "2022/04/06 00:06:10 - INFO - Distillation -  Global step: 23409, epoch step:1933\n",
      "2022/04/06 00:06:52 - INFO - Distillation -  Global step: 23562, epoch step:2086\n",
      "2022/04/06 00:07:34 - INFO - Distillation -  Global step: 23715, epoch step:2239\n",
      "2022/04/06 00:08:16 - INFO - Distillation -  Global step: 23868, epoch step:2392\n",
      "2022/04/06 00:08:57 - INFO - Distillation -  Global step: 24021, epoch step:2545\n",
      "2022/04/06 00:09:39 - INFO - Distillation -  Global step: 24174, epoch step:2698\n",
      "2022/04/06 00:10:21 - INFO - Distillation -  Global step: 24327, epoch step:2851\n",
      "2022/04/06 00:11:03 - INFO - Distillation -  Global step: 24480, epoch step:3004\n",
      "2022/04/06 00:11:21 - INFO - Distillation -  Saving at global step 24544, epoch step 3068 epoch 8\n",
      "2022/04/06 00:11:22 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 00:15:29 - INFO - Distillation -  {'mnli': {'accuracy': 0.5963321446765155}, 'mnli-mm': {'accuracy': 0.5998779495524816}, 'train': {'accuracy': 0.7582696293881874}}\n",
      "2022/04/06 00:15:29 - INFO - Distillation -  Epoch 8 finished\n",
      "2022/04/06 00:15:29 - INFO - Distillation -  Epoch 9\n",
      "2022/04/06 00:15:29 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 00:15:54 - INFO - Distillation -  Global step: 24633, epoch step:89\n",
      "2022/04/06 00:16:35 - INFO - Distillation -  Global step: 24786, epoch step:242\n",
      "2022/04/06 00:17:16 - INFO - Distillation -  Global step: 24939, epoch step:395\n",
      "2022/04/06 00:17:58 - INFO - Distillation -  Global step: 25092, epoch step:548\n",
      "2022/04/06 00:18:39 - INFO - Distillation -  Global step: 25245, epoch step:701\n",
      "2022/04/06 00:19:21 - INFO - Distillation -  Global step: 25398, epoch step:854\n",
      "2022/04/06 00:20:03 - INFO - Distillation -  Global step: 25551, epoch step:1007\n",
      "2022/04/06 00:20:46 - INFO - Distillation -  Global step: 25704, epoch step:1160\n",
      "2022/04/06 00:21:29 - INFO - Distillation -  Global step: 25857, epoch step:1313\n",
      "2022/04/06 00:22:11 - INFO - Distillation -  Global step: 26010, epoch step:1466\n",
      "2022/04/06 00:22:53 - INFO - Distillation -  Global step: 26163, epoch step:1619\n",
      "2022/04/06 00:23:37 - INFO - Distillation -  Global step: 26316, epoch step:1772\n",
      "2022/04/06 00:24:18 - INFO - Distillation -  Global step: 26469, epoch step:1925\n",
      "2022/04/06 00:25:00 - INFO - Distillation -  Global step: 26622, epoch step:2078\n",
      "2022/04/06 00:25:42 - INFO - Distillation -  Global step: 26775, epoch step:2231\n",
      "2022/04/06 00:26:24 - INFO - Distillation -  Global step: 26928, epoch step:2384\n",
      "2022/04/06 00:27:06 - INFO - Distillation -  Global step: 27081, epoch step:2537\n",
      "2022/04/06 00:27:48 - INFO - Distillation -  Global step: 27234, epoch step:2690\n",
      "2022/04/06 00:28:30 - INFO - Distillation -  Global step: 27387, epoch step:2843\n",
      "2022/04/06 00:29:12 - INFO - Distillation -  Global step: 27540, epoch step:2996\n",
      "2022/04/06 00:29:32 - INFO - Distillation -  Saving at global step 27612, epoch step 3068 epoch 9\n",
      "2022/04/06 00:29:33 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 00:35:06 - INFO - Distillation -  {'mnli': {'accuracy': 0.6016301579215486}, 'mnli-mm': {'accuracy': 0.5907241659886087}, 'train': {'accuracy': 0.7750839058624606}}\n",
      "2022/04/06 00:35:06 - INFO - Distillation -  Epoch 9 finished\n",
      "2022/04/06 00:35:06 - INFO - Distillation -  Epoch 10\n",
      "2022/04/06 00:35:06 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 00:35:46 - INFO - Distillation -  Global step: 27693, epoch step:81\n",
      "2022/04/06 00:37:01 - INFO - Distillation -  Global step: 27846, epoch step:234\n",
      "2022/04/06 00:38:20 - INFO - Distillation -  Global step: 27999, epoch step:387\n",
      "2022/04/06 00:39:39 - INFO - Distillation -  Global step: 28152, epoch step:540\n",
      "2022/04/06 00:41:22 - INFO - Distillation -  Global step: 28305, epoch step:693\n",
      "2022/04/06 00:43:05 - INFO - Distillation -  Global step: 28458, epoch step:846\n",
      "2022/04/06 00:44:49 - INFO - Distillation -  Global step: 28611, epoch step:999\n",
      "2022/04/06 00:46:31 - INFO - Distillation -  Global step: 28764, epoch step:1152\n",
      "2022/04/06 00:48:09 - INFO - Distillation -  Global step: 28917, epoch step:1305\n",
      "2022/04/06 00:49:52 - INFO - Distillation -  Global step: 29070, epoch step:1458\n",
      "2022/04/06 00:51:35 - INFO - Distillation -  Global step: 29223, epoch step:1611\n",
      "2022/04/06 00:53:18 - INFO - Distillation -  Global step: 29376, epoch step:1764\n",
      "2022/04/06 00:55:01 - INFO - Distillation -  Global step: 29529, epoch step:1917\n",
      "2022/04/06 00:56:44 - INFO - Distillation -  Global step: 29682, epoch step:2070\n",
      "2022/04/06 00:58:27 - INFO - Distillation -  Global step: 29835, epoch step:2223\n",
      "2022/04/06 01:00:10 - INFO - Distillation -  Global step: 29988, epoch step:2376\n",
      "2022/04/06 01:01:53 - INFO - Distillation -  Global step: 30141, epoch step:2529\n",
      "2022/04/06 01:03:36 - INFO - Distillation -  Global step: 30294, epoch step:2682\n",
      "2022/04/06 01:05:05 - INFO - Distillation -  Global step: 30447, epoch step:2835\n",
      "2022/04/06 01:06:30 - INFO - Distillation -  Global step: 30600, epoch step:2988\n",
      "2022/04/06 01:07:15 - INFO - Distillation -  Saving at global step 30680, epoch step 3068 epoch 10\n",
      "2022/04/06 01:07:16 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 01:16:16 - INFO - Distillation -  {'mnli': {'accuracy': 0.5966377992868059}, 'mnli-mm': {'accuracy': 0.5955044751830757}, 'train': {'accuracy': 0.8002582110608044}}\n",
      "2022/04/06 01:16:16 - INFO - Distillation -  Epoch 10 finished\n",
      "2022/04/06 01:16:16 - INFO - Distillation -  Epoch 11\n",
      "2022/04/06 01:16:16 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 01:17:05 - INFO - Distillation -  Global step: 30753, epoch step:73\n",
      "2022/04/06 01:18:45 - INFO - Distillation -  Global step: 30906, epoch step:226\n",
      "2022/04/06 01:20:28 - INFO - Distillation -  Global step: 31059, epoch step:379\n",
      "2022/04/06 01:22:11 - INFO - Distillation -  Global step: 31212, epoch step:532\n",
      "2022/04/06 01:23:54 - INFO - Distillation -  Global step: 31365, epoch step:685\n",
      "2022/04/06 01:25:36 - INFO - Distillation -  Global step: 31518, epoch step:838\n",
      "2022/04/06 01:27:19 - INFO - Distillation -  Global step: 31671, epoch step:991\n",
      "2022/04/06 01:29:03 - INFO - Distillation -  Global step: 31824, epoch step:1144\n",
      "2022/04/06 01:30:36 - INFO - Distillation -  Global step: 31977, epoch step:1297\n",
      "2022/04/06 01:31:54 - INFO - Distillation -  Global step: 32130, epoch step:1450\n",
      "2022/04/06 01:33:12 - INFO - Distillation -  Global step: 32283, epoch step:1603\n",
      "2022/04/06 01:34:31 - INFO - Distillation -  Global step: 32436, epoch step:1756\n",
      "2022/04/06 01:35:50 - INFO - Distillation -  Global step: 32589, epoch step:1909\n",
      "2022/04/06 01:37:08 - INFO - Distillation -  Global step: 32742, epoch step:2062\n",
      "2022/04/06 01:38:24 - INFO - Distillation -  Global step: 32895, epoch step:2215\n",
      "2022/04/06 01:39:43 - INFO - Distillation -  Global step: 33048, epoch step:2368\n",
      "2022/04/06 01:41:03 - INFO - Distillation -  Global step: 33201, epoch step:2521\n",
      "2022/04/06 01:42:21 - INFO - Distillation -  Global step: 33354, epoch step:2674\n",
      "2022/04/06 01:43:22 - INFO - Distillation -  Global step: 33507, epoch step:2827\n",
      "2022/04/06 01:44:22 - INFO - Distillation -  Global step: 33660, epoch step:2980\n",
      "2022/04/06 01:44:56 - INFO - Distillation -  Saving at global step 33748, epoch step 3068 epoch 11\n",
      "2022/04/06 01:44:58 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 01:51:04 - INFO - Distillation -  {'mnli': {'accuracy': 0.5808456444218033}, 'mnli-mm': {'accuracy': 0.5717046379170057}, 'train': {'accuracy': 0.7993440318613096}}\n",
      "2022/04/06 01:51:04 - INFO - Distillation -  Epoch 11 finished\n",
      "2022/04/06 01:51:04 - INFO - Distillation -  Epoch 12\n",
      "2022/04/06 01:51:04 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 01:51:37 - INFO - Distillation -  Global step: 33813, epoch step:65\n",
      "2022/04/06 01:52:56 - INFO - Distillation -  Global step: 33966, epoch step:218\n",
      "2022/04/06 01:54:14 - INFO - Distillation -  Global step: 34119, epoch step:371\n",
      "2022/04/06 01:55:33 - INFO - Distillation -  Global step: 34272, epoch step:524\n",
      "2022/04/06 01:56:52 - INFO - Distillation -  Global step: 34425, epoch step:677\n",
      "2022/04/06 01:58:10 - INFO - Distillation -  Global step: 34578, epoch step:830\n",
      "2022/04/06 01:59:29 - INFO - Distillation -  Global step: 34731, epoch step:983\n",
      "2022/04/06 02:00:48 - INFO - Distillation -  Global step: 34884, epoch step:1136\n",
      "2022/04/06 02:02:06 - INFO - Distillation -  Global step: 35037, epoch step:1289\n",
      "2022/04/06 02:03:24 - INFO - Distillation -  Global step: 35190, epoch step:1442\n",
      "2022/04/06 02:04:41 - INFO - Distillation -  Global step: 35343, epoch step:1595\n",
      "2022/04/06 02:06:00 - INFO - Distillation -  Global step: 35496, epoch step:1748\n",
      "2022/04/06 02:07:19 - INFO - Distillation -  Global step: 35649, epoch step:1901\n",
      "2022/04/06 02:08:37 - INFO - Distillation -  Global step: 35802, epoch step:2054\n",
      "2022/04/06 02:09:54 - INFO - Distillation -  Global step: 35955, epoch step:2207\n",
      "2022/04/06 02:11:12 - INFO - Distillation -  Global step: 36108, epoch step:2360\n",
      "2022/04/06 02:12:31 - INFO - Distillation -  Global step: 36261, epoch step:2513\n",
      "2022/04/06 02:13:50 - INFO - Distillation -  Global step: 36414, epoch step:2666\n",
      "2022/04/06 02:14:54 - INFO - Distillation -  Global step: 36567, epoch step:2819\n",
      "2022/04/06 02:15:54 - INFO - Distillation -  Global step: 36720, epoch step:2972\n",
      "2022/04/06 02:16:32 - INFO - Distillation -  Saving at global step 36816, epoch step 3068 epoch 12\n",
      "2022/04/06 02:16:33 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 02:22:35 - INFO - Distillation -  {'mnli': {'accuracy': 0.5937850229240957}, 'mnli-mm': {'accuracy': 0.5878763222131814}, 'train': {'accuracy': 0.8213021578703444}}\n",
      "2022/04/06 02:22:35 - INFO - Distillation -  Epoch 12 finished\n",
      "2022/04/06 02:22:35 - INFO - Distillation -  Epoch 13\n",
      "2022/04/06 02:22:35 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 02:23:04 - INFO - Distillation -  Global step: 36873, epoch step:57\n",
      "2022/04/06 02:24:23 - INFO - Distillation -  Global step: 37026, epoch step:210\n",
      "2022/04/06 02:25:42 - INFO - Distillation -  Global step: 37179, epoch step:363\n",
      "2022/04/06 02:27:00 - INFO - Distillation -  Global step: 37332, epoch step:516\n",
      "2022/04/06 02:28:18 - INFO - Distillation -  Global step: 37485, epoch step:669\n",
      "2022/04/06 02:29:37 - INFO - Distillation -  Global step: 37638, epoch step:822\n",
      "2022/04/06 02:30:56 - INFO - Distillation -  Global step: 37791, epoch step:975\n",
      "2022/04/06 02:32:16 - INFO - Distillation -  Global step: 37944, epoch step:1128\n",
      "2022/04/06 02:33:34 - INFO - Distillation -  Global step: 38097, epoch step:1281\n",
      "2022/04/06 02:34:53 - INFO - Distillation -  Global step: 38250, epoch step:1434\n",
      "2022/04/06 02:36:11 - INFO - Distillation -  Global step: 38403, epoch step:1587\n",
      "2022/04/06 02:37:30 - INFO - Distillation -  Global step: 38556, epoch step:1740\n",
      "2022/04/06 02:38:47 - INFO - Distillation -  Global step: 38709, epoch step:1893\n",
      "2022/04/06 02:40:06 - INFO - Distillation -  Global step: 38862, epoch step:2046\n",
      "2022/04/06 02:41:25 - INFO - Distillation -  Global step: 39015, epoch step:2199\n",
      "2022/04/06 02:42:43 - INFO - Distillation -  Global step: 39168, epoch step:2352\n",
      "2022/04/06 02:44:03 - INFO - Distillation -  Global step: 39321, epoch step:2505\n",
      "2022/04/06 02:45:21 - INFO - Distillation -  Global step: 39474, epoch step:2658\n",
      "2022/04/06 02:46:24 - INFO - Distillation -  Global step: 39627, epoch step:2811\n",
      "2022/04/06 02:47:24 - INFO - Distillation -  Global step: 39780, epoch step:2964\n",
      "2022/04/06 02:48:05 - INFO - Distillation -  Saving at global step 39884, epoch step 3068 epoch 13\n",
      "2022/04/06 02:48:06 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 02:54:09 - INFO - Distillation -  {'mnli': {'accuracy': 0.5938869077941925}, 'mnli-mm': {'accuracy': 0.5869609438567941}, 'train': {'accuracy': 0.8335480848073094}}\n",
      "2022/04/06 02:54:09 - INFO - Distillation -  Epoch 13 finished\n",
      "2022/04/06 02:54:09 - INFO - Distillation -  Epoch 14\n",
      "2022/04/06 02:54:09 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 02:54:34 - INFO - Distillation -  Global step: 39933, epoch step:49\n",
      "2022/04/06 02:55:51 - INFO - Distillation -  Global step: 40086, epoch step:202\n",
      "2022/04/06 02:57:08 - INFO - Distillation -  Global step: 40239, epoch step:355\n",
      "2022/04/06 02:58:27 - INFO - Distillation -  Global step: 40392, epoch step:508\n",
      "2022/04/06 02:59:44 - INFO - Distillation -  Global step: 40545, epoch step:661\n",
      "2022/04/06 03:01:01 - INFO - Distillation -  Global step: 40698, epoch step:814\n",
      "2022/04/06 03:02:20 - INFO - Distillation -  Global step: 40851, epoch step:967\n",
      "2022/04/06 03:03:39 - INFO - Distillation -  Global step: 41004, epoch step:1120\n",
      "2022/04/06 03:04:57 - INFO - Distillation -  Global step: 41157, epoch step:1273\n",
      "2022/04/06 03:06:16 - INFO - Distillation -  Global step: 41310, epoch step:1426\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "\n",
    "from matches import matches\n",
    "intermediate_matches = None\n",
    "match_list_L4t = [\"L4t_hidden_mse\", \"L4_hidden_smmd\"]\n",
    "match_list_L3 = [\"L3_hidden_mse\", \"L3_hidden_smmd\"]\n",
    "intermediate_matches = []\n",
    "for match in match_list_L3:\n",
    "    intermediate_matches += matches[match]\n",
    "\n",
    "output_dir = \"outputs/\" + base_model_name + \"/\" + task_name + \"/\" + \"hl\"+ str(student_model.config.num_hidden_layers) + \"_hs\" +  str(student_model.config.hidden_size) + \"/\"\n",
    "distill_config = DistillationConfig()\n",
    "    #,intermediate_matches=intermediate_matches)\n",
    "train_config = TrainingConfig(device=device, output_dir = output_dir + \"models/\")\n",
    "\n",
    "\n",
    "\n",
    "task_name = \"mnli\"\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "do_train_eval = True\n",
    "\n",
    "eval_datasets = [val_dataset,val_mm_dataset]\n",
    "\n",
    "callback_func = partial(predict, eval_datasets=eval_datasets, output_dir=output_dir+\"results/\",task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device, do_train_eval=do_train_eval, train_dataset=train_dataset)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [],
   "source": [
    "test_model = RobertaForSequenceClassification(student_config)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs39884.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbrewer.distiller_utils import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_model.to(device)\n",
    "test_model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
