{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "task_name = \"stsb\"\n",
    "base_model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'stsb', split='train')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_dataset = load_dataset('glue', 'stsb', split='validation')#,cache_dir=\"/work/mhessent/cache\")\n",
    "test_dataset = load_dataset('glue', 'stsb', split='test')#,cache_dir=\"/work/mhessent/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2a1905efa4704bcb.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e194e0b596fcf478.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-c97e180e5b68e6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/nn/modules/module.py:1385: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b07cfb65e73d0a7e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4605a63dd35e4960863624919fc196c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2432b3d176f429b6.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['sentence1'],e['sentence2'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    metric = load_metric(\"glue\",\"stsb\")\n",
    "    return metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=3e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=500,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/stsb_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer, RobertaConfig, RobertaForSequenceClassification, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(base_model_name, output_hidden_states=True)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "#config = AutoConfig.from_pretrained(base_model_name, output_hidden_states=True)\n",
    "config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "config.output_hidden_states = True\n",
    "config.vocab_size = len(tokenizer)\n",
    "config.num_labels = 1\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_config(config)\n",
    "#teacher_model.load_state_dict(torch.load('/work/mhessent/master_thesis/eval_out/roberta-base/stsb/lr3e-05_bs16_epochs10/torch_state_dict.pt'))\n",
    "teacher_model.load_state_dict(torch.load('/work/mhessent/master_thesis/eval_out/bert-base-uncased/stsb/lr5e-05_bs32_epochs10/torch_state_dict.pt'))\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "#student_config = AutoConfig.from_pretrained(base_model_name, output_hidden_states=True)\n",
    "student_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config_L3_v2.json')\n",
    "student_config.output_hidden_states = True\n",
    "student_config.num_labels = 1\n",
    "student_config.num_hidden_layers = 3\n",
    "student_config.hidden_dropout_prob = 0.3\n",
    "student_config.attention_probs_dropout_prob = 0.3\n",
    "\n",
    "#student_config.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "student_model = AutoModelForSequenceClassification.from_config(student_config)\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\n",
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/04/05 20:32:04 - INFO - Distillation -  Training steps per epoch: 180\n",
      "2022/04/05 20:32:04 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/04/05 20:32:04 - INFO - Distillation -  Epoch 1\n",
      "2022/04/05 20:32:04 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:32:04 - INFO - Distillation -  Global step: 9, epoch step:9\n",
      "2022/04/05 20:32:05 - INFO - Distillation -  Global step: 18, epoch step:18\n",
      "2022/04/05 20:32:06 - INFO - Distillation -  Global step: 27, epoch step:27\n",
      "2022/04/05 20:32:07 - INFO - Distillation -  Global step: 36, epoch step:36\n",
      "2022/04/05 20:32:08 - INFO - Distillation -  Global step: 45, epoch step:45\n",
      "2022/04/05 20:32:09 - INFO - Distillation -  Global step: 54, epoch step:54\n",
      "2022/04/05 20:32:09 - INFO - Distillation -  Global step: 63, epoch step:63\n",
      "2022/04/05 20:32:10 - INFO - Distillation -  Global step: 72, epoch step:72\n",
      "2022/04/05 20:32:11 - INFO - Distillation -  Global step: 81, epoch step:81\n",
      "2022/04/05 20:32:12 - INFO - Distillation -  Global step: 90, epoch step:90\n",
      "2022/04/05 20:32:12 - INFO - Distillation -  Global step: 99, epoch step:99\n",
      "2022/04/05 20:32:13 - INFO - Distillation -  Global step: 108, epoch step:108\n",
      "2022/04/05 20:32:14 - INFO - Distillation -  Global step: 117, epoch step:117\n",
      "2022/04/05 20:32:15 - INFO - Distillation -  Global step: 126, epoch step:126\n",
      "2022/04/05 20:32:16 - INFO - Distillation -  Global step: 135, epoch step:135\n",
      "2022/04/05 20:32:17 - INFO - Distillation -  Global step: 144, epoch step:144\n",
      "2022/04/05 20:32:17 - INFO - Distillation -  Global step: 153, epoch step:153\n",
      "2022/04/05 20:32:18 - INFO - Distillation -  Global step: 162, epoch step:162\n",
      "2022/04/05 20:32:19 - INFO - Distillation -  Global step: 171, epoch step:171\n",
      "2022/04/05 20:32:20 - INFO - Distillation -  Global step: 180, epoch step:180\n",
      "2022/04/05 20:32:20 - INFO - Distillation -  Saving at global step 180, epoch step 180 epoch 1\n",
      "2022/04/05 20:32:21 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:32:30 - INFO - Distillation -  {'stsb': {'pearson': -0.0009888704329190435, 'spearmanr': 0.004783279991384033}, 'train': {'pearson': 0.20491816101315058, 'spearmanr': 0.19067380864001213}}\n",
      "2022/04/05 20:32:30 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/04/05 20:32:30 - INFO - Distillation -  Epoch 2\n",
      "2022/04/05 20:32:30 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:32:30 - INFO - Distillation -  Global step: 189, epoch step:9\n",
      "2022/04/05 20:32:31 - INFO - Distillation -  Global step: 198, epoch step:18\n",
      "2022/04/05 20:32:32 - INFO - Distillation -  Global step: 207, epoch step:27\n",
      "2022/04/05 20:32:33 - INFO - Distillation -  Global step: 216, epoch step:36\n",
      "2022/04/05 20:32:33 - INFO - Distillation -  Global step: 225, epoch step:45\n",
      "2022/04/05 20:32:34 - INFO - Distillation -  Global step: 234, epoch step:54\n",
      "2022/04/05 20:32:35 - INFO - Distillation -  Global step: 243, epoch step:63\n",
      "2022/04/05 20:32:36 - INFO - Distillation -  Global step: 252, epoch step:72\n",
      "2022/04/05 20:32:36 - INFO - Distillation -  Global step: 261, epoch step:81\n",
      "2022/04/05 20:32:37 - INFO - Distillation -  Global step: 270, epoch step:90\n",
      "2022/04/05 20:32:38 - INFO - Distillation -  Global step: 279, epoch step:99\n",
      "2022/04/05 20:32:39 - INFO - Distillation -  Global step: 288, epoch step:108\n",
      "2022/04/05 20:32:39 - INFO - Distillation -  Global step: 297, epoch step:117\n",
      "2022/04/05 20:32:40 - INFO - Distillation -  Global step: 306, epoch step:126\n",
      "2022/04/05 20:32:41 - INFO - Distillation -  Global step: 315, epoch step:135\n",
      "2022/04/05 20:32:42 - INFO - Distillation -  Global step: 324, epoch step:144\n",
      "2022/04/05 20:32:43 - INFO - Distillation -  Global step: 333, epoch step:153\n",
      "2022/04/05 20:32:44 - INFO - Distillation -  Global step: 342, epoch step:162\n",
      "2022/04/05 20:32:45 - INFO - Distillation -  Global step: 351, epoch step:171\n",
      "2022/04/05 20:32:45 - INFO - Distillation -  Global step: 360, epoch step:180\n",
      "2022/04/05 20:32:45 - INFO - Distillation -  Saving at global step 360, epoch step 180 epoch 2\n",
      "2022/04/05 20:32:47 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:32:53 - INFO - Distillation -  {'stsb': {'pearson': 0.037076797638333416, 'spearmanr': 0.036325996566917276}, 'train': {'pearson': 0.26825909176295915, 'spearmanr': 0.24875970885578982}}\n",
      "2022/04/05 20:32:53 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/04/05 20:32:53 - INFO - Distillation -  Epoch 3\n",
      "2022/04/05 20:32:53 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:32:54 - INFO - Distillation -  Global step: 369, epoch step:9\n",
      "2022/04/05 20:32:54 - INFO - Distillation -  Global step: 378, epoch step:18\n",
      "2022/04/05 20:32:55 - INFO - Distillation -  Global step: 387, epoch step:27\n",
      "2022/04/05 20:32:56 - INFO - Distillation -  Global step: 396, epoch step:36\n",
      "2022/04/05 20:32:57 - INFO - Distillation -  Global step: 405, epoch step:45\n",
      "2022/04/05 20:32:58 - INFO - Distillation -  Global step: 414, epoch step:54\n",
      "2022/04/05 20:32:59 - INFO - Distillation -  Global step: 423, epoch step:63\n",
      "2022/04/05 20:32:59 - INFO - Distillation -  Global step: 432, epoch step:72\n",
      "2022/04/05 20:33:00 - INFO - Distillation -  Global step: 441, epoch step:81\n",
      "2022/04/05 20:33:01 - INFO - Distillation -  Global step: 450, epoch step:90\n",
      "2022/04/05 20:33:02 - INFO - Distillation -  Global step: 459, epoch step:99\n",
      "2022/04/05 20:33:03 - INFO - Distillation -  Global step: 468, epoch step:108\n",
      "2022/04/05 20:33:03 - INFO - Distillation -  Global step: 477, epoch step:117\n",
      "2022/04/05 20:33:04 - INFO - Distillation -  Global step: 486, epoch step:126\n",
      "2022/04/05 20:33:05 - INFO - Distillation -  Global step: 495, epoch step:135\n",
      "2022/04/05 20:33:06 - INFO - Distillation -  Global step: 504, epoch step:144\n",
      "2022/04/05 20:33:07 - INFO - Distillation -  Global step: 513, epoch step:153\n",
      "2022/04/05 20:33:08 - INFO - Distillation -  Global step: 522, epoch step:162\n",
      "2022/04/05 20:33:08 - INFO - Distillation -  Global step: 531, epoch step:171\n",
      "2022/04/05 20:33:09 - INFO - Distillation -  Global step: 540, epoch step:180\n",
      "2022/04/05 20:33:09 - INFO - Distillation -  Saving at global step 540, epoch step 180 epoch 3\n",
      "2022/04/05 20:33:11 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:33:18 - INFO - Distillation -  {'stsb': {'pearson': 0.042294066009138515, 'spearmanr': 0.03572897311904035}, 'train': {'pearson': 0.24793288974303007, 'spearmanr': 0.23786599252530558}}\n",
      "2022/04/05 20:33:18 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/04/05 20:33:18 - INFO - Distillation -  Epoch 4\n",
      "2022/04/05 20:33:18 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:33:19 - INFO - Distillation -  Global step: 549, epoch step:9\n",
      "2022/04/05 20:33:20 - INFO - Distillation -  Global step: 558, epoch step:18\n",
      "2022/04/05 20:33:21 - INFO - Distillation -  Global step: 567, epoch step:27\n",
      "2022/04/05 20:33:22 - INFO - Distillation -  Global step: 576, epoch step:36\n",
      "2022/04/05 20:33:22 - INFO - Distillation -  Global step: 585, epoch step:45\n",
      "2022/04/05 20:33:23 - INFO - Distillation -  Global step: 594, epoch step:54\n",
      "2022/04/05 20:33:24 - INFO - Distillation -  Global step: 603, epoch step:63\n",
      "2022/04/05 20:33:25 - INFO - Distillation -  Global step: 612, epoch step:72\n",
      "2022/04/05 20:33:26 - INFO - Distillation -  Global step: 621, epoch step:81\n",
      "2022/04/05 20:33:27 - INFO - Distillation -  Global step: 630, epoch step:90\n",
      "2022/04/05 20:33:27 - INFO - Distillation -  Global step: 639, epoch step:99\n",
      "2022/04/05 20:33:28 - INFO - Distillation -  Global step: 648, epoch step:108\n",
      "2022/04/05 20:33:29 - INFO - Distillation -  Global step: 657, epoch step:117\n",
      "2022/04/05 20:33:30 - INFO - Distillation -  Global step: 666, epoch step:126\n",
      "2022/04/05 20:33:31 - INFO - Distillation -  Global step: 675, epoch step:135\n",
      "2022/04/05 20:33:31 - INFO - Distillation -  Global step: 684, epoch step:144\n",
      "2022/04/05 20:33:32 - INFO - Distillation -  Global step: 693, epoch step:153\n",
      "2022/04/05 20:33:33 - INFO - Distillation -  Global step: 702, epoch step:162\n",
      "2022/04/05 20:33:34 - INFO - Distillation -  Global step: 711, epoch step:171\n",
      "2022/04/05 20:33:35 - INFO - Distillation -  Global step: 720, epoch step:180\n",
      "2022/04/05 20:33:35 - INFO - Distillation -  Saving at global step 720, epoch step 180 epoch 4\n",
      "2022/04/05 20:33:36 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:33:43 - INFO - Distillation -  {'stsb': {'pearson': 0.04232678111739676, 'spearmanr': 0.04255760503006222}, 'train': {'pearson': 0.2634507389150607, 'spearmanr': 0.243700839122301}}\n",
      "2022/04/05 20:33:43 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/04/05 20:33:43 - INFO - Distillation -  Epoch 5\n",
      "2022/04/05 20:33:43 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:33:44 - INFO - Distillation -  Global step: 729, epoch step:9\n",
      "2022/04/05 20:33:45 - INFO - Distillation -  Global step: 738, epoch step:18\n",
      "2022/04/05 20:33:46 - INFO - Distillation -  Global step: 747, epoch step:27\n",
      "2022/04/05 20:33:47 - INFO - Distillation -  Global step: 756, epoch step:36\n",
      "2022/04/05 20:33:48 - INFO - Distillation -  Global step: 765, epoch step:45\n",
      "2022/04/05 20:33:48 - INFO - Distillation -  Global step: 774, epoch step:54\n",
      "2022/04/05 20:33:49 - INFO - Distillation -  Global step: 783, epoch step:63\n",
      "2022/04/05 20:33:50 - INFO - Distillation -  Global step: 792, epoch step:72\n",
      "2022/04/05 20:33:51 - INFO - Distillation -  Global step: 801, epoch step:81\n",
      "2022/04/05 20:33:52 - INFO - Distillation -  Global step: 810, epoch step:90\n",
      "2022/04/05 20:33:52 - INFO - Distillation -  Global step: 819, epoch step:99\n",
      "2022/04/05 20:33:53 - INFO - Distillation -  Global step: 828, epoch step:108\n",
      "2022/04/05 20:33:54 - INFO - Distillation -  Global step: 837, epoch step:117\n",
      "2022/04/05 20:33:55 - INFO - Distillation -  Global step: 846, epoch step:126\n",
      "2022/04/05 20:33:56 - INFO - Distillation -  Global step: 855, epoch step:135\n",
      "2022/04/05 20:33:57 - INFO - Distillation -  Global step: 864, epoch step:144\n",
      "2022/04/05 20:33:57 - INFO - Distillation -  Global step: 873, epoch step:153\n",
      "2022/04/05 20:33:58 - INFO - Distillation -  Global step: 882, epoch step:162\n",
      "2022/04/05 20:33:59 - INFO - Distillation -  Global step: 891, epoch step:171\n",
      "2022/04/05 20:34:00 - INFO - Distillation -  Global step: 900, epoch step:180\n",
      "2022/04/05 20:34:00 - INFO - Distillation -  Saving at global step 900, epoch step 180 epoch 5\n",
      "2022/04/05 20:34:01 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:34:09 - INFO - Distillation -  {'stsb': {'pearson': 0.0405951328283689, 'spearmanr': 0.04770028834135687}, 'train': {'pearson': 0.27347650737655166, 'spearmanr': 0.2558348013616018}}\n",
      "2022/04/05 20:34:09 - INFO - Distillation -  Epoch 5 finished\n",
      "2022/04/05 20:34:09 - INFO - Distillation -  Epoch 6\n",
      "2022/04/05 20:34:09 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:34:10 - INFO - Distillation -  Global step: 909, epoch step:9\n",
      "2022/04/05 20:34:10 - INFO - Distillation -  Global step: 918, epoch step:18\n",
      "2022/04/05 20:34:11 - INFO - Distillation -  Global step: 927, epoch step:27\n",
      "2022/04/05 20:34:12 - INFO - Distillation -  Global step: 936, epoch step:36\n",
      "2022/04/05 20:34:13 - INFO - Distillation -  Global step: 945, epoch step:45\n",
      "2022/04/05 20:34:14 - INFO - Distillation -  Global step: 954, epoch step:54\n",
      "2022/04/05 20:34:15 - INFO - Distillation -  Global step: 963, epoch step:63\n",
      "2022/04/05 20:34:15 - INFO - Distillation -  Global step: 972, epoch step:72\n",
      "2022/04/05 20:34:16 - INFO - Distillation -  Global step: 981, epoch step:81\n",
      "2022/04/05 20:34:17 - INFO - Distillation -  Global step: 990, epoch step:90\n",
      "2022/04/05 20:34:18 - INFO - Distillation -  Global step: 999, epoch step:99\n",
      "2022/04/05 20:34:19 - INFO - Distillation -  Global step: 1008, epoch step:108\n",
      "2022/04/05 20:34:19 - INFO - Distillation -  Global step: 1017, epoch step:117\n",
      "2022/04/05 20:34:20 - INFO - Distillation -  Global step: 1026, epoch step:126\n",
      "2022/04/05 20:34:21 - INFO - Distillation -  Global step: 1035, epoch step:135\n",
      "2022/04/05 20:34:22 - INFO - Distillation -  Global step: 1044, epoch step:144\n",
      "2022/04/05 20:34:23 - INFO - Distillation -  Global step: 1053, epoch step:153\n",
      "2022/04/05 20:34:24 - INFO - Distillation -  Global step: 1062, epoch step:162\n",
      "2022/04/05 20:34:24 - INFO - Distillation -  Global step: 1071, epoch step:171\n",
      "2022/04/05 20:34:25 - INFO - Distillation -  Global step: 1080, epoch step:180\n",
      "2022/04/05 20:34:25 - INFO - Distillation -  Saving at global step 1080, epoch step 180 epoch 6\n",
      "2022/04/05 20:34:27 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:34:33 - INFO - Distillation -  {'stsb': {'pearson': 0.06544308719514494, 'spearmanr': 0.07000200223245563}, 'train': {'pearson': 0.32971328658327625, 'spearmanr': 0.3117183024320616}}\n",
      "2022/04/05 20:34:33 - INFO - Distillation -  Epoch 6 finished\n",
      "2022/04/05 20:34:33 - INFO - Distillation -  Epoch 7\n",
      "2022/04/05 20:34:33 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:34:34 - INFO - Distillation -  Global step: 1089, epoch step:9\n",
      "2022/04/05 20:34:35 - INFO - Distillation -  Global step: 1098, epoch step:18\n",
      "2022/04/05 20:34:36 - INFO - Distillation -  Global step: 1107, epoch step:27\n",
      "2022/04/05 20:34:37 - INFO - Distillation -  Global step: 1116, epoch step:36\n",
      "2022/04/05 20:34:37 - INFO - Distillation -  Global step: 1125, epoch step:45\n",
      "2022/04/05 20:34:38 - INFO - Distillation -  Global step: 1134, epoch step:54\n",
      "2022/04/05 20:34:39 - INFO - Distillation -  Global step: 1143, epoch step:63\n",
      "2022/04/05 20:34:40 - INFO - Distillation -  Global step: 1152, epoch step:72\n",
      "2022/04/05 20:34:41 - INFO - Distillation -  Global step: 1161, epoch step:81\n",
      "2022/04/05 20:34:41 - INFO - Distillation -  Global step: 1170, epoch step:90\n",
      "2022/04/05 20:34:42 - INFO - Distillation -  Global step: 1179, epoch step:99\n",
      "2022/04/05 20:34:43 - INFO - Distillation -  Global step: 1188, epoch step:108\n",
      "2022/04/05 20:34:44 - INFO - Distillation -  Global step: 1197, epoch step:117\n",
      "2022/04/05 20:34:45 - INFO - Distillation -  Global step: 1206, epoch step:126\n",
      "2022/04/05 20:34:46 - INFO - Distillation -  Global step: 1215, epoch step:135\n",
      "2022/04/05 20:34:46 - INFO - Distillation -  Global step: 1224, epoch step:144\n",
      "2022/04/05 20:34:47 - INFO - Distillation -  Global step: 1233, epoch step:153\n",
      "2022/04/05 20:34:48 - INFO - Distillation -  Global step: 1242, epoch step:162\n",
      "2022/04/05 20:34:49 - INFO - Distillation -  Global step: 1251, epoch step:171\n",
      "2022/04/05 20:34:50 - INFO - Distillation -  Global step: 1260, epoch step:180\n",
      "2022/04/05 20:34:50 - INFO - Distillation -  Saving at global step 1260, epoch step 180 epoch 7\n",
      "2022/04/05 20:34:52 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:34:59 - INFO - Distillation -  {'stsb': {'pearson': 0.08071376572267291, 'spearmanr': 0.08365437623611596}, 'train': {'pearson': 0.40277297978975796, 'spearmanr': 0.39107530878629604}}\n",
      "2022/04/05 20:34:59 - INFO - Distillation -  Epoch 7 finished\n",
      "2022/04/05 20:34:59 - INFO - Distillation -  Epoch 8\n",
      "2022/04/05 20:34:59 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:35:00 - INFO - Distillation -  Global step: 1269, epoch step:9\n",
      "2022/04/05 20:35:01 - INFO - Distillation -  Global step: 1278, epoch step:18\n",
      "2022/04/05 20:35:02 - INFO - Distillation -  Global step: 1287, epoch step:27\n",
      "2022/04/05 20:35:03 - INFO - Distillation -  Global step: 1296, epoch step:36\n",
      "2022/04/05 20:35:04 - INFO - Distillation -  Global step: 1305, epoch step:45\n",
      "2022/04/05 20:35:04 - INFO - Distillation -  Global step: 1314, epoch step:54\n",
      "2022/04/05 20:35:05 - INFO - Distillation -  Global step: 1323, epoch step:63\n",
      "2022/04/05 20:35:06 - INFO - Distillation -  Global step: 1332, epoch step:72\n",
      "2022/04/05 20:35:07 - INFO - Distillation -  Global step: 1341, epoch step:81\n",
      "2022/04/05 20:35:08 - INFO - Distillation -  Global step: 1350, epoch step:90\n",
      "2022/04/05 20:35:09 - INFO - Distillation -  Global step: 1359, epoch step:99\n",
      "2022/04/05 20:35:09 - INFO - Distillation -  Global step: 1368, epoch step:108\n",
      "2022/04/05 20:35:10 - INFO - Distillation -  Global step: 1377, epoch step:117\n",
      "2022/04/05 20:35:11 - INFO - Distillation -  Global step: 1386, epoch step:126\n",
      "2022/04/05 20:35:12 - INFO - Distillation -  Global step: 1395, epoch step:135\n",
      "2022/04/05 20:35:13 - INFO - Distillation -  Global step: 1404, epoch step:144\n",
      "2022/04/05 20:35:14 - INFO - Distillation -  Global step: 1413, epoch step:153\n",
      "2022/04/05 20:35:14 - INFO - Distillation -  Global step: 1422, epoch step:162\n",
      "2022/04/05 20:35:15 - INFO - Distillation -  Global step: 1431, epoch step:171\n",
      "2022/04/05 20:35:16 - INFO - Distillation -  Global step: 1440, epoch step:180\n",
      "2022/04/05 20:35:16 - INFO - Distillation -  Saving at global step 1440, epoch step 180 epoch 8\n",
      "2022/04/05 20:35:18 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:35:25 - INFO - Distillation -  {'stsb': {'pearson': 0.16865298285590716, 'spearmanr': 0.14468558370622125}, 'train': {'pearson': 0.5516888281395812, 'spearmanr': 0.5849647940202449}}\n",
      "2022/04/05 20:35:25 - INFO - Distillation -  Epoch 8 finished\n",
      "2022/04/05 20:35:25 - INFO - Distillation -  Epoch 9\n",
      "2022/04/05 20:35:25 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:35:25 - INFO - Distillation -  Global step: 1449, epoch step:9\n",
      "2022/04/05 20:35:26 - INFO - Distillation -  Global step: 1458, epoch step:18\n",
      "2022/04/05 20:35:27 - INFO - Distillation -  Global step: 1467, epoch step:27\n",
      "2022/04/05 20:35:28 - INFO - Distillation -  Global step: 1476, epoch step:36\n",
      "2022/04/05 20:35:29 - INFO - Distillation -  Global step: 1485, epoch step:45\n",
      "2022/04/05 20:35:29 - INFO - Distillation -  Global step: 1494, epoch step:54\n",
      "2022/04/05 20:35:30 - INFO - Distillation -  Global step: 1503, epoch step:63\n",
      "2022/04/05 20:35:31 - INFO - Distillation -  Global step: 1512, epoch step:72\n",
      "2022/04/05 20:35:32 - INFO - Distillation -  Global step: 1521, epoch step:81\n",
      "2022/04/05 20:35:33 - INFO - Distillation -  Global step: 1530, epoch step:90\n",
      "2022/04/05 20:35:34 - INFO - Distillation -  Global step: 1539, epoch step:99\n",
      "2022/04/05 20:35:34 - INFO - Distillation -  Global step: 1548, epoch step:108\n",
      "2022/04/05 20:35:35 - INFO - Distillation -  Global step: 1557, epoch step:117\n",
      "2022/04/05 20:35:36 - INFO - Distillation -  Global step: 1566, epoch step:126\n",
      "2022/04/05 20:35:37 - INFO - Distillation -  Global step: 1575, epoch step:135\n",
      "2022/04/05 20:35:38 - INFO - Distillation -  Global step: 1584, epoch step:144\n",
      "2022/04/05 20:35:38 - INFO - Distillation -  Global step: 1593, epoch step:153\n",
      "2022/04/05 20:35:39 - INFO - Distillation -  Global step: 1602, epoch step:162\n",
      "2022/04/05 20:35:40 - INFO - Distillation -  Global step: 1611, epoch step:171\n",
      "2022/04/05 20:35:41 - INFO - Distillation -  Global step: 1620, epoch step:180\n",
      "2022/04/05 20:35:41 - INFO - Distillation -  Saving at global step 1620, epoch step 180 epoch 9\n",
      "2022/04/05 20:35:42 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:35:50 - INFO - Distillation -  {'stsb': {'pearson': 0.15389310372487497, 'spearmanr': 0.139091019352105}, 'train': {'pearson': 0.6476387026575736, 'spearmanr': 0.6726738588329624}}\n",
      "2022/04/05 20:35:50 - INFO - Distillation -  Epoch 9 finished\n",
      "2022/04/05 20:35:50 - INFO - Distillation -  Epoch 10\n",
      "2022/04/05 20:35:50 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:35:50 - INFO - Distillation -  Global step: 1629, epoch step:9\n",
      "2022/04/05 20:35:51 - INFO - Distillation -  Global step: 1638, epoch step:18\n",
      "2022/04/05 20:35:52 - INFO - Distillation -  Global step: 1647, epoch step:27\n",
      "2022/04/05 20:35:53 - INFO - Distillation -  Global step: 1656, epoch step:36\n",
      "2022/04/05 20:35:54 - INFO - Distillation -  Global step: 1665, epoch step:45\n",
      "2022/04/05 20:35:54 - INFO - Distillation -  Global step: 1674, epoch step:54\n",
      "2022/04/05 20:35:55 - INFO - Distillation -  Global step: 1683, epoch step:63\n",
      "2022/04/05 20:35:56 - INFO - Distillation -  Global step: 1692, epoch step:72\n",
      "2022/04/05 20:35:57 - INFO - Distillation -  Global step: 1701, epoch step:81\n",
      "2022/04/05 20:35:58 - INFO - Distillation -  Global step: 1710, epoch step:90\n",
      "2022/04/05 20:35:58 - INFO - Distillation -  Global step: 1719, epoch step:99\n",
      "2022/04/05 20:35:59 - INFO - Distillation -  Global step: 1728, epoch step:108\n",
      "2022/04/05 20:36:00 - INFO - Distillation -  Global step: 1737, epoch step:117\n",
      "2022/04/05 20:36:01 - INFO - Distillation -  Global step: 1746, epoch step:126\n",
      "2022/04/05 20:36:02 - INFO - Distillation -  Global step: 1755, epoch step:135\n",
      "2022/04/05 20:36:02 - INFO - Distillation -  Global step: 1764, epoch step:144\n",
      "2022/04/05 20:36:03 - INFO - Distillation -  Global step: 1773, epoch step:153\n",
      "2022/04/05 20:36:04 - INFO - Distillation -  Global step: 1782, epoch step:162\n",
      "2022/04/05 20:36:05 - INFO - Distillation -  Global step: 1791, epoch step:171\n",
      "2022/04/05 20:36:06 - INFO - Distillation -  Global step: 1800, epoch step:180\n",
      "2022/04/05 20:36:06 - INFO - Distillation -  Saving at global step 1800, epoch step 180 epoch 10\n",
      "2022/04/05 20:36:07 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:36:14 - INFO - Distillation -  {'stsb': {'pearson': 0.13885848732301953, 'spearmanr': 0.15545215354253547}, 'train': {'pearson': 0.7150585958492355, 'spearmanr': 0.742593640224388}}\n",
      "2022/04/05 20:36:14 - INFO - Distillation -  Epoch 10 finished\n",
      "2022/04/05 20:36:14 - INFO - Distillation -  Epoch 11\n",
      "2022/04/05 20:36:14 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:36:14 - INFO - Distillation -  Global step: 1809, epoch step:9\n",
      "2022/04/05 20:36:15 - INFO - Distillation -  Global step: 1818, epoch step:18\n",
      "2022/04/05 20:36:16 - INFO - Distillation -  Global step: 1827, epoch step:27\n",
      "2022/04/05 20:36:17 - INFO - Distillation -  Global step: 1836, epoch step:36\n",
      "2022/04/05 20:36:18 - INFO - Distillation -  Global step: 1845, epoch step:45\n",
      "2022/04/05 20:36:18 - INFO - Distillation -  Global step: 1854, epoch step:54\n",
      "2022/04/05 20:36:19 - INFO - Distillation -  Global step: 1863, epoch step:63\n",
      "2022/04/05 20:36:20 - INFO - Distillation -  Global step: 1872, epoch step:72\n",
      "2022/04/05 20:36:21 - INFO - Distillation -  Global step: 1881, epoch step:81\n",
      "2022/04/05 20:36:22 - INFO - Distillation -  Global step: 1890, epoch step:90\n",
      "2022/04/05 20:36:23 - INFO - Distillation -  Global step: 1899, epoch step:99\n",
      "2022/04/05 20:36:23 - INFO - Distillation -  Global step: 1908, epoch step:108\n",
      "2022/04/05 20:36:24 - INFO - Distillation -  Global step: 1917, epoch step:117\n",
      "2022/04/05 20:36:25 - INFO - Distillation -  Global step: 1926, epoch step:126\n",
      "2022/04/05 20:36:26 - INFO - Distillation -  Global step: 1935, epoch step:135\n",
      "2022/04/05 20:36:27 - INFO - Distillation -  Global step: 1944, epoch step:144\n",
      "2022/04/05 20:36:28 - INFO - Distillation -  Global step: 1953, epoch step:153\n",
      "2022/04/05 20:36:28 - INFO - Distillation -  Global step: 1962, epoch step:162\n",
      "2022/04/05 20:36:29 - INFO - Distillation -  Global step: 1971, epoch step:171\n",
      "2022/04/05 20:36:30 - INFO - Distillation -  Global step: 1980, epoch step:180\n",
      "2022/04/05 20:36:30 - INFO - Distillation -  Saving at global step 1980, epoch step 180 epoch 11\n",
      "2022/04/05 20:36:32 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:36:38 - INFO - Distillation -  {'stsb': {'pearson': 0.11243210574801865, 'spearmanr': 0.13917087172030826}, 'train': {'pearson': 0.7478505383990869, 'spearmanr': 0.8021751481914391}}\n",
      "2022/04/05 20:36:38 - INFO - Distillation -  Epoch 11 finished\n",
      "2022/04/05 20:36:38 - INFO - Distillation -  Epoch 12\n",
      "2022/04/05 20:36:38 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:36:39 - INFO - Distillation -  Global step: 1989, epoch step:9\n",
      "2022/04/05 20:36:40 - INFO - Distillation -  Global step: 1998, epoch step:18\n",
      "2022/04/05 20:36:41 - INFO - Distillation -  Global step: 2007, epoch step:27\n",
      "2022/04/05 20:36:42 - INFO - Distillation -  Global step: 2016, epoch step:36\n",
      "2022/04/05 20:36:43 - INFO - Distillation -  Global step: 2025, epoch step:45\n",
      "2022/04/05 20:36:44 - INFO - Distillation -  Global step: 2034, epoch step:54\n",
      "2022/04/05 20:36:44 - INFO - Distillation -  Global step: 2043, epoch step:63\n",
      "2022/04/05 20:36:45 - INFO - Distillation -  Global step: 2052, epoch step:72\n",
      "2022/04/05 20:36:46 - INFO - Distillation -  Global step: 2061, epoch step:81\n",
      "2022/04/05 20:36:47 - INFO - Distillation -  Global step: 2070, epoch step:90\n",
      "2022/04/05 20:36:48 - INFO - Distillation -  Global step: 2079, epoch step:99\n",
      "2022/04/05 20:36:49 - INFO - Distillation -  Global step: 2088, epoch step:108\n",
      "2022/04/05 20:36:49 - INFO - Distillation -  Global step: 2097, epoch step:117\n",
      "2022/04/05 20:36:50 - INFO - Distillation -  Global step: 2106, epoch step:126\n",
      "2022/04/05 20:36:51 - INFO - Distillation -  Global step: 2115, epoch step:135\n",
      "2022/04/05 20:36:52 - INFO - Distillation -  Global step: 2124, epoch step:144\n",
      "2022/04/05 20:36:53 - INFO - Distillation -  Global step: 2133, epoch step:153\n",
      "2022/04/05 20:36:53 - INFO - Distillation -  Global step: 2142, epoch step:162\n",
      "2022/04/05 20:36:54 - INFO - Distillation -  Global step: 2151, epoch step:171\n",
      "2022/04/05 20:36:55 - INFO - Distillation -  Global step: 2160, epoch step:180\n",
      "2022/04/05 20:36:55 - INFO - Distillation -  Saving at global step 2160, epoch step 180 epoch 12\n",
      "2022/04/05 20:36:56 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:37:03 - INFO - Distillation -  {'stsb': {'pearson': 0.12709298544970354, 'spearmanr': 0.138531732489764}, 'train': {'pearson': 0.8079550766754827, 'spearmanr': 0.8298838947525976}}\n",
      "2022/04/05 20:37:03 - INFO - Distillation -  Epoch 12 finished\n",
      "2022/04/05 20:37:03 - INFO - Distillation -  Epoch 13\n",
      "2022/04/05 20:37:03 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:37:04 - INFO - Distillation -  Global step: 2169, epoch step:9\n",
      "2022/04/05 20:37:04 - INFO - Distillation -  Global step: 2178, epoch step:18\n",
      "2022/04/05 20:37:05 - INFO - Distillation -  Global step: 2187, epoch step:27\n",
      "2022/04/05 20:37:06 - INFO - Distillation -  Global step: 2196, epoch step:36\n",
      "2022/04/05 20:37:07 - INFO - Distillation -  Global step: 2205, epoch step:45\n",
      "2022/04/05 20:37:08 - INFO - Distillation -  Global step: 2214, epoch step:54\n",
      "2022/04/05 20:37:08 - INFO - Distillation -  Global step: 2223, epoch step:63\n",
      "2022/04/05 20:37:09 - INFO - Distillation -  Global step: 2232, epoch step:72\n",
      "2022/04/05 20:37:10 - INFO - Distillation -  Global step: 2241, epoch step:81\n",
      "2022/04/05 20:37:11 - INFO - Distillation -  Global step: 2250, epoch step:90\n",
      "2022/04/05 20:37:12 - INFO - Distillation -  Global step: 2259, epoch step:99\n",
      "2022/04/05 20:37:12 - INFO - Distillation -  Global step: 2268, epoch step:108\n",
      "2022/04/05 20:37:13 - INFO - Distillation -  Global step: 2277, epoch step:117\n",
      "2022/04/05 20:37:14 - INFO - Distillation -  Global step: 2286, epoch step:126\n",
      "2022/04/05 20:37:15 - INFO - Distillation -  Global step: 2295, epoch step:135\n",
      "2022/04/05 20:37:15 - INFO - Distillation -  Global step: 2304, epoch step:144\n",
      "2022/04/05 20:37:16 - INFO - Distillation -  Global step: 2313, epoch step:153\n",
      "2022/04/05 20:37:17 - INFO - Distillation -  Global step: 2322, epoch step:162\n",
      "2022/04/05 20:37:18 - INFO - Distillation -  Global step: 2331, epoch step:171\n",
      "2022/04/05 20:37:19 - INFO - Distillation -  Global step: 2340, epoch step:180\n",
      "2022/04/05 20:37:19 - INFO - Distillation -  Saving at global step 2340, epoch step 180 epoch 13\n",
      "2022/04/05 20:37:19 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:37:25 - INFO - Distillation -  {'stsb': {'pearson': 0.12257712064542946, 'spearmanr': 0.13538900434507958}, 'train': {'pearson': 0.8057963322200805, 'spearmanr': 0.8387537863578067}}\n",
      "2022/04/05 20:37:25 - INFO - Distillation -  Epoch 13 finished\n",
      "2022/04/05 20:37:25 - INFO - Distillation -  Epoch 14\n",
      "2022/04/05 20:37:25 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:37:26 - INFO - Distillation -  Global step: 2349, epoch step:9\n",
      "2022/04/05 20:37:27 - INFO - Distillation -  Global step: 2358, epoch step:18\n",
      "2022/04/05 20:37:28 - INFO - Distillation -  Global step: 2367, epoch step:27\n",
      "2022/04/05 20:37:29 - INFO - Distillation -  Global step: 2376, epoch step:36\n",
      "2022/04/05 20:37:29 - INFO - Distillation -  Global step: 2385, epoch step:45\n",
      "2022/04/05 20:37:30 - INFO - Distillation -  Global step: 2394, epoch step:54\n",
      "2022/04/05 20:37:31 - INFO - Distillation -  Global step: 2403, epoch step:63\n",
      "2022/04/05 20:37:32 - INFO - Distillation -  Global step: 2412, epoch step:72\n",
      "2022/04/05 20:37:33 - INFO - Distillation -  Global step: 2421, epoch step:81\n",
      "2022/04/05 20:37:34 - INFO - Distillation -  Global step: 2430, epoch step:90\n",
      "2022/04/05 20:37:34 - INFO - Distillation -  Global step: 2439, epoch step:99\n",
      "2022/04/05 20:37:35 - INFO - Distillation -  Global step: 2448, epoch step:108\n",
      "2022/04/05 20:37:36 - INFO - Distillation -  Global step: 2457, epoch step:117\n",
      "2022/04/05 20:37:37 - INFO - Distillation -  Global step: 2466, epoch step:126\n",
      "2022/04/05 20:37:37 - INFO - Distillation -  Global step: 2475, epoch step:135\n",
      "2022/04/05 20:37:38 - INFO - Distillation -  Global step: 2484, epoch step:144\n",
      "2022/04/05 20:37:39 - INFO - Distillation -  Global step: 2493, epoch step:153\n",
      "2022/04/05 20:37:40 - INFO - Distillation -  Global step: 2502, epoch step:162\n",
      "2022/04/05 20:37:41 - INFO - Distillation -  Global step: 2511, epoch step:171\n",
      "2022/04/05 20:37:41 - INFO - Distillation -  Global step: 2520, epoch step:180\n",
      "2022/04/05 20:37:41 - INFO - Distillation -  Saving at global step 2520, epoch step 180 epoch 14\n",
      "2022/04/05 20:37:44 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:37:51 - INFO - Distillation -  {'stsb': {'pearson': 0.13259138232146028, 'spearmanr': 0.14108624136782488}, 'train': {'pearson': 0.8327538802675433, 'spearmanr': 0.8532554987698137}}\n",
      "2022/04/05 20:37:51 - INFO - Distillation -  Epoch 14 finished\n",
      "2022/04/05 20:37:51 - INFO - Distillation -  Epoch 15\n",
      "2022/04/05 20:37:51 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:37:52 - INFO - Distillation -  Global step: 2529, epoch step:9\n",
      "2022/04/05 20:37:52 - INFO - Distillation -  Global step: 2538, epoch step:18\n",
      "2022/04/05 20:37:53 - INFO - Distillation -  Global step: 2547, epoch step:27\n",
      "2022/04/05 20:37:54 - INFO - Distillation -  Global step: 2556, epoch step:36\n",
      "2022/04/05 20:37:55 - INFO - Distillation -  Global step: 2565, epoch step:45\n",
      "2022/04/05 20:37:55 - INFO - Distillation -  Global step: 2574, epoch step:54\n",
      "2022/04/05 20:37:56 - INFO - Distillation -  Global step: 2583, epoch step:63\n",
      "2022/04/05 20:37:57 - INFO - Distillation -  Global step: 2592, epoch step:72\n",
      "2022/04/05 20:37:58 - INFO - Distillation -  Global step: 2601, epoch step:81\n",
      "2022/04/05 20:37:58 - INFO - Distillation -  Global step: 2610, epoch step:90\n",
      "2022/04/05 20:37:59 - INFO - Distillation -  Global step: 2619, epoch step:99\n",
      "2022/04/05 20:38:00 - INFO - Distillation -  Global step: 2628, epoch step:108\n",
      "2022/04/05 20:38:01 - INFO - Distillation -  Global step: 2637, epoch step:117\n",
      "2022/04/05 20:38:02 - INFO - Distillation -  Global step: 2646, epoch step:126\n",
      "2022/04/05 20:38:02 - INFO - Distillation -  Global step: 2655, epoch step:135\n",
      "2022/04/05 20:38:03 - INFO - Distillation -  Global step: 2664, epoch step:144\n",
      "2022/04/05 20:38:04 - INFO - Distillation -  Global step: 2673, epoch step:153\n",
      "2022/04/05 20:38:05 - INFO - Distillation -  Global step: 2682, epoch step:162\n",
      "2022/04/05 20:38:05 - INFO - Distillation -  Global step: 2691, epoch step:171\n",
      "2022/04/05 20:38:06 - INFO - Distillation -  Global step: 2700, epoch step:180\n",
      "2022/04/05 20:38:06 - INFO - Distillation -  Saving at global step 2700, epoch step 180 epoch 15\n",
      "2022/04/05 20:38:08 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:38:14 - INFO - Distillation -  {'stsb': {'pearson': 0.13230808791429086, 'spearmanr': 0.14126077542151183}, 'train': {'pearson': 0.8542118575079211, 'spearmanr': 0.8695868014506224}}\n",
      "2022/04/05 20:38:14 - INFO - Distillation -  Epoch 15 finished\n",
      "2022/04/05 20:38:14 - INFO - Distillation -  Epoch 16\n",
      "2022/04/05 20:38:14 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:38:14 - INFO - Distillation -  Global step: 2709, epoch step:9\n",
      "2022/04/05 20:38:15 - INFO - Distillation -  Global step: 2718, epoch step:18\n",
      "2022/04/05 20:38:16 - INFO - Distillation -  Global step: 2727, epoch step:27\n",
      "2022/04/05 20:38:17 - INFO - Distillation -  Global step: 2736, epoch step:36\n",
      "2022/04/05 20:38:18 - INFO - Distillation -  Global step: 2745, epoch step:45\n",
      "2022/04/05 20:38:18 - INFO - Distillation -  Global step: 2754, epoch step:54\n",
      "2022/04/05 20:38:19 - INFO - Distillation -  Global step: 2763, epoch step:63\n",
      "2022/04/05 20:38:20 - INFO - Distillation -  Global step: 2772, epoch step:72\n",
      "2022/04/05 20:38:21 - INFO - Distillation -  Global step: 2781, epoch step:81\n",
      "2022/04/05 20:38:22 - INFO - Distillation -  Global step: 2790, epoch step:90\n",
      "2022/04/05 20:38:22 - INFO - Distillation -  Global step: 2799, epoch step:99\n",
      "2022/04/05 20:38:23 - INFO - Distillation -  Global step: 2808, epoch step:108\n",
      "2022/04/05 20:38:24 - INFO - Distillation -  Global step: 2817, epoch step:117\n",
      "2022/04/05 20:38:25 - INFO - Distillation -  Global step: 2826, epoch step:126\n",
      "2022/04/05 20:38:25 - INFO - Distillation -  Global step: 2835, epoch step:135\n",
      "2022/04/05 20:38:26 - INFO - Distillation -  Global step: 2844, epoch step:144\n",
      "2022/04/05 20:38:27 - INFO - Distillation -  Global step: 2853, epoch step:153\n",
      "2022/04/05 20:38:28 - INFO - Distillation -  Global step: 2862, epoch step:162\n",
      "2022/04/05 20:38:28 - INFO - Distillation -  Global step: 2871, epoch step:171\n",
      "2022/04/05 20:38:29 - INFO - Distillation -  Global step: 2880, epoch step:180\n",
      "2022/04/05 20:38:29 - INFO - Distillation -  Saving at global step 2880, epoch step 180 epoch 16\n",
      "2022/04/05 20:38:31 - INFO - Distillation -  Running callback function...\n",
      "2022/04/05 20:38:37 - INFO - Distillation -  {'stsb': {'pearson': 0.11735977968387964, 'spearmanr': 0.1206779301489473}, 'train': {'pearson': 0.8728050580670947, 'spearmanr': 0.8807997149961895}}\n",
      "2022/04/05 20:38:37 - INFO - Distillation -  Epoch 16 finished\n",
      "2022/04/05 20:38:37 - INFO - Distillation -  Epoch 17\n",
      "2022/04/05 20:38:38 - INFO - Distillation -  Length of current epoch in forward batch: 180\n",
      "2022/04/05 20:38:38 - INFO - Distillation -  Global step: 2889, epoch step:9\n",
      "2022/04/05 20:38:39 - INFO - Distillation -  Global step: 2898, epoch step:18\n",
      "2022/04/05 20:38:40 - INFO - Distillation -  Global step: 2907, epoch step:27\n",
      "2022/04/05 20:38:40 - INFO - Distillation -  Global step: 2916, epoch step:36\n",
      "2022/04/05 20:38:41 - INFO - Distillation -  Global step: 2925, epoch step:45\n",
      "2022/04/05 20:38:42 - INFO - Distillation -  Global step: 2934, epoch step:54\n",
      "2022/04/05 20:38:43 - INFO - Distillation -  Global step: 2943, epoch step:63\n",
      "2022/04/05 20:38:43 - INFO - Distillation -  Global step: 2952, epoch step:72\n",
      "2022/04/05 20:38:44 - INFO - Distillation -  Global step: 2961, epoch step:81\n",
      "2022/04/05 20:38:45 - INFO - Distillation -  Global step: 2970, epoch step:90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_927960/3463007727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, dataloader, num_epochs, scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain_with_num_epochs\u001b[0;34m(self, optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_epochs, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mwriter_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mwrite_loss\u001b[0;34m(self, total_loss, writer_step, losses_dict)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mcpu_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scalar/total_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlosses_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "\n",
    "from matches import matches\n",
    "intermediate_matches = None\n",
    "match_list_L4t = [\"L4t_hidden_mse\", \"L4_hidden_smmd\"]\n",
    "match_list_L3 = [\"L3_hidden_mse\", \"L3_hidden_smmd\"]\n",
    "intermediate_matches = []\n",
    "for match in match_list_L3:\n",
    "        intermediate_matches += matches[match]\n",
    "\n",
    "output_dir = \"outputs/\" + base_model_name + \"/\" + task_name + \"/\" + \"hl\"+ str(student_model.config.num_hidden_layers) + \"_hs\" +  str(student_model.config.hidden_size) + \"/\"\n",
    "\n",
    "distill_config = DistillationConfig(kd_loss_type='mse',temperature=4,intermediate_matches=intermediate_matches)\n",
    "train_config = TrainingConfig(device=device, output_dir = output_dir + \"models/\")\n",
    "\n",
    "\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "output_dir = \"outputs/\" + task_name + \"/\" \n",
    "eval_datasets = [val_dataset]\n",
    "do_train_eval = True\n",
    "\n",
    "callback_func = partial(predict, eval_datasets=eval_datasets, output_dir=output_dir+\"results/\",task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device, do_train_eval=do_train_eval, train_dataset=train_dataset)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbrewer.distiller_utils import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [],
   "source": [
    "test_model = RobertaForSequenceClassification(student_config)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs9900.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [],
   "source": [
    "metric= load_metric(\"glue\",\"stsb\")\n",
    "test_model.to(device)\n",
    "test_model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    metric.add_batch(predictions=logits, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher_model = RobertaForSequenceClassification.from_pretrained('/work/mhessent/master_thesis/eval_out/roberta-base/stsb/lr3e-05_bs32_epochs10/checkpoint-1620')\n",
    "metric= load_metric(\"glue\",\"stsb\")\n",
    "#teacher_model.cpu()\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "for batch in train_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch = move_to_device(batch,device)\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    metric.add_batch(predictions=logits, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
