{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'sst2', split='train')\n",
    "val_dataset = load_dataset('glue', 'sst2', split='validation')\n",
    "test_dataset = load_dataset('glue', 'sst2', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b9e78673c79f89ac.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2258e450e7114d5b.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fc7e588b0e3f2b71.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/nn/modules/module.py:1385: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"howey/bert-base-uncased-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8b91720ce2aeadc7.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-31c6ef88b4a5a656.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-caa256527e820588.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=3e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=100,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/sst2_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "#train_dataset = train_dataset.select(range(10))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbert_config = BertConfig.from_json_file(\\'/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json\\')\\nbert_config.output_hidden_states = True\\nbert_config.vocab_size = 30522\\n#print(bert_config)\\nteacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\\nteacher_model.load_state_dict(torch.load(\\'outputs/sst2_teacher_model.pt\\'))\\n#pretrained_config = BertConfig(output_hidden_states=True, output_attentions=True, num_labels=2)\\n#teacher_model = AutoModelForSequenceClassification.from_pretrained(\"howey/bert-base-uncased-sst2\", config=pretrained_config)\\n\\nteacher_model = teacher_model.to(device=device)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(\"howey/bert-base-uncased-sst2\")\n",
    "#torch.save(model.state_dict(), 'outputs/hub_sst_teacher_model.pt')\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "#bert_config.num_labels = 2\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_sst_teacher_model.pt'))\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "#bert_config_T3 = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config_L3_v2.json')#相对路径\n",
    "bert_config_T3 = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config_L3.json')\n",
    "bert_config_T3.output_hidden_states = True\n",
    "bert_config_T3.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "student_model = BertForSequenceClassification(bert_config_T3) #, num_labels = 2\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\"\"\"\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "#print(bert_config)\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/sst2_teacher_model.pt'))\n",
    "#pretrained_config = BertConfig(output_hidden_states=True, output_attentions=True, num_labels=2)\n",
    "#teacher_model = AutoModelForSequenceClassification.from_pretrained(\"howey/bert-base-uncased-sst2\", config=pretrained_config)\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2022/03/18 18:42:50 - INFO - Distillation -  Training steps per epoch: 2105\n",
      "2022/03/18 18:42:50 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/03/18 18:42:50 - INFO - Distillation -  Epoch 1\n",
      "2022/03/18 18:42:50 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:43:00 - INFO - Distillation -  Global step: 105, epoch step:105\n",
      "2022/03/18 18:43:09 - INFO - Distillation -  Global step: 210, epoch step:210\n",
      "2022/03/18 18:43:18 - INFO - Distillation -  Global step: 315, epoch step:315\n",
      "2022/03/18 18:43:28 - INFO - Distillation -  Global step: 420, epoch step:420\n",
      "2022/03/18 18:43:37 - INFO - Distillation -  Global step: 525, epoch step:525\n",
      "2022/03/18 18:43:47 - INFO - Distillation -  Global step: 630, epoch step:630\n",
      "2022/03/18 18:43:56 - INFO - Distillation -  Global step: 735, epoch step:735\n",
      "2022/03/18 18:44:06 - INFO - Distillation -  Global step: 840, epoch step:840\n",
      "2022/03/18 18:44:15 - INFO - Distillation -  Global step: 945, epoch step:945\n",
      "2022/03/18 18:44:25 - INFO - Distillation -  Global step: 1050, epoch step:1050\n",
      "2022/03/18 18:44:34 - INFO - Distillation -  Global step: 1155, epoch step:1155\n",
      "2022/03/18 18:44:44 - INFO - Distillation -  Global step: 1260, epoch step:1260\n",
      "2022/03/18 18:44:53 - INFO - Distillation -  Global step: 1365, epoch step:1365\n",
      "2022/03/18 18:45:03 - INFO - Distillation -  Global step: 1470, epoch step:1470\n",
      "2022/03/18 18:45:12 - INFO - Distillation -  Global step: 1575, epoch step:1575\n",
      "2022/03/18 18:45:22 - INFO - Distillation -  Global step: 1680, epoch step:1680\n",
      "2022/03/18 18:45:31 - INFO - Distillation -  Global step: 1785, epoch step:1785\n",
      "2022/03/18 18:45:41 - INFO - Distillation -  Global step: 1890, epoch step:1890\n",
      "2022/03/18 18:45:50 - INFO - Distillation -  Global step: 1995, epoch step:1995\n",
      "2022/03/18 18:46:00 - INFO - Distillation -  Global step: 2100, epoch step:2100\n",
      "2022/03/18 18:46:00 - INFO - Distillation -  Saving at global step 2105, epoch step 2105 epoch 1\n",
      "2022/03/18 18:46:02 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 18:46:02 - INFO - Distillation -  {'sst-2': {'acc': 0.786697247706422}}\n",
      "2022/03/18 18:46:02 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/03/18 18:46:02 - INFO - Distillation -  Epoch 2\n",
      "2022/03/18 18:46:02 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:46:11 - INFO - Distillation -  Global step: 2205, epoch step:100\n",
      "2022/03/18 18:46:21 - INFO - Distillation -  Global step: 2310, epoch step:205\n",
      "2022/03/18 18:46:30 - INFO - Distillation -  Global step: 2415, epoch step:310\n",
      "2022/03/18 18:46:39 - INFO - Distillation -  Global step: 2520, epoch step:415\n",
      "2022/03/18 18:46:49 - INFO - Distillation -  Global step: 2625, epoch step:520\n",
      "2022/03/18 18:46:58 - INFO - Distillation -  Global step: 2730, epoch step:625\n",
      "2022/03/18 18:47:08 - INFO - Distillation -  Global step: 2835, epoch step:730\n",
      "2022/03/18 18:47:17 - INFO - Distillation -  Global step: 2940, epoch step:835\n",
      "2022/03/18 18:47:27 - INFO - Distillation -  Global step: 3045, epoch step:940\n",
      "2022/03/18 18:47:36 - INFO - Distillation -  Global step: 3150, epoch step:1045\n",
      "2022/03/18 18:47:45 - INFO - Distillation -  Global step: 3255, epoch step:1150\n",
      "2022/03/18 18:47:55 - INFO - Distillation -  Global step: 3360, epoch step:1255\n",
      "2022/03/18 18:48:04 - INFO - Distillation -  Global step: 3465, epoch step:1360\n",
      "2022/03/18 18:48:14 - INFO - Distillation -  Global step: 3570, epoch step:1465\n",
      "2022/03/18 18:48:23 - INFO - Distillation -  Global step: 3675, epoch step:1570\n",
      "2022/03/18 18:48:33 - INFO - Distillation -  Global step: 3780, epoch step:1675\n",
      "2022/03/18 18:48:42 - INFO - Distillation -  Global step: 3885, epoch step:1780\n",
      "2022/03/18 18:48:52 - INFO - Distillation -  Global step: 3990, epoch step:1885\n",
      "2022/03/18 18:49:01 - INFO - Distillation -  Global step: 4095, epoch step:1990\n",
      "2022/03/18 18:49:11 - INFO - Distillation -  Global step: 4200, epoch step:2095\n",
      "2022/03/18 18:49:12 - INFO - Distillation -  Saving at global step 4210, epoch step 2105 epoch 2\n",
      "2022/03/18 18:49:13 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 18:49:13 - INFO - Distillation -  {'sst-2': {'acc': 0.7809633027522935}}\n",
      "2022/03/18 18:49:13 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/03/18 18:49:13 - INFO - Distillation -  Epoch 3\n",
      "2022/03/18 18:49:13 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:49:22 - INFO - Distillation -  Global step: 4305, epoch step:95\n",
      "2022/03/18 18:49:32 - INFO - Distillation -  Global step: 4410, epoch step:200\n",
      "2022/03/18 18:49:41 - INFO - Distillation -  Global step: 4515, epoch step:305\n",
      "2022/03/18 18:49:50 - INFO - Distillation -  Global step: 4620, epoch step:410\n",
      "2022/03/18 18:50:00 - INFO - Distillation -  Global step: 4725, epoch step:515\n",
      "2022/03/18 18:50:09 - INFO - Distillation -  Global step: 4830, epoch step:620\n",
      "2022/03/18 18:50:19 - INFO - Distillation -  Global step: 4935, epoch step:725\n",
      "2022/03/18 18:50:28 - INFO - Distillation -  Global step: 5040, epoch step:830\n",
      "2022/03/18 18:50:37 - INFO - Distillation -  Global step: 5145, epoch step:935\n",
      "2022/03/18 18:50:46 - INFO - Distillation -  Global step: 5250, epoch step:1040\n",
      "2022/03/18 18:50:56 - INFO - Distillation -  Global step: 5355, epoch step:1145\n",
      "2022/03/18 18:51:05 - INFO - Distillation -  Global step: 5460, epoch step:1250\n",
      "2022/03/18 18:51:15 - INFO - Distillation -  Global step: 5565, epoch step:1355\n",
      "2022/03/18 18:51:24 - INFO - Distillation -  Global step: 5670, epoch step:1460\n",
      "2022/03/18 18:51:33 - INFO - Distillation -  Global step: 5775, epoch step:1565\n",
      "2022/03/18 18:51:43 - INFO - Distillation -  Global step: 5880, epoch step:1670\n",
      "2022/03/18 18:51:52 - INFO - Distillation -  Global step: 5985, epoch step:1775\n",
      "2022/03/18 18:52:02 - INFO - Distillation -  Global step: 6090, epoch step:1880\n",
      "2022/03/18 18:52:11 - INFO - Distillation -  Global step: 6195, epoch step:1985\n",
      "2022/03/18 18:52:21 - INFO - Distillation -  Global step: 6300, epoch step:2090\n",
      "2022/03/18 18:52:22 - INFO - Distillation -  Saving at global step 6315, epoch step 2105 epoch 3\n",
      "2022/03/18 18:52:23 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 18:52:23 - INFO - Distillation -  {'sst-2': {'acc': 0.7912844036697247}}\n",
      "2022/03/18 18:52:23 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/03/18 18:52:23 - INFO - Distillation -  Epoch 4\n",
      "2022/03/18 18:52:23 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:52:31 - INFO - Distillation -  Global step: 6405, epoch step:90\n",
      "2022/03/18 18:52:41 - INFO - Distillation -  Global step: 6510, epoch step:195\n",
      "2022/03/18 18:52:50 - INFO - Distillation -  Global step: 6615, epoch step:300\n",
      "2022/03/18 18:52:59 - INFO - Distillation -  Global step: 6720, epoch step:405\n",
      "2022/03/18 18:53:09 - INFO - Distillation -  Global step: 6825, epoch step:510\n",
      "2022/03/18 18:53:18 - INFO - Distillation -  Global step: 6930, epoch step:615\n",
      "2022/03/18 18:53:28 - INFO - Distillation -  Global step: 7035, epoch step:720\n",
      "2022/03/18 18:53:37 - INFO - Distillation -  Global step: 7140, epoch step:825\n",
      "2022/03/18 18:53:47 - INFO - Distillation -  Global step: 7245, epoch step:930\n",
      "2022/03/18 18:53:56 - INFO - Distillation -  Global step: 7350, epoch step:1035\n",
      "2022/03/18 18:54:06 - INFO - Distillation -  Global step: 7455, epoch step:1140\n",
      "2022/03/18 18:54:15 - INFO - Distillation -  Global step: 7560, epoch step:1245\n",
      "2022/03/18 18:54:25 - INFO - Distillation -  Global step: 7665, epoch step:1350\n",
      "2022/03/18 18:54:34 - INFO - Distillation -  Global step: 7770, epoch step:1455\n",
      "2022/03/18 18:54:43 - INFO - Distillation -  Global step: 7875, epoch step:1560\n",
      "2022/03/18 18:54:53 - INFO - Distillation -  Global step: 7980, epoch step:1665\n",
      "2022/03/18 18:55:02 - INFO - Distillation -  Global step: 8085, epoch step:1770\n",
      "2022/03/18 18:55:12 - INFO - Distillation -  Global step: 8190, epoch step:1875\n",
      "2022/03/18 18:55:21 - INFO - Distillation -  Global step: 8295, epoch step:1980\n",
      "2022/03/18 18:55:30 - INFO - Distillation -  Global step: 8400, epoch step:2085\n",
      "2022/03/18 18:55:32 - INFO - Distillation -  Saving at global step 8420, epoch step 2105 epoch 4\n",
      "2022/03/18 18:55:33 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 18:55:34 - INFO - Distillation -  {'sst-2': {'acc': 0.7993119266055045}}\n",
      "2022/03/18 18:55:34 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/03/18 18:55:34 - INFO - Distillation -  Epoch 5\n",
      "2022/03/18 18:55:34 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:55:41 - INFO - Distillation -  Global step: 8505, epoch step:85\n",
      "2022/03/18 18:55:51 - INFO - Distillation -  Global step: 8610, epoch step:190\n",
      "2022/03/18 18:56:00 - INFO - Distillation -  Global step: 8715, epoch step:295\n",
      "2022/03/18 18:56:09 - INFO - Distillation -  Global step: 8820, epoch step:400\n",
      "2022/03/18 18:56:18 - INFO - Distillation -  Global step: 8925, epoch step:505\n",
      "2022/03/18 18:56:28 - INFO - Distillation -  Global step: 9030, epoch step:610\n",
      "2022/03/18 18:56:37 - INFO - Distillation -  Global step: 9135, epoch step:715\n",
      "2022/03/18 18:56:47 - INFO - Distillation -  Global step: 9240, epoch step:820\n",
      "2022/03/18 18:56:56 - INFO - Distillation -  Global step: 9345, epoch step:925\n",
      "2022/03/18 18:57:05 - INFO - Distillation -  Global step: 9450, epoch step:1030\n",
      "2022/03/18 18:57:15 - INFO - Distillation -  Global step: 9555, epoch step:1135\n",
      "2022/03/18 18:57:24 - INFO - Distillation -  Global step: 9660, epoch step:1240\n",
      "2022/03/18 18:57:34 - INFO - Distillation -  Global step: 9765, epoch step:1345\n",
      "2022/03/18 18:57:43 - INFO - Distillation -  Global step: 9870, epoch step:1450\n",
      "2022/03/18 18:57:53 - INFO - Distillation -  Global step: 9975, epoch step:1555\n",
      "2022/03/18 18:58:02 - INFO - Distillation -  Global step: 10080, epoch step:1660\n",
      "2022/03/18 18:58:11 - INFO - Distillation -  Global step: 10185, epoch step:1765\n",
      "2022/03/18 18:58:21 - INFO - Distillation -  Global step: 10290, epoch step:1870\n",
      "2022/03/18 18:58:30 - INFO - Distillation -  Global step: 10395, epoch step:1975\n",
      "2022/03/18 18:58:40 - INFO - Distillation -  Global step: 10500, epoch step:2080\n",
      "2022/03/18 18:58:42 - INFO - Distillation -  Saving at global step 10525, epoch step 2105 epoch 5\n",
      "2022/03/18 18:58:43 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 18:58:43 - INFO - Distillation -  {'sst-2': {'acc': 0.7763761467889908}}\n",
      "2022/03/18 18:58:43 - INFO - Distillation -  Epoch 5 finished\n",
      "2022/03/18 18:58:43 - INFO - Distillation -  Epoch 6\n",
      "2022/03/18 18:58:43 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 18:58:51 - INFO - Distillation -  Global step: 10605, epoch step:80\n",
      "2022/03/18 18:59:00 - INFO - Distillation -  Global step: 10710, epoch step:185\n",
      "2022/03/18 18:59:09 - INFO - Distillation -  Global step: 10815, epoch step:290\n",
      "2022/03/18 18:59:19 - INFO - Distillation -  Global step: 10920, epoch step:395\n",
      "2022/03/18 18:59:28 - INFO - Distillation -  Global step: 11025, epoch step:500\n",
      "2022/03/18 18:59:38 - INFO - Distillation -  Global step: 11130, epoch step:605\n",
      "2022/03/18 18:59:47 - INFO - Distillation -  Global step: 11235, epoch step:710\n",
      "2022/03/18 18:59:57 - INFO - Distillation -  Global step: 11340, epoch step:815\n",
      "2022/03/18 19:00:06 - INFO - Distillation -  Global step: 11445, epoch step:920\n",
      "2022/03/18 19:00:15 - INFO - Distillation -  Global step: 11550, epoch step:1025\n",
      "2022/03/18 19:00:25 - INFO - Distillation -  Global step: 11655, epoch step:1130\n",
      "2022/03/18 19:00:34 - INFO - Distillation -  Global step: 11760, epoch step:1235\n",
      "2022/03/18 19:00:44 - INFO - Distillation -  Global step: 11865, epoch step:1340\n",
      "2022/03/18 19:00:53 - INFO - Distillation -  Global step: 11970, epoch step:1445\n",
      "2022/03/18 19:01:02 - INFO - Distillation -  Global step: 12075, epoch step:1550\n",
      "2022/03/18 19:01:12 - INFO - Distillation -  Global step: 12180, epoch step:1655\n",
      "2022/03/18 19:01:21 - INFO - Distillation -  Global step: 12285, epoch step:1760\n",
      "2022/03/18 19:01:30 - INFO - Distillation -  Global step: 12390, epoch step:1865\n",
      "2022/03/18 19:01:40 - INFO - Distillation -  Global step: 12495, epoch step:1970\n",
      "2022/03/18 19:01:49 - INFO - Distillation -  Global step: 12600, epoch step:2075\n",
      "2022/03/18 19:01:52 - INFO - Distillation -  Saving at global step 12630, epoch step 2105 epoch 6\n",
      "2022/03/18 19:01:53 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:01:53 - INFO - Distillation -  {'sst-2': {'acc': 0.8038990825688074}}\n",
      "2022/03/18 19:01:53 - INFO - Distillation -  Epoch 6 finished\n",
      "2022/03/18 19:01:53 - INFO - Distillation -  Epoch 7\n",
      "2022/03/18 19:01:53 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:02:00 - INFO - Distillation -  Global step: 12705, epoch step:75\n",
      "2022/03/18 19:02:10 - INFO - Distillation -  Global step: 12810, epoch step:180\n",
      "2022/03/18 19:02:19 - INFO - Distillation -  Global step: 12915, epoch step:285\n",
      "2022/03/18 19:02:28 - INFO - Distillation -  Global step: 13020, epoch step:390\n",
      "2022/03/18 19:02:38 - INFO - Distillation -  Global step: 13125, epoch step:495\n",
      "2022/03/18 19:02:47 - INFO - Distillation -  Global step: 13230, epoch step:600\n",
      "2022/03/18 19:02:57 - INFO - Distillation -  Global step: 13335, epoch step:705\n",
      "2022/03/18 19:03:06 - INFO - Distillation -  Global step: 13440, epoch step:810\n",
      "2022/03/18 19:03:15 - INFO - Distillation -  Global step: 13545, epoch step:915\n",
      "2022/03/18 19:03:25 - INFO - Distillation -  Global step: 13650, epoch step:1020\n",
      "2022/03/18 19:03:34 - INFO - Distillation -  Global step: 13755, epoch step:1125\n",
      "2022/03/18 19:03:44 - INFO - Distillation -  Global step: 13860, epoch step:1230\n",
      "2022/03/18 19:03:53 - INFO - Distillation -  Global step: 13965, epoch step:1335\n",
      "2022/03/18 19:04:03 - INFO - Distillation -  Global step: 14070, epoch step:1440\n",
      "2022/03/18 19:04:12 - INFO - Distillation -  Global step: 14175, epoch step:1545\n",
      "2022/03/18 19:04:22 - INFO - Distillation -  Global step: 14280, epoch step:1650\n",
      "2022/03/18 19:04:31 - INFO - Distillation -  Global step: 14385, epoch step:1755\n",
      "2022/03/18 19:04:41 - INFO - Distillation -  Global step: 14490, epoch step:1860\n",
      "2022/03/18 19:04:50 - INFO - Distillation -  Global step: 14595, epoch step:1965\n",
      "2022/03/18 19:05:00 - INFO - Distillation -  Global step: 14700, epoch step:2070\n",
      "2022/03/18 19:05:03 - INFO - Distillation -  Saving at global step 14735, epoch step 2105 epoch 7\n",
      "2022/03/18 19:05:03 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:05:04 - INFO - Distillation -  {'sst-2': {'acc': 0.8084862385321101}}\n",
      "2022/03/18 19:05:04 - INFO - Distillation -  Epoch 7 finished\n",
      "2022/03/18 19:05:04 - INFO - Distillation -  Epoch 8\n",
      "2022/03/18 19:05:04 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:05:10 - INFO - Distillation -  Global step: 14805, epoch step:70\n",
      "2022/03/18 19:05:20 - INFO - Distillation -  Global step: 14910, epoch step:175\n",
      "2022/03/18 19:05:29 - INFO - Distillation -  Global step: 15015, epoch step:280\n",
      "2022/03/18 19:05:38 - INFO - Distillation -  Global step: 15120, epoch step:385\n",
      "2022/03/18 19:05:48 - INFO - Distillation -  Global step: 15225, epoch step:490\n",
      "2022/03/18 19:05:57 - INFO - Distillation -  Global step: 15330, epoch step:595\n",
      "2022/03/18 19:06:07 - INFO - Distillation -  Global step: 15435, epoch step:700\n",
      "2022/03/18 19:06:16 - INFO - Distillation -  Global step: 15540, epoch step:805\n",
      "2022/03/18 19:06:26 - INFO - Distillation -  Global step: 15645, epoch step:910\n",
      "2022/03/18 19:06:35 - INFO - Distillation -  Global step: 15750, epoch step:1015\n",
      "2022/03/18 19:06:44 - INFO - Distillation -  Global step: 15855, epoch step:1120\n",
      "2022/03/18 19:06:53 - INFO - Distillation -  Global step: 15960, epoch step:1225\n",
      "2022/03/18 19:07:03 - INFO - Distillation -  Global step: 16065, epoch step:1330\n",
      "2022/03/18 19:07:12 - INFO - Distillation -  Global step: 16170, epoch step:1435\n",
      "2022/03/18 19:07:21 - INFO - Distillation -  Global step: 16275, epoch step:1540\n",
      "2022/03/18 19:07:30 - INFO - Distillation -  Global step: 16380, epoch step:1645\n",
      "2022/03/18 19:07:40 - INFO - Distillation -  Global step: 16485, epoch step:1750\n",
      "2022/03/18 19:07:49 - INFO - Distillation -  Global step: 16590, epoch step:1855\n",
      "2022/03/18 19:07:59 - INFO - Distillation -  Global step: 16695, epoch step:1960\n",
      "2022/03/18 19:08:08 - INFO - Distillation -  Global step: 16800, epoch step:2065\n",
      "2022/03/18 19:08:11 - INFO - Distillation -  Saving at global step 16840, epoch step 2105 epoch 8\n",
      "2022/03/18 19:08:14 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:08:14 - INFO - Distillation -  {'sst-2': {'acc': 0.8119266055045872}}\n",
      "2022/03/18 19:08:14 - INFO - Distillation -  Epoch 8 finished\n",
      "2022/03/18 19:08:14 - INFO - Distillation -  Epoch 9\n",
      "2022/03/18 19:08:14 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:08:20 - INFO - Distillation -  Global step: 16905, epoch step:65\n",
      "2022/03/18 19:08:29 - INFO - Distillation -  Global step: 17010, epoch step:170\n",
      "2022/03/18 19:08:39 - INFO - Distillation -  Global step: 17115, epoch step:275\n",
      "2022/03/18 19:08:48 - INFO - Distillation -  Global step: 17220, epoch step:380\n",
      "2022/03/18 19:08:58 - INFO - Distillation -  Global step: 17325, epoch step:485\n",
      "2022/03/18 19:09:07 - INFO - Distillation -  Global step: 17430, epoch step:590\n",
      "2022/03/18 19:09:17 - INFO - Distillation -  Global step: 17535, epoch step:695\n",
      "2022/03/18 19:09:26 - INFO - Distillation -  Global step: 17640, epoch step:800\n",
      "2022/03/18 19:09:36 - INFO - Distillation -  Global step: 17745, epoch step:905\n",
      "2022/03/18 19:09:46 - INFO - Distillation -  Global step: 17850, epoch step:1010\n",
      "2022/03/18 19:09:55 - INFO - Distillation -  Global step: 17955, epoch step:1115\n",
      "2022/03/18 19:10:05 - INFO - Distillation -  Global step: 18060, epoch step:1220\n",
      "2022/03/18 19:10:14 - INFO - Distillation -  Global step: 18165, epoch step:1325\n",
      "2022/03/18 19:10:24 - INFO - Distillation -  Global step: 18270, epoch step:1430\n",
      "2022/03/18 19:10:33 - INFO - Distillation -  Global step: 18375, epoch step:1535\n",
      "2022/03/18 19:10:42 - INFO - Distillation -  Global step: 18480, epoch step:1640\n",
      "2022/03/18 19:10:52 - INFO - Distillation -  Global step: 18585, epoch step:1745\n",
      "2022/03/18 19:11:01 - INFO - Distillation -  Global step: 18690, epoch step:1850\n",
      "2022/03/18 19:11:11 - INFO - Distillation -  Global step: 18795, epoch step:1955\n",
      "2022/03/18 19:11:20 - INFO - Distillation -  Global step: 18900, epoch step:2060\n",
      "2022/03/18 19:11:24 - INFO - Distillation -  Saving at global step 18945, epoch step 2105 epoch 9\n",
      "2022/03/18 19:11:25 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:11:26 - INFO - Distillation -  {'sst-2': {'acc': 0.8004587155963303}}\n",
      "2022/03/18 19:11:26 - INFO - Distillation -  Epoch 9 finished\n",
      "2022/03/18 19:11:26 - INFO - Distillation -  Epoch 10\n",
      "2022/03/18 19:11:26 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:11:31 - INFO - Distillation -  Global step: 19005, epoch step:60\n",
      "2022/03/18 19:11:41 - INFO - Distillation -  Global step: 19110, epoch step:165\n",
      "2022/03/18 19:11:50 - INFO - Distillation -  Global step: 19215, epoch step:270\n",
      "2022/03/18 19:12:00 - INFO - Distillation -  Global step: 19320, epoch step:375\n",
      "2022/03/18 19:12:09 - INFO - Distillation -  Global step: 19425, epoch step:480\n",
      "2022/03/18 19:12:19 - INFO - Distillation -  Global step: 19530, epoch step:585\n",
      "2022/03/18 19:12:28 - INFO - Distillation -  Global step: 19635, epoch step:690\n",
      "2022/03/18 19:12:38 - INFO - Distillation -  Global step: 19740, epoch step:795\n",
      "2022/03/18 19:12:47 - INFO - Distillation -  Global step: 19845, epoch step:900\n",
      "2022/03/18 19:12:57 - INFO - Distillation -  Global step: 19950, epoch step:1005\n",
      "2022/03/18 19:13:06 - INFO - Distillation -  Global step: 20055, epoch step:1110\n",
      "2022/03/18 19:13:15 - INFO - Distillation -  Global step: 20160, epoch step:1215\n",
      "2022/03/18 19:13:25 - INFO - Distillation -  Global step: 20265, epoch step:1320\n",
      "2022/03/18 19:13:34 - INFO - Distillation -  Global step: 20370, epoch step:1425\n",
      "2022/03/18 19:13:44 - INFO - Distillation -  Global step: 20475, epoch step:1530\n",
      "2022/03/18 19:13:53 - INFO - Distillation -  Global step: 20580, epoch step:1635\n",
      "2022/03/18 19:14:03 - INFO - Distillation -  Global step: 20685, epoch step:1740\n",
      "2022/03/18 19:14:12 - INFO - Distillation -  Global step: 20790, epoch step:1845\n",
      "2022/03/18 19:14:22 - INFO - Distillation -  Global step: 20895, epoch step:1950\n",
      "2022/03/18 19:14:31 - INFO - Distillation -  Global step: 21000, epoch step:2055\n",
      "2022/03/18 19:14:36 - INFO - Distillation -  Saving at global step 21050, epoch step 2105 epoch 10\n",
      "2022/03/18 19:14:37 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:14:37 - INFO - Distillation -  {'sst-2': {'acc': 0.8142201834862385}}\n",
      "2022/03/18 19:14:37 - INFO - Distillation -  Epoch 10 finished\n",
      "2022/03/18 19:14:37 - INFO - Distillation -  Epoch 11\n",
      "2022/03/18 19:14:37 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:14:42 - INFO - Distillation -  Global step: 21105, epoch step:55\n",
      "2022/03/18 19:14:51 - INFO - Distillation -  Global step: 21210, epoch step:160\n",
      "2022/03/18 19:15:01 - INFO - Distillation -  Global step: 21315, epoch step:265\n",
      "2022/03/18 19:15:10 - INFO - Distillation -  Global step: 21420, epoch step:370\n",
      "2022/03/18 19:15:20 - INFO - Distillation -  Global step: 21525, epoch step:475\n",
      "2022/03/18 19:15:29 - INFO - Distillation -  Global step: 21630, epoch step:580\n",
      "2022/03/18 19:15:38 - INFO - Distillation -  Global step: 21735, epoch step:685\n",
      "2022/03/18 19:15:48 - INFO - Distillation -  Global step: 21840, epoch step:790\n",
      "2022/03/18 19:15:57 - INFO - Distillation -  Global step: 21945, epoch step:895\n",
      "2022/03/18 19:16:07 - INFO - Distillation -  Global step: 22050, epoch step:1000\n",
      "2022/03/18 19:16:16 - INFO - Distillation -  Global step: 22155, epoch step:1105\n",
      "2022/03/18 19:16:25 - INFO - Distillation -  Global step: 22260, epoch step:1210\n",
      "2022/03/18 19:16:35 - INFO - Distillation -  Global step: 22365, epoch step:1315\n",
      "2022/03/18 19:16:44 - INFO - Distillation -  Global step: 22470, epoch step:1420\n",
      "2022/03/18 19:16:53 - INFO - Distillation -  Global step: 22575, epoch step:1525\n",
      "2022/03/18 19:17:03 - INFO - Distillation -  Global step: 22680, epoch step:1630\n",
      "2022/03/18 19:17:12 - INFO - Distillation -  Global step: 22785, epoch step:1735\n",
      "2022/03/18 19:17:21 - INFO - Distillation -  Global step: 22890, epoch step:1840\n",
      "2022/03/18 19:17:31 - INFO - Distillation -  Global step: 22995, epoch step:1945\n",
      "2022/03/18 19:17:40 - INFO - Distillation -  Global step: 23100, epoch step:2050\n",
      "2022/03/18 19:17:45 - INFO - Distillation -  Saving at global step 23155, epoch step 2105 epoch 11\n",
      "2022/03/18 19:17:46 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:17:47 - INFO - Distillation -  {'sst-2': {'acc': 0.819954128440367}}\n",
      "2022/03/18 19:17:47 - INFO - Distillation -  Epoch 11 finished\n",
      "2022/03/18 19:17:47 - INFO - Distillation -  Epoch 12\n",
      "2022/03/18 19:17:47 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:17:51 - INFO - Distillation -  Global step: 23205, epoch step:50\n",
      "2022/03/18 19:18:00 - INFO - Distillation -  Global step: 23310, epoch step:155\n",
      "2022/03/18 19:18:10 - INFO - Distillation -  Global step: 23415, epoch step:260\n",
      "2022/03/18 19:18:19 - INFO - Distillation -  Global step: 23520, epoch step:365\n",
      "2022/03/18 19:18:29 - INFO - Distillation -  Global step: 23625, epoch step:470\n",
      "2022/03/18 19:18:38 - INFO - Distillation -  Global step: 23730, epoch step:575\n",
      "2022/03/18 19:18:48 - INFO - Distillation -  Global step: 23835, epoch step:680\n",
      "2022/03/18 19:18:57 - INFO - Distillation -  Global step: 23940, epoch step:785\n",
      "2022/03/18 19:19:07 - INFO - Distillation -  Global step: 24045, epoch step:890\n",
      "2022/03/18 19:19:16 - INFO - Distillation -  Global step: 24150, epoch step:995\n",
      "2022/03/18 19:19:26 - INFO - Distillation -  Global step: 24255, epoch step:1100\n",
      "2022/03/18 19:19:35 - INFO - Distillation -  Global step: 24360, epoch step:1205\n",
      "2022/03/18 19:19:45 - INFO - Distillation -  Global step: 24465, epoch step:1310\n",
      "2022/03/18 19:19:54 - INFO - Distillation -  Global step: 24570, epoch step:1415\n",
      "2022/03/18 19:20:03 - INFO - Distillation -  Global step: 24675, epoch step:1520\n",
      "2022/03/18 19:20:13 - INFO - Distillation -  Global step: 24780, epoch step:1625\n",
      "2022/03/18 19:20:22 - INFO - Distillation -  Global step: 24885, epoch step:1730\n",
      "2022/03/18 19:20:32 - INFO - Distillation -  Global step: 24990, epoch step:1835\n",
      "2022/03/18 19:20:41 - INFO - Distillation -  Global step: 25095, epoch step:1940\n",
      "2022/03/18 19:20:51 - INFO - Distillation -  Global step: 25200, epoch step:2045\n",
      "2022/03/18 19:20:56 - INFO - Distillation -  Saving at global step 25260, epoch step 2105 epoch 12\n",
      "2022/03/18 19:20:58 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:20:58 - INFO - Distillation -  {'sst-2': {'acc': 0.8176605504587156}}\n",
      "2022/03/18 19:20:58 - INFO - Distillation -  Epoch 12 finished\n",
      "2022/03/18 19:20:58 - INFO - Distillation -  Epoch 13\n",
      "2022/03/18 19:20:58 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:21:02 - INFO - Distillation -  Global step: 25305, epoch step:45\n",
      "2022/03/18 19:21:12 - INFO - Distillation -  Global step: 25410, epoch step:150\n",
      "2022/03/18 19:21:21 - INFO - Distillation -  Global step: 25515, epoch step:255\n",
      "2022/03/18 19:21:31 - INFO - Distillation -  Global step: 25620, epoch step:360\n",
      "2022/03/18 19:21:40 - INFO - Distillation -  Global step: 25725, epoch step:465\n",
      "2022/03/18 19:21:50 - INFO - Distillation -  Global step: 25830, epoch step:570\n",
      "2022/03/18 19:21:59 - INFO - Distillation -  Global step: 25935, epoch step:675\n",
      "2022/03/18 19:22:08 - INFO - Distillation -  Global step: 26040, epoch step:780\n",
      "2022/03/18 19:22:18 - INFO - Distillation -  Global step: 26145, epoch step:885\n",
      "2022/03/18 19:22:27 - INFO - Distillation -  Global step: 26250, epoch step:990\n",
      "2022/03/18 19:22:36 - INFO - Distillation -  Global step: 26355, epoch step:1095\n",
      "2022/03/18 19:22:45 - INFO - Distillation -  Global step: 26460, epoch step:1200\n",
      "2022/03/18 19:22:55 - INFO - Distillation -  Global step: 26565, epoch step:1305\n",
      "2022/03/18 19:23:04 - INFO - Distillation -  Global step: 26670, epoch step:1410\n",
      "2022/03/18 19:23:13 - INFO - Distillation -  Global step: 26775, epoch step:1515\n",
      "2022/03/18 19:23:23 - INFO - Distillation -  Global step: 26880, epoch step:1620\n",
      "2022/03/18 19:23:32 - INFO - Distillation -  Global step: 26985, epoch step:1725\n",
      "2022/03/18 19:23:42 - INFO - Distillation -  Global step: 27090, epoch step:1830\n",
      "2022/03/18 19:23:51 - INFO - Distillation -  Global step: 27195, epoch step:1935\n",
      "2022/03/18 19:24:00 - INFO - Distillation -  Global step: 27300, epoch step:2040\n",
      "2022/03/18 19:24:06 - INFO - Distillation -  Saving at global step 27365, epoch step 2105 epoch 13\n",
      "2022/03/18 19:24:07 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:24:08 - INFO - Distillation -  {'sst-2': {'acc': 0.8211009174311926}}\n",
      "2022/03/18 19:24:08 - INFO - Distillation -  Epoch 13 finished\n",
      "2022/03/18 19:24:08 - INFO - Distillation -  Epoch 14\n",
      "2022/03/18 19:24:08 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:24:11 - INFO - Distillation -  Global step: 27405, epoch step:40\n",
      "2022/03/18 19:24:20 - INFO - Distillation -  Global step: 27510, epoch step:145\n",
      "2022/03/18 19:24:30 - INFO - Distillation -  Global step: 27615, epoch step:250\n",
      "2022/03/18 19:24:39 - INFO - Distillation -  Global step: 27720, epoch step:355\n",
      "2022/03/18 19:24:48 - INFO - Distillation -  Global step: 27825, epoch step:460\n",
      "2022/03/18 19:24:57 - INFO - Distillation -  Global step: 27930, epoch step:565\n",
      "2022/03/18 19:25:07 - INFO - Distillation -  Global step: 28035, epoch step:670\n",
      "2022/03/18 19:25:16 - INFO - Distillation -  Global step: 28140, epoch step:775\n",
      "2022/03/18 19:25:25 - INFO - Distillation -  Global step: 28245, epoch step:880\n",
      "2022/03/18 19:25:34 - INFO - Distillation -  Global step: 28350, epoch step:985\n",
      "2022/03/18 19:25:43 - INFO - Distillation -  Global step: 28455, epoch step:1090\n",
      "2022/03/18 19:25:53 - INFO - Distillation -  Global step: 28560, epoch step:1195\n",
      "2022/03/18 19:26:02 - INFO - Distillation -  Global step: 28665, epoch step:1300\n",
      "2022/03/18 19:26:11 - INFO - Distillation -  Global step: 28770, epoch step:1405\n",
      "2022/03/18 19:26:21 - INFO - Distillation -  Global step: 28875, epoch step:1510\n",
      "2022/03/18 19:26:30 - INFO - Distillation -  Global step: 28980, epoch step:1615\n",
      "2022/03/18 19:26:40 - INFO - Distillation -  Global step: 29085, epoch step:1720\n",
      "2022/03/18 19:26:49 - INFO - Distillation -  Global step: 29190, epoch step:1825\n",
      "2022/03/18 19:26:58 - INFO - Distillation -  Global step: 29295, epoch step:1930\n",
      "2022/03/18 19:27:08 - INFO - Distillation -  Global step: 29400, epoch step:2035\n",
      "2022/03/18 19:27:14 - INFO - Distillation -  Saving at global step 29470, epoch step 2105 epoch 14\n",
      "2022/03/18 19:27:14 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:27:15 - INFO - Distillation -  {'sst-2': {'acc': 0.8107798165137615}}\n",
      "2022/03/18 19:27:15 - INFO - Distillation -  Epoch 14 finished\n",
      "2022/03/18 19:27:15 - INFO - Distillation -  Epoch 15\n",
      "2022/03/18 19:27:15 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:27:18 - INFO - Distillation -  Global step: 29505, epoch step:35\n",
      "2022/03/18 19:27:28 - INFO - Distillation -  Global step: 29610, epoch step:140\n",
      "2022/03/18 19:27:37 - INFO - Distillation -  Global step: 29715, epoch step:245\n",
      "2022/03/18 19:27:47 - INFO - Distillation -  Global step: 29820, epoch step:350\n",
      "2022/03/18 19:27:56 - INFO - Distillation -  Global step: 29925, epoch step:455\n",
      "2022/03/18 19:28:06 - INFO - Distillation -  Global step: 30030, epoch step:560\n",
      "2022/03/18 19:28:15 - INFO - Distillation -  Global step: 30135, epoch step:665\n",
      "2022/03/18 19:28:24 - INFO - Distillation -  Global step: 30240, epoch step:770\n",
      "2022/03/18 19:28:34 - INFO - Distillation -  Global step: 30345, epoch step:875\n",
      "2022/03/18 19:28:43 - INFO - Distillation -  Global step: 30450, epoch step:980\n",
      "2022/03/18 19:28:53 - INFO - Distillation -  Global step: 30555, epoch step:1085\n",
      "2022/03/18 19:29:02 - INFO - Distillation -  Global step: 30660, epoch step:1190\n",
      "2022/03/18 19:29:12 - INFO - Distillation -  Global step: 30765, epoch step:1295\n",
      "2022/03/18 19:29:21 - INFO - Distillation -  Global step: 30870, epoch step:1400\n",
      "2022/03/18 19:29:30 - INFO - Distillation -  Global step: 30975, epoch step:1505\n",
      "2022/03/18 19:29:40 - INFO - Distillation -  Global step: 31080, epoch step:1610\n",
      "2022/03/18 19:29:49 - INFO - Distillation -  Global step: 31185, epoch step:1715\n",
      "2022/03/18 19:29:59 - INFO - Distillation -  Global step: 31290, epoch step:1820\n",
      "2022/03/18 19:30:08 - INFO - Distillation -  Global step: 31395, epoch step:1925\n",
      "2022/03/18 19:30:17 - INFO - Distillation -  Global step: 31500, epoch step:2030\n",
      "2022/03/18 19:30:24 - INFO - Distillation -  Saving at global step 31575, epoch step 2105 epoch 15\n",
      "2022/03/18 19:30:25 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:30:25 - INFO - Distillation -  {'sst-2': {'acc': 0.8142201834862385}}\n",
      "2022/03/18 19:30:25 - INFO - Distillation -  Epoch 15 finished\n",
      "2022/03/18 19:30:25 - INFO - Distillation -  Epoch 16\n",
      "2022/03/18 19:30:25 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:30:28 - INFO - Distillation -  Global step: 31605, epoch step:30\n",
      "2022/03/18 19:30:37 - INFO - Distillation -  Global step: 31710, epoch step:135\n",
      "2022/03/18 19:30:47 - INFO - Distillation -  Global step: 31815, epoch step:240\n",
      "2022/03/18 19:30:56 - INFO - Distillation -  Global step: 31920, epoch step:345\n",
      "2022/03/18 19:31:06 - INFO - Distillation -  Global step: 32025, epoch step:450\n",
      "2022/03/18 19:31:15 - INFO - Distillation -  Global step: 32130, epoch step:555\n",
      "2022/03/18 19:31:24 - INFO - Distillation -  Global step: 32235, epoch step:660\n",
      "2022/03/18 19:31:34 - INFO - Distillation -  Global step: 32340, epoch step:765\n",
      "2022/03/18 19:31:43 - INFO - Distillation -  Global step: 32445, epoch step:870\n",
      "2022/03/18 19:31:53 - INFO - Distillation -  Global step: 32550, epoch step:975\n",
      "2022/03/18 19:32:02 - INFO - Distillation -  Global step: 32655, epoch step:1080\n",
      "2022/03/18 19:32:12 - INFO - Distillation -  Global step: 32760, epoch step:1185\n",
      "2022/03/18 19:32:21 - INFO - Distillation -  Global step: 32865, epoch step:1290\n",
      "2022/03/18 19:32:30 - INFO - Distillation -  Global step: 32970, epoch step:1395\n",
      "2022/03/18 19:32:39 - INFO - Distillation -  Global step: 33075, epoch step:1500\n",
      "2022/03/18 19:32:49 - INFO - Distillation -  Global step: 33180, epoch step:1605\n",
      "2022/03/18 19:32:59 - INFO - Distillation -  Global step: 33285, epoch step:1710\n",
      "2022/03/18 19:33:08 - INFO - Distillation -  Global step: 33390, epoch step:1815\n",
      "2022/03/18 19:33:17 - INFO - Distillation -  Global step: 33495, epoch step:1920\n",
      "2022/03/18 19:33:27 - INFO - Distillation -  Global step: 33600, epoch step:2025\n",
      "2022/03/18 19:33:34 - INFO - Distillation -  Saving at global step 33680, epoch step 2105 epoch 16\n",
      "2022/03/18 19:33:35 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:33:36 - INFO - Distillation -  {'sst-2': {'acc': 0.823394495412844}}\n",
      "2022/03/18 19:33:36 - INFO - Distillation -  Epoch 16 finished\n",
      "2022/03/18 19:33:36 - INFO - Distillation -  Epoch 17\n",
      "2022/03/18 19:33:36 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:33:38 - INFO - Distillation -  Global step: 33705, epoch step:25\n",
      "2022/03/18 19:33:47 - INFO - Distillation -  Global step: 33810, epoch step:130\n",
      "2022/03/18 19:33:57 - INFO - Distillation -  Global step: 33915, epoch step:235\n",
      "2022/03/18 19:34:06 - INFO - Distillation -  Global step: 34020, epoch step:340\n",
      "2022/03/18 19:34:16 - INFO - Distillation -  Global step: 34125, epoch step:445\n",
      "2022/03/18 19:34:25 - INFO - Distillation -  Global step: 34230, epoch step:550\n",
      "2022/03/18 19:34:35 - INFO - Distillation -  Global step: 34335, epoch step:655\n",
      "2022/03/18 19:34:44 - INFO - Distillation -  Global step: 34440, epoch step:760\n",
      "2022/03/18 19:34:54 - INFO - Distillation -  Global step: 34545, epoch step:865\n",
      "2022/03/18 19:35:03 - INFO - Distillation -  Global step: 34650, epoch step:970\n",
      "2022/03/18 19:35:12 - INFO - Distillation -  Global step: 34755, epoch step:1075\n",
      "2022/03/18 19:35:22 - INFO - Distillation -  Global step: 34860, epoch step:1180\n",
      "2022/03/18 19:35:31 - INFO - Distillation -  Global step: 34965, epoch step:1285\n",
      "2022/03/18 19:35:41 - INFO - Distillation -  Global step: 35070, epoch step:1390\n",
      "2022/03/18 19:35:50 - INFO - Distillation -  Global step: 35175, epoch step:1495\n",
      "2022/03/18 19:35:59 - INFO - Distillation -  Global step: 35280, epoch step:1600\n",
      "2022/03/18 19:36:09 - INFO - Distillation -  Global step: 35385, epoch step:1705\n",
      "2022/03/18 19:36:19 - INFO - Distillation -  Global step: 35490, epoch step:1810\n",
      "2022/03/18 19:36:28 - INFO - Distillation -  Global step: 35595, epoch step:1915\n",
      "2022/03/18 19:36:37 - INFO - Distillation -  Global step: 35700, epoch step:2020\n",
      "2022/03/18 19:36:45 - INFO - Distillation -  Saving at global step 35785, epoch step 2105 epoch 17\n",
      "2022/03/18 19:36:45 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:36:46 - INFO - Distillation -  {'sst-2': {'acc': 0.8222477064220184}}\n",
      "2022/03/18 19:36:46 - INFO - Distillation -  Epoch 17 finished\n",
      "2022/03/18 19:36:46 - INFO - Distillation -  Epoch 18\n",
      "2022/03/18 19:36:46 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:36:48 - INFO - Distillation -  Global step: 35805, epoch step:20\n",
      "2022/03/18 19:36:57 - INFO - Distillation -  Global step: 35910, epoch step:125\n",
      "2022/03/18 19:37:07 - INFO - Distillation -  Global step: 36015, epoch step:230\n",
      "2022/03/18 19:37:16 - INFO - Distillation -  Global step: 36120, epoch step:335\n",
      "2022/03/18 19:37:25 - INFO - Distillation -  Global step: 36225, epoch step:440\n",
      "2022/03/18 19:37:35 - INFO - Distillation -  Global step: 36330, epoch step:545\n",
      "2022/03/18 19:37:44 - INFO - Distillation -  Global step: 36435, epoch step:650\n",
      "2022/03/18 19:37:54 - INFO - Distillation -  Global step: 36540, epoch step:755\n",
      "2022/03/18 19:38:03 - INFO - Distillation -  Global step: 36645, epoch step:860\n",
      "2022/03/18 19:38:13 - INFO - Distillation -  Global step: 36750, epoch step:965\n",
      "2022/03/18 19:38:22 - INFO - Distillation -  Global step: 36855, epoch step:1070\n",
      "2022/03/18 19:38:31 - INFO - Distillation -  Global step: 36960, epoch step:1175\n",
      "2022/03/18 19:38:41 - INFO - Distillation -  Global step: 37065, epoch step:1280\n",
      "2022/03/18 19:38:50 - INFO - Distillation -  Global step: 37170, epoch step:1385\n",
      "2022/03/18 19:38:59 - INFO - Distillation -  Global step: 37275, epoch step:1490\n",
      "2022/03/18 19:39:09 - INFO - Distillation -  Global step: 37380, epoch step:1595\n",
      "2022/03/18 19:39:18 - INFO - Distillation -  Global step: 37485, epoch step:1700\n",
      "2022/03/18 19:39:27 - INFO - Distillation -  Global step: 37590, epoch step:1805\n",
      "2022/03/18 19:39:37 - INFO - Distillation -  Global step: 37695, epoch step:1910\n",
      "2022/03/18 19:39:46 - INFO - Distillation -  Global step: 37800, epoch step:2015\n",
      "2022/03/18 19:39:54 - INFO - Distillation -  Saving at global step 37890, epoch step 2105 epoch 18\n",
      "2022/03/18 19:39:55 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:39:56 - INFO - Distillation -  {'sst-2': {'acc': 0.8302752293577982}}\n",
      "2022/03/18 19:39:56 - INFO - Distillation -  Epoch 18 finished\n",
      "2022/03/18 19:39:56 - INFO - Distillation -  Epoch 19\n",
      "2022/03/18 19:39:56 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:39:57 - INFO - Distillation -  Global step: 37905, epoch step:15\n",
      "2022/03/18 19:40:07 - INFO - Distillation -  Global step: 38010, epoch step:120\n",
      "2022/03/18 19:40:16 - INFO - Distillation -  Global step: 38115, epoch step:225\n",
      "2022/03/18 19:40:25 - INFO - Distillation -  Global step: 38220, epoch step:330\n",
      "2022/03/18 19:40:35 - INFO - Distillation -  Global step: 38325, epoch step:435\n",
      "2022/03/18 19:40:44 - INFO - Distillation -  Global step: 38430, epoch step:540\n",
      "2022/03/18 19:40:53 - INFO - Distillation -  Global step: 38535, epoch step:645\n",
      "2022/03/18 19:41:03 - INFO - Distillation -  Global step: 38640, epoch step:750\n",
      "2022/03/18 19:41:13 - INFO - Distillation -  Global step: 38745, epoch step:855\n",
      "2022/03/18 19:41:22 - INFO - Distillation -  Global step: 38850, epoch step:960\n",
      "2022/03/18 19:41:31 - INFO - Distillation -  Global step: 38955, epoch step:1065\n",
      "2022/03/18 19:41:41 - INFO - Distillation -  Global step: 39060, epoch step:1170\n",
      "2022/03/18 19:41:50 - INFO - Distillation -  Global step: 39165, epoch step:1275\n",
      "2022/03/18 19:42:00 - INFO - Distillation -  Global step: 39270, epoch step:1380\n",
      "2022/03/18 19:42:09 - INFO - Distillation -  Global step: 39375, epoch step:1485\n",
      "2022/03/18 19:42:18 - INFO - Distillation -  Global step: 39480, epoch step:1590\n",
      "2022/03/18 19:42:27 - INFO - Distillation -  Global step: 39585, epoch step:1695\n",
      "2022/03/18 19:42:37 - INFO - Distillation -  Global step: 39690, epoch step:1800\n",
      "2022/03/18 19:42:46 - INFO - Distillation -  Global step: 39795, epoch step:1905\n",
      "2022/03/18 19:42:55 - INFO - Distillation -  Global step: 39900, epoch step:2010\n",
      "2022/03/18 19:43:04 - INFO - Distillation -  Saving at global step 39995, epoch step 2105 epoch 19\n",
      "2022/03/18 19:43:05 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:43:05 - INFO - Distillation -  {'sst-2': {'acc': 0.8325688073394495}}\n",
      "2022/03/18 19:43:05 - INFO - Distillation -  Epoch 19 finished\n",
      "2022/03/18 19:43:05 - INFO - Distillation -  Epoch 20\n",
      "2022/03/18 19:43:05 - INFO - Distillation -  Length of current epoch in forward batch: 2105\n",
      "2022/03/18 19:43:06 - INFO - Distillation -  Global step: 40005, epoch step:10\n",
      "2022/03/18 19:43:16 - INFO - Distillation -  Global step: 40110, epoch step:115\n",
      "2022/03/18 19:43:25 - INFO - Distillation -  Global step: 40215, epoch step:220\n",
      "2022/03/18 19:43:35 - INFO - Distillation -  Global step: 40320, epoch step:325\n",
      "2022/03/18 19:43:44 - INFO - Distillation -  Global step: 40425, epoch step:430\n",
      "2022/03/18 19:43:54 - INFO - Distillation -  Global step: 40530, epoch step:535\n",
      "2022/03/18 19:44:03 - INFO - Distillation -  Global step: 40635, epoch step:640\n",
      "2022/03/18 19:44:13 - INFO - Distillation -  Global step: 40740, epoch step:745\n",
      "2022/03/18 19:44:22 - INFO - Distillation -  Global step: 40845, epoch step:850\n",
      "2022/03/18 19:44:31 - INFO - Distillation -  Global step: 40950, epoch step:955\n",
      "2022/03/18 19:44:40 - INFO - Distillation -  Global step: 41055, epoch step:1060\n",
      "2022/03/18 19:44:50 - INFO - Distillation -  Global step: 41160, epoch step:1165\n",
      "2022/03/18 19:44:59 - INFO - Distillation -  Global step: 41265, epoch step:1270\n",
      "2022/03/18 19:45:08 - INFO - Distillation -  Global step: 41370, epoch step:1375\n",
      "2022/03/18 19:45:18 - INFO - Distillation -  Global step: 41475, epoch step:1480\n",
      "2022/03/18 19:45:27 - INFO - Distillation -  Global step: 41580, epoch step:1585\n",
      "2022/03/18 19:45:37 - INFO - Distillation -  Global step: 41685, epoch step:1690\n",
      "2022/03/18 19:45:46 - INFO - Distillation -  Global step: 41790, epoch step:1795\n",
      "2022/03/18 19:45:56 - INFO - Distillation -  Global step: 41895, epoch step:1900\n",
      "2022/03/18 19:46:05 - INFO - Distillation -  Global step: 42000, epoch step:2005\n",
      "2022/03/18 19:46:14 - INFO - Distillation -  Saving at global step 42100, epoch step 2105 epoch 20\n",
      "2022/03/18 19:46:15 - INFO - Distillation -  Running callback function...\n",
      "2022/03/18 19:46:16 - INFO - Distillation -  {'sst-2': {'acc': 0.8337155963302753}}\n",
      "2022/03/18 19:46:16 - INFO - Distillation -  Epoch 20 finished\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "distill_config = DistillationConfig(\n",
    "    intermediate_matches=[    \n",
    "     {'layer_T':0, 'layer_S':0, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1},\n",
    "     {'layer_T':8, 'layer_S':2, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1}])\n",
    "train_config = TrainingConfig(device=device)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "task_name = \"sst-2\"\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "output_dir = \"outputs/\" + task_name + \"/\" \n",
    "\n",
    "callback_func = partial(predict, eval_datasets=[val_dataset], output_dir=output_dir,task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device)\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = BertForSequenceClassification(bert_config_T3)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models_old/gs2105.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7901376146788991}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
