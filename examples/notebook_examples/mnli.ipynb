{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'mnli', split='train')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_dataset = load_dataset('glue', 'mnli', split='validation_matched')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_mm_dataset = load_dataset('glue', 'mnli', split='validation_mismatched')\n",
    "test_dataset = load_dataset('glue', 'mnli', split='test_matched')#,cache_dir=\"/work/mhessent/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b84661f5fe95e1e2.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43d59c952978c460.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cce2791de0a64e6f.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d933186d36b8e3ab.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "val_mm_dataset = val_mm_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"ishan/bert-base-uncased-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6226276aad90cc7c.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f9f7bddc6b4c027d.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-39816bc57b32097a.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-033706f2a112a9eb.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_mm_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:][\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=13e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=100,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=13e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/mnli_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"/work/mhessent/master_thesis/eval_out/bert-base-uncased/mnli/lr3e-05_bs32_epochs3/checkpoint-36816\")\n",
    "torch.save(model.state_dict(), 'outputs/hub_mnli_teacher_model.pt')\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "bert_config_T3 = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config_L3_v2.json')#相对路径\n",
    "bert_config_T3.output_hidden_states = True\n",
    "bert_config_T3.num_labels = 3\n",
    "bert_config_T3.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "continue_training = False\n",
    "student_model = BertForSequenceClassification(bert_config_T3)\n",
    "if continue_training:\n",
    "    student_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs490880.pkl'))\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\n",
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2022/03/24 17:00:29 - INFO - Distillation -  Training steps per epoch: 3068\n",
      "2022/03/24 17:00:29 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/03/24 17:00:29 - INFO - Distillation -  Epoch 1\n",
      "2022/03/24 17:00:29 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/03/24 17:01:39 - INFO - Distillation -  Global step: 153, epoch step:153\n",
      "2022/03/24 17:02:50 - INFO - Distillation -  Global step: 306, epoch step:306\n",
      "2022/03/24 17:04:00 - INFO - Distillation -  Global step: 459, epoch step:459\n",
      "2022/03/24 17:05:10 - INFO - Distillation -  Global step: 612, epoch step:612\n",
      "2022/03/24 17:06:20 - INFO - Distillation -  Global step: 765, epoch step:765\n",
      "2022/03/24 17:07:31 - INFO - Distillation -  Global step: 918, epoch step:918\n",
      "2022/03/24 17:08:41 - INFO - Distillation -  Global step: 1071, epoch step:1071\n",
      "2022/03/24 17:09:52 - INFO - Distillation -  Global step: 1224, epoch step:1224\n",
      "2022/03/24 17:10:57 - INFO - Distillation -  Global step: 1377, epoch step:1377\n",
      "2022/03/24 17:12:04 - INFO - Distillation -  Global step: 1530, epoch step:1530\n",
      "2022/03/24 17:13:13 - INFO - Distillation -  Global step: 1683, epoch step:1683\n",
      "2022/03/24 17:14:22 - INFO - Distillation -  Global step: 1836, epoch step:1836\n",
      "2022/03/24 17:15:31 - INFO - Distillation -  Global step: 1989, epoch step:1989\n",
      "2022/03/24 17:16:39 - INFO - Distillation -  Global step: 2142, epoch step:2142\n",
      "2022/03/24 17:17:50 - INFO - Distillation -  Global step: 2295, epoch step:2295\n",
      "2022/03/24 17:19:01 - INFO - Distillation -  Global step: 2448, epoch step:2448\n",
      "2022/03/24 17:20:10 - INFO - Distillation -  Global step: 2601, epoch step:2601\n",
      "2022/03/24 17:21:20 - INFO - Distillation -  Global step: 2754, epoch step:2754\n",
      "2022/03/24 17:22:30 - INFO - Distillation -  Global step: 2907, epoch step:2907\n",
      "2022/03/24 17:23:41 - INFO - Distillation -  Global step: 3060, epoch step:3060\n",
      "2022/03/24 17:23:45 - INFO - Distillation -  Saving at global step 3068, epoch step 3068 epoch 1\n",
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/nn/modules/module.py:1385: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "2022/03/24 17:23:46 - INFO - Distillation -  Running callback function...\n",
      "2022/03/24 17:24:06 - INFO - Distillation -  {'mnli': {'acc': 0.45206316861946}, 'mnli-mm': {'acc': 0.45850284784377543}}\n",
      "2022/03/24 17:24:06 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/03/24 17:24:06 - INFO - Distillation -  Epoch 2\n",
      "2022/03/24 17:24:06 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/03/24 17:25:13 - INFO - Distillation -  Global step: 3213, epoch step:145\n",
      "2022/03/24 17:26:23 - INFO - Distillation -  Global step: 3366, epoch step:298\n",
      "2022/03/24 17:27:33 - INFO - Distillation -  Global step: 3519, epoch step:451\n",
      "2022/03/24 17:28:42 - INFO - Distillation -  Global step: 3672, epoch step:604\n",
      "2022/03/24 17:29:53 - INFO - Distillation -  Global step: 3825, epoch step:757\n",
      "2022/03/24 17:31:02 - INFO - Distillation -  Global step: 3978, epoch step:910\n",
      "2022/03/24 17:32:10 - INFO - Distillation -  Global step: 4131, epoch step:1063\n",
      "2022/03/24 17:33:20 - INFO - Distillation -  Global step: 4284, epoch step:1216\n",
      "2022/03/24 17:34:30 - INFO - Distillation -  Global step: 4437, epoch step:1369\n",
      "2022/03/24 17:35:36 - INFO - Distillation -  Global step: 4590, epoch step:1522\n",
      "2022/03/24 17:36:19 - INFO - Distillation -  Global step: 4743, epoch step:1675\n",
      "2022/03/24 17:37:03 - INFO - Distillation -  Global step: 4896, epoch step:1828\n",
      "2022/03/24 17:37:47 - INFO - Distillation -  Global step: 5049, epoch step:1981\n",
      "2022/03/24 17:38:30 - INFO - Distillation -  Global step: 5202, epoch step:2134\n",
      "2022/03/24 17:39:13 - INFO - Distillation -  Global step: 5355, epoch step:2287\n",
      "2022/03/24 17:39:54 - INFO - Distillation -  Global step: 5508, epoch step:2440\n",
      "2022/03/24 17:40:36 - INFO - Distillation -  Global step: 5661, epoch step:2593\n",
      "2022/03/24 17:41:19 - INFO - Distillation -  Global step: 5814, epoch step:2746\n",
      "2022/03/24 17:42:02 - INFO - Distillation -  Global step: 5967, epoch step:2899\n",
      "2022/03/24 17:42:46 - INFO - Distillation -  Global step: 6120, epoch step:3052\n",
      "2022/03/24 17:42:51 - INFO - Distillation -  Saving at global step 6136, epoch step 3068 epoch 2\n",
      "2022/03/24 17:42:52 - INFO - Distillation -  Running callback function...\n",
      "2022/03/24 17:43:04 - INFO - Distillation -  {'mnli': {'acc': 0.5802343352012226}, 'mnli-mm': {'acc': 0.5751627339300244}}\n",
      "2022/03/24 17:43:04 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/03/24 17:43:04 - INFO - Distillation -  Epoch 3\n",
      "2022/03/24 17:43:04 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/03/24 17:43:43 - INFO - Distillation -  Global step: 6273, epoch step:137\n",
      "2022/03/24 17:44:26 - INFO - Distillation -  Global step: 6426, epoch step:290\n",
      "2022/03/24 17:45:10 - INFO - Distillation -  Global step: 6579, epoch step:443\n",
      "2022/03/24 17:45:54 - INFO - Distillation -  Global step: 6732, epoch step:596\n",
      "2022/03/24 17:46:38 - INFO - Distillation -  Global step: 6885, epoch step:749\n",
      "2022/03/24 17:47:22 - INFO - Distillation -  Global step: 7038, epoch step:902\n",
      "2022/03/24 17:48:05 - INFO - Distillation -  Global step: 7191, epoch step:1055\n",
      "2022/03/24 17:48:49 - INFO - Distillation -  Global step: 7344, epoch step:1208\n",
      "2022/03/24 17:49:32 - INFO - Distillation -  Global step: 7497, epoch step:1361\n",
      "2022/03/24 17:50:16 - INFO - Distillation -  Global step: 7650, epoch step:1514\n",
      "2022/03/24 17:51:00 - INFO - Distillation -  Global step: 7803, epoch step:1667\n",
      "2022/03/24 17:51:43 - INFO - Distillation -  Global step: 7956, epoch step:1820\n",
      "2022/03/24 17:52:27 - INFO - Distillation -  Global step: 8109, epoch step:1973\n",
      "2022/03/24 17:53:11 - INFO - Distillation -  Global step: 8262, epoch step:2126\n",
      "2022/03/24 17:53:55 - INFO - Distillation -  Global step: 8415, epoch step:2279\n",
      "2022/03/24 17:54:38 - INFO - Distillation -  Global step: 8568, epoch step:2432\n",
      "2022/03/24 17:55:22 - INFO - Distillation -  Global step: 8721, epoch step:2585\n",
      "2022/03/24 17:56:06 - INFO - Distillation -  Global step: 8874, epoch step:2738\n",
      "2022/03/24 17:56:50 - INFO - Distillation -  Global step: 9027, epoch step:2891\n",
      "2022/03/24 17:57:34 - INFO - Distillation -  Global step: 9180, epoch step:3044\n",
      "2022/03/24 17:57:41 - INFO - Distillation -  Saving at global step 9204, epoch step 3068 epoch 3\n",
      "2022/03/24 17:57:42 - INFO - Distillation -  Running callback function...\n",
      "2022/03/24 17:57:54 - INFO - Distillation -  {'mnli': {'acc': 0.6127356087620989}, 'mnli-mm': {'acc': 0.6153376729048007}}\n",
      "2022/03/24 17:57:54 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/03/24 17:57:54 - INFO - Distillation -  Epoch 4\n",
      "2022/03/24 17:57:54 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/03/24 17:58:31 - INFO - Distillation -  Global step: 9333, epoch step:129\n",
      "2022/03/24 17:59:15 - INFO - Distillation -  Global step: 9486, epoch step:282\n",
      "2022/03/24 17:59:58 - INFO - Distillation -  Global step: 9639, epoch step:435\n",
      "2022/03/24 18:00:42 - INFO - Distillation -  Global step: 9792, epoch step:588\n",
      "2022/03/24 18:01:26 - INFO - Distillation -  Global step: 9945, epoch step:741\n",
      "2022/03/24 18:02:09 - INFO - Distillation -  Global step: 10098, epoch step:894\n",
      "2022/03/24 18:02:53 - INFO - Distillation -  Global step: 10251, epoch step:1047\n",
      "2022/03/24 18:03:37 - INFO - Distillation -  Global step: 10404, epoch step:1200\n",
      "2022/03/24 18:04:20 - INFO - Distillation -  Global step: 10557, epoch step:1353\n",
      "2022/03/24 18:05:04 - INFO - Distillation -  Global step: 10710, epoch step:1506\n",
      "2022/03/24 18:05:47 - INFO - Distillation -  Global step: 10863, epoch step:1659\n",
      "2022/03/24 18:06:31 - INFO - Distillation -  Global step: 11016, epoch step:1812\n",
      "2022/03/24 18:07:14 - INFO - Distillation -  Global step: 11169, epoch step:1965\n",
      "2022/03/24 18:07:58 - INFO - Distillation -  Global step: 11322, epoch step:2118\n",
      "2022/03/24 18:08:42 - INFO - Distillation -  Global step: 11475, epoch step:2271\n",
      "2022/03/24 18:09:25 - INFO - Distillation -  Global step: 11628, epoch step:2424\n",
      "2022/03/24 18:10:09 - INFO - Distillation -  Global step: 11781, epoch step:2577\n",
      "2022/03/24 18:10:53 - INFO - Distillation -  Global step: 11934, epoch step:2730\n",
      "2022/03/24 18:11:37 - INFO - Distillation -  Global step: 12087, epoch step:2883\n",
      "2022/03/24 18:12:20 - INFO - Distillation -  Global step: 12240, epoch step:3036\n",
      "2022/03/24 18:12:30 - INFO - Distillation -  Saving at global step 12272, epoch step 3068 epoch 4\n",
      "2022/03/24 18:12:31 - INFO - Distillation -  Running callback function...\n",
      "2022/03/24 18:12:43 - INFO - Distillation -  {'mnli': {'acc': 0.6555272542027509}, 'mnli-mm': {'acc': 0.6599877949552482}}\n",
      "2022/03/24 18:12:43 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/03/24 18:12:43 - INFO - Distillation -  Epoch 5\n",
      "2022/03/24 18:12:43 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/03/24 18:13:17 - INFO - Distillation -  Global step: 12393, epoch step:121\n",
      "2022/03/24 18:14:01 - INFO - Distillation -  Global step: 12546, epoch step:274\n",
      "2022/03/24 18:14:44 - INFO - Distillation -  Global step: 12699, epoch step:427\n",
      "2022/03/24 18:15:28 - INFO - Distillation -  Global step: 12852, epoch step:580\n",
      "2022/03/24 18:16:12 - INFO - Distillation -  Global step: 13005, epoch step:733\n",
      "2022/03/24 18:16:55 - INFO - Distillation -  Global step: 13158, epoch step:886\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_358243/2872527670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, dataloader, num_epochs, scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_num_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_disable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_postprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mtrain_with_num_epochs\u001b[0;34m(self, optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_epochs, callback, batch_postprocessor, **args)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mwriter_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/textbrewer/distiller_basic.py\u001b[0m in \u001b[0;36mwrite_loss\u001b[0;34m(self, total_loss, writer_step, losses_dict)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mcpu_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scalar/total_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlosses_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "\n",
    "from matches import matches\n",
    "intermediate_matches = None\n",
    "match_list_L4t = [\"L4t_hidden_mse\", \"L4_hidden_smmd\"]\n",
    "match_list_L3 = [\"L3_hidden_mse\", \"L3_hidden_smmd\"]\n",
    "intermediate_matches = []\n",
    "for match in match_list_L3:\n",
    "    intermediate_matches += matches[match]\n",
    "\n",
    "distill_config = DistillationConfig(\n",
    "    intermediate_matches=[    \n",
    "     {'layer_T':0, 'layer_S':0, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1},\n",
    "     {'layer_T':8, 'layer_S':2, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1}])\n",
    "train_config = TrainingConfig(device=device)\n",
    "\n",
    "\n",
    "\n",
    "task_name = \"mnli\"\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "output_dir = \"outputs/\" + task_name + \"/\" \n",
    "eval_datasets = [val_dataset,val_mm_dataset]\n",
    "\n",
    "callback_func = partial(predict, eval_datasets=eval_datasets, output_dir=output_dir,task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [],
   "source": [
    "test_model = BertForSequenceClassification(bert_config_T3)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs490880.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbrewer.distiller_utils import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
