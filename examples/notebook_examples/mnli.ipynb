{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lokwq/TextBrewer/blob/add_note_examples/sst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMExDS48VN58"
   },
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oVTjuvH0rPsT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qqu-aNtc3QgP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from functools import partial\n",
    "from predict_function import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "task_name = \"mnli\"\n",
    "base_model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5ww8ad58D8v"
   },
   "source": [
    "### Prepare dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9-8wYOHG4WVq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('glue', 'mnli', split='train')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_dataset = load_dataset('glue', 'mnli', split='validation_matched')#,cache_dir=\"/work/mhessent/cache\")\n",
    "val_mm_dataset = load_dataset('glue', 'mnli', split='validation_mismatched')\n",
    "test_dataset = load_dataset('glue', 'mnli', split='test_matched')#,cache_dir=\"/work/mhessent/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iQSki-hv5Imc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b84661f5fe95e1e2.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43d59c952978c460.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cce2791de0a64e6f.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d933186d36b8e3ab.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(['label'])\n",
    "val_mm_dataset = val_mm_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "train_dataset = train_dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "whL22dsx5QU5"
   },
   "outputs": [],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(base_model_name, num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eH4rBumG5i6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6226276aad90cc7c.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f9f7bddc6b4c027d.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-39816bc57b32097a.arrow\n",
      "Loading cached processed dataset at /home/mhessent/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-033706f2a112a9eb.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "val_mm_dataset = val_mm_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['premise'],e['hypothesis'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_mm_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:][\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6jwP2aHv6EU6"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KonAbPBj6NCK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_args = TrainingArguments(\\n    output_dir=\\'outputs/results\\',          #output directory\\n    learning_rate=13e-5,\\n    num_train_epochs=3,              \\n    per_device_train_batch_size=32,                #batch size per device during training\\n    per_device_eval_batch_size=32,                #batch size for evaluation\\n    logging_dir=\\'outputs/logs\\',            \\n    logging_steps=100,\\n    do_train=True,\\n    do_eval=True,\\n    no_cuda=False,\\n    load_best_model_at_end=True,\\n    # eval_steps=100,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\"\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         \\n    args=training_args,                  \\n    train_dataset=train_dataset,         \\n    eval_dataset=val_dataset,            \\n    compute_metrics=compute_metrics\\n)\\n\\ntrain_out = trainer.train()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training \n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/results',          #output directory\n",
    "    learning_rate=13e-5,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,                #batch size per device during training\n",
    "    per_device_eval_batch_size=32,                #batch size for evaluation\n",
    "    logging_dir='outputs/logs',            \n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "\"\"\"\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1H8Dod2y6R8c"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'outputs/mnli_teacher_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gov66CaFNAgg"
   },
   "source": [
    "### Start distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IA-gwQKNB8fs"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128) #prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YD8qPZmUiTKH"
   },
   "outputs": [],
   "source": [
    "import textbrewer\n",
    "from textbrewer import GeneralDistiller\n",
    "from textbrewer import TrainingConfig, DistillationConfig\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emuX8UK8Mup"
   },
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CKLaqSPCiX1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "bert_config = BertConfig.from_pretrained(base_model_name)\n",
    "#bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "#bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"/work/mhessent/master_thesis/eval_out/bert-base-uncased/mnli/lr3e-05_bs32_epochs3/checkpoint-36816\")\n",
    "torch.save(model.state_dict(), 'outputs/hub_mnli_teacher_model.pt')\n",
    "bert_config = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config.json')\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.vocab_size = 30522\n",
    "bert_config.num_labels = 3\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\n",
    "teacher_model.load_state_dict(torch.load('outputs/hub_mnli_teacher_model.pt'))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "teacher_model = teacher_model.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "bert_config_T3 = BertConfig.from_json_file('/work/mhessent/TextBrewer/examples/student_config/bert_base_cased_config/bert_config_L3_v2.json')#相对路径\n",
    "bert_config_T3 = BertConfig.from_pretrained(base_model_name)\n",
    "bert_config_T3.num_hidden_layers = 3\n",
    "bert_config_T3.output_hidden_states = True\n",
    "bert_config_T3.num_labels = 3\n",
    "#bert_config_T3.vocab_size = teacher_model.config.vocab_size\n",
    "\n",
    "continue_training = False\n",
    "student_model = BertForSequenceClassification(bert_config_T3)\n",
    "if continue_training:\n",
    "    student_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs490880.pkl'))\n",
    "student_model = student_model.to(device=device)\n",
    "\n",
    "\n",
    "print(teacher_model.config.vocab_size)\n",
    "print(student_model.config.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6SuVnpa8RAm"
   },
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIxaegSUikGX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2022/04/06 10:13:01 - INFO - Distillation -  Training steps per epoch: 3068\n",
      "2022/04/06 10:13:01 - INFO - Distillation -  Checkpoints(step): [0]\n",
      "2022/04/06 10:13:01 - INFO - Distillation -  Epoch 1\n",
      "2022/04/06 10:13:01 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 10:14:12 - INFO - Distillation -  Global step: 153, epoch step:153\n",
      "2022/04/06 10:15:32 - INFO - Distillation -  Global step: 306, epoch step:306\n",
      "2022/04/06 10:16:51 - INFO - Distillation -  Global step: 459, epoch step:459\n",
      "2022/04/06 10:18:09 - INFO - Distillation -  Global step: 612, epoch step:612\n",
      "2022/04/06 10:19:27 - INFO - Distillation -  Global step: 765, epoch step:765\n",
      "2022/04/06 10:20:46 - INFO - Distillation -  Global step: 918, epoch step:918\n",
      "2022/04/06 10:22:05 - INFO - Distillation -  Global step: 1071, epoch step:1071\n",
      "2022/04/06 10:23:24 - INFO - Distillation -  Global step: 1224, epoch step:1224\n",
      "2022/04/06 10:24:41 - INFO - Distillation -  Global step: 1377, epoch step:1377\n",
      "2022/04/06 10:25:58 - INFO - Distillation -  Global step: 1530, epoch step:1530\n",
      "2022/04/06 10:27:18 - INFO - Distillation -  Global step: 1683, epoch step:1683\n",
      "2022/04/06 10:28:36 - INFO - Distillation -  Global step: 1836, epoch step:1836\n",
      "2022/04/06 10:29:56 - INFO - Distillation -  Global step: 1989, epoch step:1989\n",
      "2022/04/06 10:31:16 - INFO - Distillation -  Global step: 2142, epoch step:2142\n",
      "2022/04/06 10:32:34 - INFO - Distillation -  Global step: 2295, epoch step:2295\n",
      "2022/04/06 10:33:53 - INFO - Distillation -  Global step: 2448, epoch step:2448\n",
      "2022/04/06 10:35:13 - INFO - Distillation -  Global step: 2601, epoch step:2601\n",
      "2022/04/06 10:36:31 - INFO - Distillation -  Global step: 2754, epoch step:2754\n",
      "2022/04/06 10:37:50 - INFO - Distillation -  Global step: 2907, epoch step:2907\n",
      "2022/04/06 10:39:09 - INFO - Distillation -  Global step: 3060, epoch step:3060\n",
      "2022/04/06 10:39:13 - INFO - Distillation -  Saving at global step 3068, epoch step 3068 epoch 1\n",
      "/work/mhessent/miniconda/envs/thesis_test/lib/python3.8/site-packages/torch/nn/modules/module.py:1385: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "2022/04/06 10:39:16 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 10:44:26 - INFO - Distillation -  {'mnli': {'accuracy': 0.4804890473764646}, 'mnli-mm': {'accuracy': 0.4888120423108218}, 'train': {'accuracy': 0.4927043916252018}}\n",
      "2022/04/06 10:44:26 - INFO - Distillation -  Epoch 1 finished\n",
      "2022/04/06 10:44:26 - INFO - Distillation -  Epoch 2\n",
      "2022/04/06 10:44:26 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 10:45:34 - INFO - Distillation -  Global step: 3213, epoch step:145\n",
      "2022/04/06 10:46:53 - INFO - Distillation -  Global step: 3366, epoch step:298\n",
      "2022/04/06 10:48:11 - INFO - Distillation -  Global step: 3519, epoch step:451\n",
      "2022/04/06 10:49:30 - INFO - Distillation -  Global step: 3672, epoch step:604\n",
      "2022/04/06 10:50:49 - INFO - Distillation -  Global step: 3825, epoch step:757\n",
      "2022/04/06 10:52:06 - INFO - Distillation -  Global step: 3978, epoch step:910\n",
      "2022/04/06 10:53:25 - INFO - Distillation -  Global step: 4131, epoch step:1063\n",
      "2022/04/06 10:54:43 - INFO - Distillation -  Global step: 4284, epoch step:1216\n",
      "2022/04/06 10:56:02 - INFO - Distillation -  Global step: 4437, epoch step:1369\n",
      "2022/04/06 10:57:22 - INFO - Distillation -  Global step: 4590, epoch step:1522\n",
      "2022/04/06 10:58:41 - INFO - Distillation -  Global step: 4743, epoch step:1675\n",
      "2022/04/06 11:00:00 - INFO - Distillation -  Global step: 4896, epoch step:1828\n",
      "2022/04/06 11:01:19 - INFO - Distillation -  Global step: 5049, epoch step:1981\n",
      "2022/04/06 11:02:38 - INFO - Distillation -  Global step: 5202, epoch step:2134\n",
      "2022/04/06 11:03:58 - INFO - Distillation -  Global step: 5355, epoch step:2287\n",
      "2022/04/06 11:05:16 - INFO - Distillation -  Global step: 5508, epoch step:2440\n",
      "2022/04/06 11:06:33 - INFO - Distillation -  Global step: 5661, epoch step:2593\n",
      "2022/04/06 11:07:51 - INFO - Distillation -  Global step: 5814, epoch step:2746\n",
      "2022/04/06 11:09:08 - INFO - Distillation -  Global step: 5967, epoch step:2899\n",
      "2022/04/06 11:10:28 - INFO - Distillation -  Global step: 6120, epoch step:3052\n",
      "2022/04/06 11:10:36 - INFO - Distillation -  Saving at global step 6136, epoch step 3068 epoch 2\n",
      "2022/04/06 11:10:37 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 11:20:36 - INFO - Distillation -  {'mnli': {'accuracy': 0.5841059602649007}, 'mnli-mm': {'accuracy': 0.5844182262001627}, 'train': {'accuracy': 0.5981023778845027}}\n",
      "2022/04/06 11:20:36 - INFO - Distillation -  Epoch 2 finished\n",
      "2022/04/06 11:20:36 - INFO - Distillation -  Epoch 3\n",
      "2022/04/06 11:20:36 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 11:21:45 - INFO - Distillation -  Global step: 6273, epoch step:137\n",
      "2022/04/06 11:23:04 - INFO - Distillation -  Global step: 6426, epoch step:290\n",
      "2022/04/06 11:24:23 - INFO - Distillation -  Global step: 6579, epoch step:443\n",
      "2022/04/06 11:25:40 - INFO - Distillation -  Global step: 6732, epoch step:596\n",
      "2022/04/06 11:26:59 - INFO - Distillation -  Global step: 6885, epoch step:749\n",
      "2022/04/06 11:28:17 - INFO - Distillation -  Global step: 7038, epoch step:902\n",
      "2022/04/06 11:29:14 - INFO - Distillation -  Global step: 7191, epoch step:1055\n",
      "2022/04/06 11:29:56 - INFO - Distillation -  Global step: 7344, epoch step:1208\n",
      "2022/04/06 11:30:39 - INFO - Distillation -  Global step: 7497, epoch step:1361\n",
      "2022/04/06 11:31:21 - INFO - Distillation -  Global step: 7650, epoch step:1514\n",
      "2022/04/06 11:32:02 - INFO - Distillation -  Global step: 7803, epoch step:1667\n",
      "2022/04/06 11:32:44 - INFO - Distillation -  Global step: 7956, epoch step:1820\n",
      "2022/04/06 11:33:25 - INFO - Distillation -  Global step: 8109, epoch step:1973\n",
      "2022/04/06 11:34:06 - INFO - Distillation -  Global step: 8262, epoch step:2126\n",
      "2022/04/06 11:34:47 - INFO - Distillation -  Global step: 8415, epoch step:2279\n",
      "2022/04/06 11:35:29 - INFO - Distillation -  Global step: 8568, epoch step:2432\n",
      "2022/04/06 11:36:09 - INFO - Distillation -  Global step: 8721, epoch step:2585\n",
      "2022/04/06 11:36:49 - INFO - Distillation -  Global step: 8874, epoch step:2738\n",
      "2022/04/06 11:37:29 - INFO - Distillation -  Global step: 9027, epoch step:2891\n",
      "2022/04/06 11:38:11 - INFO - Distillation -  Global step: 9180, epoch step:3044\n",
      "2022/04/06 11:38:18 - INFO - Distillation -  Saving at global step 9204, epoch step 3068 epoch 3\n",
      "2022/04/06 11:38:19 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 11:42:37 - INFO - Distillation -  {'mnli': {'accuracy': 0.6189505858380031}, 'mnli-mm': {'accuracy': 0.6258136696501221}, 'train': {'accuracy': 0.6460318511237528}}\n",
      "2022/04/06 11:42:37 - INFO - Distillation -  Epoch 3 finished\n",
      "2022/04/06 11:42:37 - INFO - Distillation -  Epoch 4\n",
      "2022/04/06 11:42:37 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 11:43:13 - INFO - Distillation -  Global step: 9333, epoch step:129\n",
      "2022/04/06 11:43:55 - INFO - Distillation -  Global step: 9486, epoch step:282\n",
      "2022/04/06 11:44:39 - INFO - Distillation -  Global step: 9639, epoch step:435\n",
      "2022/04/06 11:45:22 - INFO - Distillation -  Global step: 9792, epoch step:588\n",
      "2022/04/06 11:46:04 - INFO - Distillation -  Global step: 9945, epoch step:741\n",
      "2022/04/06 11:46:48 - INFO - Distillation -  Global step: 10098, epoch step:894\n",
      "2022/04/06 11:47:31 - INFO - Distillation -  Global step: 10251, epoch step:1047\n",
      "2022/04/06 11:48:13 - INFO - Distillation -  Global step: 10404, epoch step:1200\n",
      "2022/04/06 11:48:56 - INFO - Distillation -  Global step: 10557, epoch step:1353\n",
      "2022/04/06 11:49:40 - INFO - Distillation -  Global step: 10710, epoch step:1506\n",
      "2022/04/06 11:50:23 - INFO - Distillation -  Global step: 10863, epoch step:1659\n",
      "2022/04/06 11:51:06 - INFO - Distillation -  Global step: 11016, epoch step:1812\n",
      "2022/04/06 11:51:49 - INFO - Distillation -  Global step: 11169, epoch step:1965\n",
      "2022/04/06 11:52:30 - INFO - Distillation -  Global step: 11322, epoch step:2118\n",
      "2022/04/06 11:53:12 - INFO - Distillation -  Global step: 11475, epoch step:2271\n",
      "2022/04/06 11:53:54 - INFO - Distillation -  Global step: 11628, epoch step:2424\n",
      "2022/04/06 11:54:37 - INFO - Distillation -  Global step: 11781, epoch step:2577\n",
      "2022/04/06 11:55:19 - INFO - Distillation -  Global step: 11934, epoch step:2730\n",
      "2022/04/06 11:56:01 - INFO - Distillation -  Global step: 12087, epoch step:2883\n",
      "2022/04/06 11:56:43 - INFO - Distillation -  Global step: 12240, epoch step:3036\n",
      "2022/04/06 11:56:51 - INFO - Distillation -  Saving at global step 12272, epoch step 3068 epoch 4\n",
      "2022/04/06 11:56:59 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 12:00:59 - INFO - Distillation -  {'mnli': {'accuracy': 0.6707080998471727}, 'mnli-mm': {'accuracy': 0.6749389747762409}, 'train': {'accuracy': 0.7059475123630641}}\n",
      "2022/04/06 12:00:59 - INFO - Distillation -  Epoch 4 finished\n",
      "2022/04/06 12:00:59 - INFO - Distillation -  Epoch 5\n",
      "2022/04/06 12:00:59 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 12:01:30 - INFO - Distillation -  Global step: 12393, epoch step:121\n",
      "2022/04/06 12:02:07 - INFO - Distillation -  Global step: 12546, epoch step:274\n",
      "2022/04/06 12:02:47 - INFO - Distillation -  Global step: 12699, epoch step:427\n",
      "2022/04/06 12:03:27 - INFO - Distillation -  Global step: 12852, epoch step:580\n",
      "2022/04/06 12:04:05 - INFO - Distillation -  Global step: 13005, epoch step:733\n",
      "2022/04/06 12:04:44 - INFO - Distillation -  Global step: 13158, epoch step:886\n",
      "2022/04/06 12:05:22 - INFO - Distillation -  Global step: 13311, epoch step:1039\n",
      "2022/04/06 12:06:04 - INFO - Distillation -  Global step: 13464, epoch step:1192\n",
      "2022/04/06 12:06:45 - INFO - Distillation -  Global step: 13617, epoch step:1345\n",
      "2022/04/06 12:07:27 - INFO - Distillation -  Global step: 13770, epoch step:1498\n",
      "2022/04/06 12:08:09 - INFO - Distillation -  Global step: 13923, epoch step:1651\n",
      "2022/04/06 12:08:52 - INFO - Distillation -  Global step: 14076, epoch step:1804\n",
      "2022/04/06 12:09:34 - INFO - Distillation -  Global step: 14229, epoch step:1957\n",
      "2022/04/06 12:10:16 - INFO - Distillation -  Global step: 14382, epoch step:2110\n",
      "2022/04/06 12:10:58 - INFO - Distillation -  Global step: 14535, epoch step:2263\n",
      "2022/04/06 12:11:42 - INFO - Distillation -  Global step: 14688, epoch step:2416\n",
      "2022/04/06 12:12:24 - INFO - Distillation -  Global step: 14841, epoch step:2569\n",
      "2022/04/06 12:13:06 - INFO - Distillation -  Global step: 14994, epoch step:2722\n",
      "2022/04/06 12:13:48 - INFO - Distillation -  Global step: 15147, epoch step:2875\n",
      "2022/04/06 12:14:31 - INFO - Distillation -  Global step: 15300, epoch step:3028\n",
      "2022/04/06 12:14:42 - INFO - Distillation -  Saving at global step 15340, epoch step 3068 epoch 5\n",
      "2022/04/06 12:15:03 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 12:19:22 - INFO - Distillation -  {'mnli': {'accuracy': 0.6922058074375955}, 'mnli-mm': {'accuracy': 0.6984336859235151}, 'train': {'accuracy': 0.7464311360777384}}\n",
      "2022/04/06 12:19:22 - INFO - Distillation -  Epoch 5 finished\n",
      "2022/04/06 12:19:22 - INFO - Distillation -  Epoch 6\n",
      "2022/04/06 12:19:22 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 12:19:51 - INFO - Distillation -  Global step: 15453, epoch step:113\n",
      "2022/04/06 12:20:33 - INFO - Distillation -  Global step: 15606, epoch step:266\n",
      "2022/04/06 12:21:18 - INFO - Distillation -  Global step: 15759, epoch step:419\n",
      "2022/04/06 12:21:58 - INFO - Distillation -  Global step: 15912, epoch step:572\n",
      "2022/04/06 12:22:33 - INFO - Distillation -  Global step: 16065, epoch step:725\n",
      "2022/04/06 12:23:10 - INFO - Distillation -  Global step: 16218, epoch step:878\n",
      "2022/04/06 12:23:46 - INFO - Distillation -  Global step: 16371, epoch step:1031\n",
      "2022/04/06 12:24:22 - INFO - Distillation -  Global step: 16524, epoch step:1184\n",
      "2022/04/06 12:24:58 - INFO - Distillation -  Global step: 16677, epoch step:1337\n",
      "2022/04/06 12:25:33 - INFO - Distillation -  Global step: 16830, epoch step:1490\n",
      "2022/04/06 12:26:08 - INFO - Distillation -  Global step: 16983, epoch step:1643\n",
      "2022/04/06 12:26:44 - INFO - Distillation -  Global step: 17136, epoch step:1796\n",
      "2022/04/06 12:27:19 - INFO - Distillation -  Global step: 17289, epoch step:1949\n",
      "2022/04/06 12:27:55 - INFO - Distillation -  Global step: 17442, epoch step:2102\n",
      "2022/04/06 12:28:30 - INFO - Distillation -  Global step: 17595, epoch step:2255\n",
      "2022/04/06 12:29:06 - INFO - Distillation -  Global step: 17748, epoch step:2408\n",
      "2022/04/06 12:29:42 - INFO - Distillation -  Global step: 17901, epoch step:2561\n",
      "2022/04/06 12:30:18 - INFO - Distillation -  Global step: 18054, epoch step:2714\n",
      "2022/04/06 12:30:55 - INFO - Distillation -  Global step: 18207, epoch step:2867\n",
      "2022/04/06 12:31:30 - INFO - Distillation -  Global step: 18360, epoch step:3020\n",
      "2022/04/06 12:31:41 - INFO - Distillation -  Saving at global step 18408, epoch step 3068 epoch 6\n",
      "2022/04/06 12:32:26 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 12:36:37 - INFO - Distillation -  {'mnli': {'accuracy': 0.7045338767193072}, 'mnli-mm': {'accuracy': 0.7086045565500407}, 'train': {'accuracy': 0.766667855014744}}\n",
      "2022/04/06 12:36:37 - INFO - Distillation -  Epoch 6 finished\n",
      "2022/04/06 12:36:37 - INFO - Distillation -  Epoch 7\n",
      "2022/04/06 12:36:37 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 12:37:01 - INFO - Distillation -  Global step: 18513, epoch step:105\n",
      "2022/04/06 12:37:35 - INFO - Distillation -  Global step: 18666, epoch step:258\n",
      "2022/04/06 12:38:10 - INFO - Distillation -  Global step: 18819, epoch step:411\n",
      "2022/04/06 12:38:46 - INFO - Distillation -  Global step: 18972, epoch step:564\n",
      "2022/04/06 12:39:21 - INFO - Distillation -  Global step: 19125, epoch step:717\n",
      "2022/04/06 12:39:57 - INFO - Distillation -  Global step: 19278, epoch step:870\n",
      "2022/04/06 12:40:33 - INFO - Distillation -  Global step: 19431, epoch step:1023\n",
      "2022/04/06 12:41:08 - INFO - Distillation -  Global step: 19584, epoch step:1176\n",
      "2022/04/06 12:41:44 - INFO - Distillation -  Global step: 19737, epoch step:1329\n",
      "2022/04/06 12:42:21 - INFO - Distillation -  Global step: 19890, epoch step:1482\n",
      "2022/04/06 12:42:56 - INFO - Distillation -  Global step: 20043, epoch step:1635\n",
      "2022/04/06 12:43:33 - INFO - Distillation -  Global step: 20196, epoch step:1788\n",
      "2022/04/06 12:44:11 - INFO - Distillation -  Global step: 20349, epoch step:1941\n",
      "2022/04/06 12:44:47 - INFO - Distillation -  Global step: 20502, epoch step:2094\n",
      "2022/04/06 12:45:22 - INFO - Distillation -  Global step: 20655, epoch step:2247\n",
      "2022/04/06 12:46:00 - INFO - Distillation -  Global step: 20808, epoch step:2400\n",
      "2022/04/06 12:46:37 - INFO - Distillation -  Global step: 20961, epoch step:2553\n",
      "2022/04/06 12:47:13 - INFO - Distillation -  Global step: 21114, epoch step:2706\n",
      "2022/04/06 12:47:50 - INFO - Distillation -  Global step: 21267, epoch step:2859\n",
      "2022/04/06 12:48:24 - INFO - Distillation -  Global step: 21420, epoch step:3012\n",
      "2022/04/06 12:48:37 - INFO - Distillation -  Saving at global step 21476, epoch step 3068 epoch 7\n",
      "2022/04/06 12:49:57 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 12:55:29 - INFO - Distillation -  {'mnli': {'accuracy': 0.7224656138563423}, 'mnli-mm': {'accuracy': 0.7220301057770545}, 'train': {'accuracy': 0.7920178659645227}}\n",
      "2022/04/06 12:55:29 - INFO - Distillation -  Epoch 7 finished\n",
      "2022/04/06 12:55:29 - INFO - Distillation -  Epoch 8\n",
      "2022/04/06 12:55:29 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 12:55:56 - INFO - Distillation -  Global step: 21573, epoch step:97\n",
      "2022/04/06 12:56:38 - INFO - Distillation -  Global step: 21726, epoch step:250\n",
      "2022/04/06 12:57:20 - INFO - Distillation -  Global step: 21879, epoch step:403\n",
      "2022/04/06 12:58:03 - INFO - Distillation -  Global step: 22032, epoch step:556\n",
      "2022/04/06 12:58:43 - INFO - Distillation -  Global step: 22185, epoch step:709\n",
      "2022/04/06 12:59:25 - INFO - Distillation -  Global step: 22338, epoch step:862\n",
      "2022/04/06 13:00:07 - INFO - Distillation -  Global step: 22491, epoch step:1015\n",
      "2022/04/06 13:00:50 - INFO - Distillation -  Global step: 22644, epoch step:1168\n",
      "2022/04/06 13:01:32 - INFO - Distillation -  Global step: 22797, epoch step:1321\n",
      "2022/04/06 13:02:14 - INFO - Distillation -  Global step: 22950, epoch step:1474\n",
      "2022/04/06 13:02:57 - INFO - Distillation -  Global step: 23103, epoch step:1627\n",
      "2022/04/06 13:03:40 - INFO - Distillation -  Global step: 23256, epoch step:1780\n",
      "2022/04/06 13:04:22 - INFO - Distillation -  Global step: 23409, epoch step:1933\n",
      "2022/04/06 13:05:04 - INFO - Distillation -  Global step: 23562, epoch step:2086\n",
      "2022/04/06 13:05:46 - INFO - Distillation -  Global step: 23715, epoch step:2239\n",
      "2022/04/06 13:06:29 - INFO - Distillation -  Global step: 23868, epoch step:2392\n",
      "2022/04/06 13:07:11 - INFO - Distillation -  Global step: 24021, epoch step:2545\n",
      "2022/04/06 13:07:53 - INFO - Distillation -  Global step: 24174, epoch step:2698\n",
      "2022/04/06 13:08:36 - INFO - Distillation -  Global step: 24327, epoch step:2851\n",
      "2022/04/06 13:09:19 - INFO - Distillation -  Global step: 24480, epoch step:3004\n",
      "2022/04/06 13:09:37 - INFO - Distillation -  Saving at global step 24544, epoch step 3068 epoch 8\n",
      "2022/04/06 13:09:44 - INFO - Distillation -  Running callback function...\n",
      "2022/04/06 13:14:03 - INFO - Distillation -  {'mnli': {'accuracy': 0.7270504330106979}, 'mnli-mm': {'accuracy': 0.7347436940602116}, 'train': {'accuracy': 0.8103498326975671}}\n",
      "2022/04/06 13:14:03 - INFO - Distillation -  Epoch 8 finished\n",
      "2022/04/06 13:14:03 - INFO - Distillation -  Epoch 9\n",
      "2022/04/06 13:14:03 - INFO - Distillation -  Length of current epoch in forward batch: 3068\n",
      "2022/04/06 13:14:28 - INFO - Distillation -  Global step: 24633, epoch step:89\n",
      "2022/04/06 13:15:10 - INFO - Distillation -  Global step: 24786, epoch step:242\n",
      "2022/04/06 13:15:52 - INFO - Distillation -  Global step: 24939, epoch step:395\n",
      "2022/04/06 13:16:34 - INFO - Distillation -  Global step: 25092, epoch step:548\n",
      "2022/04/06 13:17:16 - INFO - Distillation -  Global step: 25245, epoch step:701\n",
      "2022/04/06 13:17:58 - INFO - Distillation -  Global step: 25398, epoch step:854\n",
      "2022/04/06 13:18:40 - INFO - Distillation -  Global step: 25551, epoch step:1007\n",
      "2022/04/06 13:19:24 - INFO - Distillation -  Global step: 25704, epoch step:1160\n",
      "2022/04/06 13:20:07 - INFO - Distillation -  Global step: 25857, epoch step:1313\n",
      "2022/04/06 13:20:47 - INFO - Distillation -  Global step: 26010, epoch step:1466\n",
      "2022/04/06 13:21:29 - INFO - Distillation -  Global step: 26163, epoch step:1619\n",
      "2022/04/06 13:22:05 - INFO - Distillation -  Global step: 26316, epoch step:1772\n",
      "2022/04/06 13:22:42 - INFO - Distillation -  Global step: 26469, epoch step:1925\n",
      "2022/04/06 13:23:23 - INFO - Distillation -  Global step: 26622, epoch step:2078\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "\n",
    "from matches import matches\n",
    "intermediate_matches = None\n",
    "match_list_L4t = [\"L4t_hidden_mse\", \"L4_hidden_smmd\"]\n",
    "match_list_L3 = [\"L3_hidden_mse\", \"L3_hidden_smmd\"]\n",
    "intermediate_matches = []\n",
    "for match in match_list_L3:\n",
    "    intermediate_matches += matches[match]\n",
    "\n",
    "output_dir = \"outputs/\" + base_model_name + \"/\" + task_name + \"/\" + \"hl\"+ str(student_model.config.num_hidden_layers) + \"_hs\" +  str(student_model.config.hidden_size) + \"/\"\n",
    "distill_config = DistillationConfig(\n",
    "    intermediate_matches=[    \n",
    "     {'layer_T':0, 'layer_S':0, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1},\n",
    "     {'layer_T':8, 'layer_S':2, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1}])\n",
    "train_config = TrainingConfig(device=device, output_dir = output_dir + \"models/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_rank = -1\n",
    "predict_batch_size = 32\n",
    "device = device\n",
    "do_train_eval = True\n",
    "\n",
    "eval_datasets = [val_dataset,val_mm_dataset]\n",
    "\n",
    "callback_func = partial(predict, eval_datasets=eval_datasets, output_dir=output_dir+\"results/\",task_name=task_name,local_rank=local_rank,predict_batch_size=predict_batch_size,device=device, do_train_eval=do_train_eval, train_dataset=train_dataset)\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   },
   "outputs": [],
   "source": [
    "test_model = BertForSequenceClassification(bert_config_T3)\n",
    "test_model.load_state_dict(torch.load('/work/mhessent/TextBrewer/examples/notebook_examples/saved_models/gs490880.pkl'))#gs4210 is the distilled model weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYCFnAMkpE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbrewer.distiller_utils import move_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   },
   "outputs": [],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
